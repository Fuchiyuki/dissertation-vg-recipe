{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I5WCJhQtmjFW"
      },
      "outputs": [],
      "source": [
        "# --- ライブラリの入れ直し ---\n",
        "!pip uninstall -y openai -q        # ← いったん削除\n",
        "!pip -q install openai==0.27.2 tqdm rouge-score evaluate pandas tiktoken\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai | grep Version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB3u8PwwjH6L",
        "outputId": "b7a3a923-4f23-4dbc-b057-52b561c8dc16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import getpass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Iterator, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import openai\n",
        "import evaluate\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3hXZtETmpbZ",
        "outputId": "95b654fe-4347-459b-cbfa-d35ca78ba141"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure API key input\n",
        "print(\"🔑 Please enter your OpenAI API key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"API Key: \")\n",
        "\n",
        "# Verify API key is set\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"✅ API key configured successfully!\")\n",
        "else:\n",
        "    print(\"❌ API key not set. Please run this cell again.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YatbNeNvm5tV",
        "outputId": "f3aa1d3d-91f7-496f-9947-264434469b41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Please enter your OpenAI API key:\n",
            "API Key: ··········\n",
            "✅ API key configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls /content/drive/MyDrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65QaZpstnjA3",
        "outputId": "e917042f-4f2c-451b-d7d0-634a2cf4831c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'2019世界各国官製地図発表（3E17関淵之） (1).pptx'\n",
            " 2019世界各国官製地図発表（3E17関淵之）.pptx\n",
            " 2019世界各国官製地図発表aa（3E17関淵之）.pptx\n",
            "'Activity Diagram.drawio.png'\n",
            " adult.zip\n",
            " bc_01-17.zip\n",
            " bc_18-23.zip\n",
            " bc_24-27.zip\n",
            " bc_28-33.zip\n",
            " BGMshuu\n",
            " blind_test\n",
            "'Career Event by FAST OFFER.gform'\n",
            "'Class Diagram.drawio.png'\n",
            "'Class Diagram_White.drawio.png'\n",
            " Colab\n",
            "'Colab Notebooks'\n",
            " Component_sht.drawio.png\n",
            "'Coronavirus (COVID-19) records (1).pdf'\n",
            " CV_Fuchiyuki.docx\n",
            " data_weights\n",
            "'Emailing Offer Letter View.pdf'\n",
            "'Extracting Actionable Knowledge from Cooking Recipes: An LLM Approach to Commonsense Reasoning and Graphical RepresentationI.gdoc'\n",
            "'family mart in oomiya.csv'\n",
            "'File to replace '\n",
            "'Fuchiyuki Seki.pdf'\n",
            " GIS発表1A18関淵之.pptx\n",
            "'Google AI Studio'\n",
            " hihihi\n",
            " IMG_1826.MOV\n",
            " IMG_1827.MOV\n",
            " IMG_9927.jpeg\n",
            " japan_ver80_prefecture.shp\n",
            "'Khash Bonus'\n",
            "'Level_3_Week_14_Free_Lesson_copy (1).gslides'\n",
            " Level_3_Week_14_Free_Lesson_copy.gslides\n",
            "'Literature Review.gdoc'\n",
            "'Liz to Aoi Tori Movie.mp4'\n",
            "'new doc 2019-04-18 14.14.14.pdf'\n",
            " Persona.gslides\n",
            "'persona-template (1).gslides'\n",
            " pizza_recipes\n",
            " PPC_test.ipynb\n",
            " PPC_test_new.ipynb\n",
            " PPC_weight\n",
            "'Project Plan.gdoc'\n",
            " Prompt_for_eSNLI.gdoc\n",
            " recipe1m\n",
            "'Recipe Commonsense Reasoning Project Details'$'\\n'' (1).gdoc'\n",
            "'Recipe Commonsense Reasoning Project Details'$'\\n''.gdoc'\n",
            "'Recycling and CO2 emissions helper application（回答）.gsheet'\n",
            " Resources.docx\n",
            "'SnapCrab_Piriform Speccy_2019-1-31_2-58-40_No-00.png'\n",
            "'SnapCrab_Piriform Speccy_2019-1-31_3-1-58_No-00.png'\n",
            " src\n",
            " train\n",
            " Transcript.pdf\n",
            " unet_weights.pth\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled presentation (1).gslides'\n",
            "'Untitled presentation (2).gslides'\n",
            "'Untitled presentation.gslides'\n",
            "'Untitled spreadsheet.gsheet'\n",
            " utils.py\n",
            " veg_recipe1m_output\n",
            " VID_20190109_175011.mp4\n",
            " VID_20190109_175016.mp4\n",
            " VID_20190109_175633.mp4\n",
            " VID_20190109_180103.mp4\n",
            " Weights\n",
            "'例題0(バレーボール部員名簿①)_20171004.xlsm'\n",
            "'例題1(教p81)順次構造_20171101.xlsm'\n",
            "'例題1(教p84)逐次探索_20171101.xlsm'\n",
            "'例題1(教p84)逐次探索(応用)_20171115.xlsm'\n",
            "'例題2(教p82)選択構造_20171101.xlsm'\n",
            "'例題2(教p82)選択構造応用(合格表示)_20171101.xlsm'\n",
            "'例題2(教p85)逐次探索の活用_20171108.xlsm'\n",
            "'例題3-1(商品分類売上高一覧表)_20171004.xlsm'\n",
            "'例題3-2(週刊売上一覧表)_20171004.xlsm'\n",
            "'例題3-3(時刻の表示)_20171004.xlsm'\n",
            "'例題3-4(シートの追加)_20171011.xlsm'\n",
            "'例題3-5(判定)_20171011.xlsm'\n",
            "'例題3-6(ピザ売上一覧表)_20171025.xlsm'\n",
            "'例題3-7(テニスコート)_20171204.xlsm'\n",
            "'例題3-8(フォームの作成1)_20171108.xlsm'\n",
            "'例題3-9(フォームの作成)_20171108.xlsm'\n",
            "'例題3(教p83)繰り返し構造_20171101.xlsm'\n",
            "'例題3(教p83)繰り返し構造(応用)_20171101.xlsm'\n",
            "'例題3(教p86)二分探索_20171115.xlsm'\n",
            "'例題3(教p86)二分探索(応用)_20171115.xlsm'\n",
            "'例題4(教p88)交換法のプログラム_20171122.xlsm'\n",
            "'例題4(教p88)交換法のプログラム_20171122.xlsx'\n",
            "'例題5(教p90)交換法のプログラム_20171122.xlsm'\n",
            "'例題5(教p90)復元交換法_20171122.xlsm'\n",
            " 情報の科学_授業資料_20180101.pdf\n",
            "'情報の科学_授業資料(問題解決の考え 第３章)_20170906.pdf'\n",
            "'無題のドキュメント (1).gdoc'\n",
            " 無題のドキュメント.gdoc\n",
            " 無題のフォーム.gform\n",
            "'知識連鎖（Chain of Knowledge）を用いた手続き的常識推論の再評価：PizzaCommonsenseデータセットに関する研究提案.gdoc'\n",
            "'練習問題(RGB)_20171025.xlsm'\n",
            "'練習問題(シート追加日付き)_20171011.xlsm'\n",
            "'練習問題(ナンバリング)_20171025.xlsm'\n",
            "'練習問題(合否判定)_20171011.xlsm'\n",
            "'練習問題(担当者名入力)_20171011.xlsm'\n",
            "'練習問題(背景色の変更)_20171011.xlsm'\n",
            " 自分史.docx\n",
            " 高忠実度ベジタリアンレシピコーパスのキュレーションと知識の連鎖プロンプティングによる手続き的コモンセンス推論の導出に関する方法論的フレームワーク.gdoc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class PizzaDataLoader:\n",
        "    \"\"\"\n",
        "    Handles folder traversal and JSON parsing for PizzaCommonSense dataset.\n",
        "    デフォルトの data_path を Google Drive 内の MyDrive/train フォルダに変更。\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path: str = \"/content/drive/MyDrive/train\"):\n",
        "        self.data_path = Path(data_path)\n",
        "        if not self.data_path.exists():\n",
        "            raise FileNotFoundError(f\"Data path {data_path} does not exist\")\n",
        "        print(f\"✅ Using data path: {self.data_path}\")\n",
        "\n",
        "    def iter_tables(self, split: str = None) -> Iterator[dict]:\n",
        "        \"\"\"\n",
        "        split 引数は不要 (すでに train フォルダ直下を想定)。\n",
        "        フォルダ内のすべての .txt ファイルを読み込みます。\n",
        "        \"\"\"\n",
        "        txt_files = list(self.data_path.glob(\"*.txt\"))\n",
        "        print(f\"Found {len(txt_files)} recipe files in {self.data_path.name}\")\n",
        "\n",
        "        for file_path in txt_files:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                if 'table' in data and isinstance(data['table'], list):\n",
        "                    for step in data['table']:\n",
        "                        if self._validate_step(step):\n",
        "                            yield step\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Skipping {file_path.name}: {e}\")\n",
        "\n",
        "    def _validate_step(self, step: dict) -> bool:\n",
        "        required = ['instructions', 'actions', 'input', 'output']\n",
        "        return all(k in step for k in required)\n",
        "\n",
        "    def get_split_stats(self) -> dict:\n",
        "        steps = list(self.iter_tables())\n",
        "        return {\n",
        "            'total_steps': len(steps),\n",
        "            'unique_actions': len(set(s['actions'] for s in steps)),\n",
        "            'avg_instruction_length': np.mean([len(s['instructions']) for s in steps])\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Fj-_tkmdnC96"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ドライブマウント後に実行\n",
        "data_loader = PizzaDataLoader()  # デフォルトで /content/drive/MyDrive/train を参照\n",
        "stats = data_loader.get_split_stats()\n",
        "print(\"📊 Dataset stats:\", stats)\n",
        "\n",
        "# サンプル3件を表示\n",
        "for i, step in enumerate(data_loader.iter_tables()):\n",
        "    print(f\"\\nStep {i+1}\")\n",
        "    print(\" Instructions:\", step['instructions'])\n",
        "    print(\" Actions:     \", step['actions'])\n",
        "    print(\" Input:       \", step['input'])\n",
        "    print(\" Output:      \", step['output'])\n",
        "    if i >= 2:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBsPBJOlbqbv",
        "outputId": "69802644-7bb4-4b66-f4d7-471983b30956"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using data path: /content/drive/MyDrive/train\n",
            "Found 744 recipe files in train\n",
            "📊 Dataset stats: {'total_steps': 3069, 'unique_actions': 168, 'avg_instruction_length': np.float64(40.33919843597263)}\n",
            "Found 744 recipe files in train\n",
            "\n",
            "Step 1\n",
            " Instructions: preheat the oven to 400f .\n",
            " Actions:      preheat\n",
            " Input:        NA\n",
            " Output:       NA\n",
            "\n",
            "Step 2\n",
            " Instructions: heat the oil in a large non stick frying pan\n",
            " Actions:      heat\n",
            " Input:        oil\n",
            " Output:       heated_oil\n",
            "\n",
            "Step 3\n",
            " Instructions: add the onion , pepper and zucchini\n",
            " Actions:      add\n",
            " Input:        (onion; pepper; zucchini; heated_oil)\n",
            " Output:       onion, pepper and zucchini added to heated oil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CoTPromptGenerator:\n",
        "    \"\"\"Chain‑of‑Thought prompt generator for PizzaCommonSense.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # ✅ ルールを明文化\n",
        "        self.system_message = (\n",
        "            \"You are an expert cooking‑reasoning assistant.\\n\"\n",
        "            \"For every recipe step you must predict\\n\"\n",
        "            \"  • the *input* comestibles/items that go into the action（原材料がない場合は NA）\\n\"\n",
        "            \"  • the *output* comestible/result that comes out （結果が食材でない／道具のみの場合は NA）\\n\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"1. If the step does NOT consume or transform any food, write exactly 'NA' for BOTH input and output.\\n\"\n",
        "            \"   ‑ e.g. pre‑heating an oven, washing a utensil, setting a timer.\\n\"\n",
        "            \"2. Otherwise list the food items succinctly; join multiple items with semicolons.\\n\"\n",
        "            \"3. Respond **only** with the two lines:\\n\"\n",
        "            \"     Input: <your prediction>\\n\"\n",
        "            \"     Output: <your prediction>\\n\"\n",
        "        )\n",
        "\n",
        "    def make_prompt(self, step: dict) -> str:\n",
        "        \"\"\"Create a deterministic CoT prompt for one recipe step.\"\"\"\n",
        "        instr  = self._clean(step[\"instructions\"])\n",
        "        action = self._clean(step[\"action\"  ] if \"action\" in step else step[\"actions\"])\n",
        "\n",
        "        return (\n",
        "            f\"Instruction: {instr}\\n\"\n",
        "            f\"Action: {action}\\n\\n\"\n",
        "            # CoT を誘発\n",
        "            \"Let's reason step by step.\\n\"\n",
        "            \"1️⃣ Identify whether any food or edible item is being used or produced.\\n\"\n",
        "            \"2️⃣ If none, decide Input=NA and Output=NA immediately.\\n\"\n",
        "            \"3️⃣ Otherwise, list the food that goes *into* the action (Input) and \"\n",
        "            \"the food/result that comes *out of* the action (Output).\\n\\n\"\n",
        "            \"Remember the required format:\\n\"\n",
        "            \"Input: <prediction or NA>\\n\"\n",
        "            \"Output: <prediction or NA>\"\n",
        "        )\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "    @staticmethod\n",
        "    def _clean(text: str) -> str:\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "        return text.replace(\"\\u2012\", \"-\")      # ‑→-\n",
        "\n",
        "    def get_system_message(self) -> str:\n",
        "        return self.system_message\n",
        "\n",
        "    # Initialize prompt generator\n",
        "        return self.system_message\n",
        "\n",
        "# Initialize prompt generator\n",
        "prompt_generator = CoTPromptGenerator()\n",
        "print(\"✅ CoT prompt generator initialized successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMl1ZTFArYPD",
        "outputId": "a28324c6-e5aa-4427-d93c-96fa644dc77a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CoT prompt generator initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompt generation with sample data\n",
        "print(\"🧪 Testing Prompt Generation:\\n\" + \"=\"*50)\n",
        "print(\"SYSTEM MESSAGE:\")\n",
        "print(\"=\"*50)\n",
        "print(prompt_generator.get_system_message())\n",
        "\n",
        "# Get a sample step for testing\n",
        "sample_step = next(data_loader.iter_tables(\"val\"))\n",
        "test_prompt = prompt_generator.make_prompt(sample_step)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAMPLE PROMPT:\")\n",
        "print(\"=\"*50)\n",
        "print(test_prompt)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GROUND TRUTH:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input: {sample_step['input']}\")\n",
        "print(f\"Output: {sample_step['output']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BNLV9HAraOG",
        "outputId": "025a9f53-f653-480c-eb15-e58820909892"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Testing Prompt Generation:\n",
            "==================================================\n",
            "SYSTEM MESSAGE:\n",
            "==================================================\n",
            "You are an expert cooking‑reasoning assistant.\n",
            "For every recipe step you must predict\n",
            "  • the *input* comestibles/items that go into the action（原材料がない場合は NA）\n",
            "  • the *output* comestible/result that comes out （結果が食材でない／道具のみの場合は NA）\n",
            "\n",
            "Rules:\n",
            "1. If the step does NOT consume or transform any food, write exactly 'NA' for BOTH input and output.\n",
            "   ‑ e.g. pre‑heating an oven, washing a utensil, setting a timer.\n",
            "2. Otherwise list the food items succinctly; join multiple items with semicolons.\n",
            "3. Respond **only** with the two lines:\n",
            "     Input: <your prediction>\n",
            "     Output: <your prediction>\n",
            "\n",
            "Found 744 recipe files in train\n",
            "\n",
            "==================================================\n",
            "SAMPLE PROMPT:\n",
            "==================================================\n",
            "Instruction: preheat the oven to 400f .\n",
            "Action: preheat\n",
            "\n",
            "Let's reason step by step.\n",
            "1️⃣ Identify whether any food or edible item is being used or produced.\n",
            "2️⃣ If none, decide Input=NA and Output=NA immediately.\n",
            "3️⃣ Otherwise, list the food that goes *into* the action (Input) and the food/result that comes *out of* the action (Output).\n",
            "\n",
            "Remember the required format:\n",
            "Input: <prediction or NA>\n",
            "Output: <prediction or NA>\n",
            "\n",
            "==================================================\n",
            "GROUND TRUTH:\n",
            "==================================================\n",
            "Input: NA\n",
            "Output: NA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai | grep Version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvKpMwUSif5x",
        "outputId": "893ce428-6c07-45bd-8202-da3ea3f0626f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, re, os, openai\n",
        "\n",
        "# ← 0.27 系ではグローバル変数でキーを渡す\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "class GPT4Predictor:\n",
        "    \"\"\"\n",
        "    OpenAI 0.27.x 用 GPT‑4 呼び出しクラス\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4.1-mini\",\n",
        "                 max_retries: int = 3, base_delay: float = 0.5):\n",
        "        self.model = model\n",
        "        self.max_retries = max_retries\n",
        "        self.base_delay  = base_delay\n",
        "\n",
        "        # 接続テスト\n",
        "        openai.ChatCompletion.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"ping\"}],\n",
        "            max_tokens=1,\n",
        "            temperature=0\n",
        "        )\n",
        "        print(f\"✅ API connection successful with model: {self.model}\")\n",
        "\n",
        "    # ---------- 推論 ----------\n",
        "    def predict(self, prompt: str, system_message: str) -> str:\n",
        "        for i in range(self.max_retries):\n",
        "            try:\n",
        "                resp = openai.ChatCompletion.create(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_message},\n",
        "                        {\"role\": \"user\",   \"content\": prompt}\n",
        "                    ],\n",
        "                    temperature=0,\n",
        "                    max_tokens=200\n",
        "                )\n",
        "                return resp.choices[0].message[\"content\"].strip()\n",
        "\n",
        "            except openai.error.RateLimitError:\n",
        "                wait = self.base_delay * (2 ** i)\n",
        "                print(f\"⏳ Rate‑limit, retrying in {wait}s …\")\n",
        "                time.sleep(wait)\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                print(f\"❌ API error on attempt {i+1}: {e}\")\n",
        "                time.sleep(self.base_delay)\n",
        "\n",
        "        raise RuntimeError(\"Failed to get response after retries\")\n",
        "\n",
        "    # ---------- 応答パース ----------\n",
        "    @staticmethod\n",
        "    def parse_io(text: str) -> tuple[str, str]:\n",
        "        in_pat  = r\"Input\\s*[:：]\\s*(.+?)(?=\\n|Output|$)\"\n",
        "        out_pat = r\"Output\\s*[:：]\\s*(.+?)(?=\\n|$)\"\n",
        "        inp  = re.search(in_pat,  text, re.I | re.S)\n",
        "        out  = re.search(out_pat, text, re.I | re.S)\n",
        "        return (inp.group(1).strip()  if inp else \"\",\n",
        "                out.group(1).strip()  if out else \"\")\n",
        "\n",
        "    def add_delay(self):\n",
        "        time.sleep(self.base_delay)\n",
        "\n",
        "# -------- テスト --------\n",
        "predictor = GPT4Predictor()\n",
        "print(\"✅ GPT4Predictor is ready!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzp9HCRuhgYk",
        "outputId": "183ced7f-b08d-4450-a084-b32a38b68af2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API connection successful with model: gpt-4.1-mini\n",
            "✅ GPT4Predictor is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test GPT-4 prediction with sample data\n",
        "print(\"🧪 Testing GPT-4 Prediction:\")\n",
        "\n",
        "# Use the same sample step from before\n",
        "test_prompt = prompt_generator.make_prompt(sample_step)\n",
        "system_msg = prompt_generator.get_system_message()\n",
        "\n",
        "print(\"\\n⏳ Generating prediction...\")\n",
        "response = predictor.predict(test_prompt, system_msg)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL RESPONSE:\")\n",
        "print(\"=\"*50)\n",
        "print(response)\n",
        "\n",
        "# Parse the response\n",
        "pred_input, pred_output = predictor.parse_io(response)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PARSED PREDICTIONS:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Predicted Input: '{pred_input}'\")\n",
        "print(f\"Predicted Output: '{pred_output}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GROUND TRUTH COMPARISON:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Ground Truth Input:  '{sample_step['input']}'\")\n",
        "print(f\"Ground Truth Output: '{sample_step['output']}'\")\n",
        "\n",
        "# Check exact matches\n",
        "input_match = pred_input.lower().strip() == sample_step['input'].lower().strip()\n",
        "output_match = pred_output.lower().strip() == sample_step['output'].lower().strip()\n",
        "\n",
        "print(f\"\\n📊 Exact Match Results:\")\n",
        "print(f\"Input Match: {'✅' if input_match else '❌'}\")\n",
        "print(f\"Output Match: {'✅' if output_match else '❌'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882nVJa_rgHb",
        "outputId": "f2e762bd-a79b-42ae-cd4d-5a904bfc6149"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Testing GPT-4 Prediction:\n",
            "\n",
            "⏳ Generating prediction...\n",
            "\n",
            "==================================================\n",
            "MODEL RESPONSE:\n",
            "==================================================\n",
            "Input: NA\n",
            "Output: NA\n",
            "\n",
            "==================================================\n",
            "PARSED PREDICTIONS:\n",
            "==================================================\n",
            "Predicted Input: 'NA'\n",
            "Predicted Output: 'NA'\n",
            "\n",
            "==================================================\n",
            "GROUND TRUTH COMPARISON:\n",
            "==================================================\n",
            "Ground Truth Input:  'NA'\n",
            "Ground Truth Output: 'NA'\n",
            "\n",
            "📊 Exact Match Results:\n",
            "Input Match: ✅\n",
            "Output Match: ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_batch_predictions(split: str = \"val\", max_samples: int = None, save_interval: int = 50):\n",
        "    \"\"\"Run batch predictions on the dataset with progress tracking.\n",
        "\n",
        "    Args:\n",
        "        split: Dataset split to process ('val' or 'train')\n",
        "        max_samples: Maximum number of samples to process (None for all)\n",
        "        save_interval: Save results every N samples\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Results with predictions and ground truth\n",
        "    \"\"\"\n",
        "    print(f\"🚀 Starting batch prediction on {split} split...\")\n",
        "\n",
        "    # Collect all steps first to get total count\n",
        "    all_steps = list(data_loader.iter_tables(split))\n",
        "\n",
        "    if max_samples:\n",
        "        all_steps = all_steps[:max_samples]\n",
        "\n",
        "    print(f\"📊 Processing {len(all_steps)} recipe steps\")\n",
        "\n",
        "    results = []\n",
        "    system_msg = prompt_generator.get_system_message()\n",
        "\n",
        "    # Process with progress bar\n",
        "    for i, step in enumerate(tqdm(all_steps, desc=\"Generating predictions\")):\n",
        "        try:\n",
        "            # Generate prompt\n",
        "            prompt = prompt_generator.make_prompt(step)\n",
        "\n",
        "            # Get prediction\n",
        "            response = predictor.predict(prompt, system_msg)\n",
        "\n",
        "            # Parse response\n",
        "            pred_input, pred_output = predictor.parse_io(response)\n",
        "\n",
        "            # Store result\n",
        "            result = {\n",
        "                'instructions': step['instructions'],\n",
        "                'actions': step['actions'],\n",
        "                'input': step['input'],\n",
        "                'output': step['output'],\n",
        "                'pred_input': pred_input,\n",
        "                'pred_output': pred_output,\n",
        "                'response': response  # Keep full response for debugging\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Add delay for rate limiting\n",
        "            predictor.add_delay()\n",
        "\n",
        "            # Periodic saving\n",
        "            if (i + 1) % save_interval == 0:\n",
        "                temp_df = pd.DataFrame(results)\n",
        "                temp_df.to_csv(f\"temp_predictions_{i+1}.csv\", index=False)\n",
        "                print(f\"💾 Saved temporary results at step {i+1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing step {i+1}: {e}\")\n",
        "            # Add placeholder result to maintain alignment\n",
        "            result = {\n",
        "                'instructions': step['instructions'],\n",
        "                'actions': step['actions'],\n",
        "                'input': step['input'],\n",
        "                'output': step['output'],\n",
        "                'pred_input': '',\n",
        "                'pred_output': '',\n",
        "                'response': f'ERROR: {str(e)}'\n",
        "            }\n",
        "            results.append(result)\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    print(f\"✅ Batch processing complete! Processed {len(df)} steps\")\n",
        "    print(f\"📊 Success rate: {(df['pred_input'] != '').sum()}/{len(df)} ({(df['pred_input'] != '').mean():.1%})\")\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"✅ Batch processing function ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMhdXVA_rhQE",
        "outputId": "918a466d-18aa-4957-b328-534c69e1a1a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch processing function ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a small test first (10 samples)\n",
        "print(\"🧪 Running small test with 10 samples...\")\n",
        "test_df = run_batch_predictions(split=\"val\", max_samples=10)\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\n📋 Sample Results:\")\n",
        "display_cols = ['instructions', 'actions', 'input', 'output', 'pred_input', 'pred_output']\n",
        "print(test_df[display_cols].head())\n",
        "\n",
        "# Quick evaluation\n",
        "input_matches = (test_df['input'].str.lower().str.strip() ==\n",
        "                test_df['pred_input'].str.lower().str.strip()).sum()\n",
        "output_matches = (test_df['output'].str.lower().str.strip() ==\n",
        "                 test_df['pred_output'].str.lower().str.strip()).sum()\n",
        "\n",
        "print(f\"\\n📊 Quick Test Results:\")\n",
        "print(f\"Input EMA: {input_matches}/{len(test_df)} ({input_matches/len(test_df):.1%})\")\n",
        "print(f\"Output EMA: {output_matches}/{len(test_df)} ({output_matches/len(test_df):.1%})\")\n",
        "print(f\"Average EMA: {(input_matches + output_matches)/(2*len(test_df)):.1%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUkbNlA3lGp9",
        "outputId": "5448a640-2015-4837-90ea-3309c2f164d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Running small test with 10 samples...\n",
            "🚀 Starting batch prediction on val split...\n",
            "Found 744 recipe files in train\n",
            "📊 Processing 10 recipe steps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 10/10 [00:09<00:00,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch processing complete! Processed 10 steps\n",
            "📊 Success rate: 10/10 (100.0%)\n",
            "\n",
            "📋 Sample Results:\n",
            "                                   instructions  actions  \\\n",
            "0                    preheat the oven to 400f .  preheat   \n",
            "1  heat the oil in a large non stick frying pan     heat   \n",
            "2           add the onion , pepper and zucchini      add   \n",
            "3        saute over a medium heat for 4 5mins .    saute   \n",
            "4                                 add the herbs      add   \n",
            "\n",
            "                                   input  \\\n",
            "0                                     NA   \n",
            "1                                    oil   \n",
            "2  (onion; pepper; zucchini; heated_oil)   \n",
            "3               vegetables in heated_oil   \n",
            "4     (herbs; sauteed vegetable mixture)   \n",
            "\n",
            "                                           output               pred_input  \\\n",
            "0                                              NA                       NA   \n",
            "1                                      heated_oil                      oil   \n",
            "2  onion, pepper and zucchini added to heated oil  onion; pepper; zucchini   \n",
            "3                       sauteed vegetable mixture                       NA   \n",
            "4        herbs added to sauteed vegetable mixture                    herbs   \n",
            "\n",
            "               pred_output  \n",
            "0                       NA  \n",
            "1               heated oil  \n",
            "2  onion; pepper; zucchini  \n",
            "3                       NA  \n",
            "4                    herbs  \n",
            "\n",
            "📊 Quick Test Results:\n",
            "Input EMA: 2/10 (20.0%)\n",
            "Output EMA: 1/10 (10.0%)\n",
            "Average EMA: 15.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 追加セル\n",
        "!pip -q install bert_score transformers torch\n"
      ],
      "metadata": {
        "id": "7uN0rvYXtcxb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Run a small test first (10 samples)\n",
        "# ------------------------------------------------------------\n",
        "print(\"🧪 Running small test with 10 samples...\")\n",
        "test_df = run_batch_predictions(split=\"val\", max_samples=10)\n",
        "\n",
        "# ------------- 結果プレビュー -------------\n",
        "print(\"\\n📋 Sample Results:\")\n",
        "display_cols = ['instructions', 'actions', 'input', 'output',\n",
        "                'pred_input', 'pred_output']\n",
        "print(test_df[display_cols].head())\n",
        "\n",
        "# ------------- Exact‑Match Accuracy (EMA) -------------\n",
        "input_matches = (\n",
        "    test_df['input'].str.lower().str.strip() ==\n",
        "    test_df['pred_input'].str.lower().str.strip()\n",
        ").sum()\n",
        "\n",
        "output_matches = (\n",
        "    test_df['output'].str.lower().str.strip() ==\n",
        "    test_df['pred_output'].str.lower().str.strip()\n",
        ").sum()\n",
        "\n",
        "# ------------- BERTScore (直接計算) -------------\n",
        "import evaluate, numpy as np\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "# 空値対策\n",
        "pred_inputs  = test_df['pred_input'].fillna(\"\").tolist()\n",
        "true_inputs  = test_df['input'].fillna(\"\").tolist()\n",
        "pred_outputs = test_df['pred_output'].fillna(\"\").tolist()\n",
        "true_outputs = test_df['output'].fillna(\"\").tolist()\n",
        "\n",
        "bert_in  = bertscore.compute(predictions=pred_inputs,  references=true_inputs,  lang=\"en\")['f1']\n",
        "bert_out = bertscore.compute(predictions=pred_outputs, references=true_outputs, lang=\"en\")['f1']\n",
        "# 追加セル\n",
        "!pip -q install bert_score transformers torch\n",
        "\n",
        "# ------------- まとめ表示 -------------\n",
        "print(f\"\\n📊 Quick Test Results:\")\n",
        "print(f\"Input EMA:   {input_matches}/{len(test_df)} \"\n",
        "      f\"({input_matches/len(test_df):.1%})\")\n",
        "print(f\"Output EMA:  {output_matches}/{len(test_df)} \"\n",
        "      f\"({output_matches/len(test_df):.1%})\")\n",
        "print(f\"Average EMA: {(input_matches + output_matches)/(2*len(test_df)):.1%}\")\n",
        "print(f\"BERTScore Input F1:  {np.mean(bert_in):.3f}\")\n",
        "print(f\"BERTScore Output F1: {np.mean(bert_out):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx_hjT3lqUEm",
        "outputId": "868f6bf5-bceb-40ba-9c9e-a244c2357d9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Running small test with 10 samples...\n",
            "🚀 Starting batch prediction on val split...\n",
            "Found 744 recipe files in train\n",
            "📊 Processing 10 recipe steps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch processing complete! Processed 10 steps\n",
            "📊 Success rate: 10/10 (100.0%)\n",
            "\n",
            "📋 Sample Results:\n",
            "                                   instructions  actions  \\\n",
            "0                    preheat the oven to 400f .  preheat   \n",
            "1  heat the oil in a large non stick frying pan     heat   \n",
            "2           add the onion , pepper and zucchini      add   \n",
            "3        saute over a medium heat for 4 5mins .    saute   \n",
            "4                                 add the herbs      add   \n",
            "\n",
            "                                   input  \\\n",
            "0                                     NA   \n",
            "1                                    oil   \n",
            "2  (onion; pepper; zucchini; heated_oil)   \n",
            "3               vegetables in heated_oil   \n",
            "4     (herbs; sauteed vegetable mixture)   \n",
            "\n",
            "                                           output               pred_input  \\\n",
            "0                                              NA                       NA   \n",
            "1                                      heated_oil                      oil   \n",
            "2  onion, pepper and zucchini added to heated oil  onion; pepper; zucchini   \n",
            "3                       sauteed vegetable mixture                       NA   \n",
            "4        herbs added to sauteed vegetable mixture                    herbs   \n",
            "\n",
            "               pred_output  \n",
            "0                       NA  \n",
            "1               heated oil  \n",
            "2  onion; pepper; zucchini  \n",
            "3                       NA  \n",
            "4                    herbs  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Quick Test Results:\n",
            "Input EMA:   2/10 (20.0%)\n",
            "Output EMA:  1/10 (10.0%)\n",
            "Average EMA: 15.0%\n",
            "BERTScore Input F1:  0.884\n",
            "BERTScore Output F1: 0.864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run full batch processing (uncomment when ready)\n",
        "# WARNING: This will process the entire validation split and may take 30-60 minutes\n",
        "# and cost several dollars in API calls\n",
        "\n",
        "print(\"⚠️  Ready to run full batch processing on validation split\")\n",
        "print(\"💰 Estimated cost: $3-10 depending on dataset size\")\n",
        "print(\"⏱️  Estimated time: 30-60 minutes\")\n",
        "print(\"\\n🔧 To run full processing, uncomment the lines below:\")\n",
        "print(\"# full_df = run_batch_predictions(split='val')\")\n",
        "print(\"# full_df.to_csv('gpt4_predictions_full.csv', index=False)\")\n",
        "print(\"# print('💾 Full results saved to gpt4_predictions_full.csv')\")\n",
        "\n",
        "# Uncomment these lines when ready to run full experiment:\n",
        "full_df = run_batch_predictions(split=\"val\")\n",
        "full_df.to_csv(\"gpt4_predictions_full.csv\", index=False)\n",
        "print(\"💾 Full results saved to gpt4_predictions_full.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFhUDXw9w-g3",
        "outputId": "d07481c5-cde5-4dcb-85ca-e9984471389a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Ready to run full batch processing on validation split\n",
            "💰 Estimated cost: $3-10 depending on dataset size\n",
            "⏱️  Estimated time: 30-60 minutes\n",
            "\n",
            "🔧 To run full processing, uncomment the lines below:\n",
            "# full_df = run_batch_predictions(split='val')\n",
            "# full_df.to_csv('gpt4_predictions_full.csv', index=False)\n",
            "# print('💾 Full results saved to gpt4_predictions_full.csv')\n",
            "🚀 Starting batch prediction on val split...\n",
            "Found 744 recipe files in train\n",
            "📊 Processing 3069 recipe steps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   2%|▏         | 50/3069 [00:47<46:11,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   3%|▎         | 100/3069 [01:48<1:39:08,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   5%|▍         | 150/3069 [02:39<50:37,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   7%|▋         | 200/3069 [03:27<46:47,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   8%|▊         | 250/3069 [04:18<47:01,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  10%|▉         | 300/3069 [05:09<47:42,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  11%|█▏        | 350/3069 [05:59<43:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  13%|█▎        | 400/3069 [06:53<59:27,  1.34s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  15%|█▍        | 450/3069 [07:41<42:45,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  16%|█▋        | 500/3069 [08:34<39:06,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  18%|█▊        | 550/3069 [09:24<38:12,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  20%|█▉        | 600/3069 [10:09<38:46,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  21%|██        | 650/3069 [11:02<37:10,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  23%|██▎       | 700/3069 [11:53<34:58,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  24%|██▍       | 750/3069 [12:44<42:30,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  26%|██▌       | 800/3069 [13:37<47:39,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  28%|██▊       | 850/3069 [14:29<36:44,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  29%|██▉       | 900/3069 [15:25<35:42,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  31%|███       | 950/3069 [16:22<36:09,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  33%|███▎      | 1000/3069 [17:14<30:21,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  34%|███▍      | 1050/3069 [18:07<41:45,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  36%|███▌      | 1100/3069 [18:58<36:42,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  37%|███▋      | 1150/3069 [19:46<29:38,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  39%|███▉      | 1200/3069 [20:41<29:51,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  41%|████      | 1250/3069 [21:41<27:13,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  42%|████▏     | 1300/3069 [22:29<24:26,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  44%|████▍     | 1350/3069 [23:17<28:18,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  46%|████▌     | 1400/3069 [24:19<31:00,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  47%|████▋     | 1450/3069 [25:07<29:03,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  49%|████▉     | 1500/3069 [25:59<25:14,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  51%|█████     | 1550/3069 [26:49<24:40,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  52%|█████▏    | 1600/3069 [27:43<28:24,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  54%|█████▍    | 1650/3069 [28:35<22:13,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  55%|█████▌    | 1700/3069 [29:24<22:48,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  57%|█████▋    | 1750/3069 [30:16<23:15,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  59%|█████▊    | 1800/3069 [31:05<20:27,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  60%|██████    | 1850/3069 [31:55<23:29,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  62%|██████▏   | 1900/3069 [32:45<18:35,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  64%|██████▎   | 1950/3069 [33:39<17:24,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 1950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  65%|██████▌   | 2000/3069 [34:30<16:04,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  67%|██████▋   | 2050/3069 [35:19<16:10,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  68%|██████▊   | 2100/3069 [36:07<17:37,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  70%|███████   | 2150/3069 [36:59<16:05,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  72%|███████▏  | 2200/3069 [37:51<12:58,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  73%|███████▎  | 2250/3069 [38:39<13:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  75%|███████▍  | 2300/3069 [39:31<12:04,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  77%|███████▋  | 2350/3069 [40:20<12:53,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  78%|███████▊  | 2400/3069 [41:14<11:18,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  80%|███████▉  | 2450/3069 [42:06<19:44,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  81%|████████▏ | 2500/3069 [42:55<09:19,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  83%|████████▎ | 2550/3069 [43:44<08:01,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  85%|████████▍ | 2600/3069 [44:35<06:55,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  86%|████████▋ | 2650/3069 [45:22<06:20,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  88%|████████▊ | 2700/3069 [46:08<05:35,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  90%|████████▉ | 2750/3069 [47:00<04:48,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  91%|█████████ | 2800/3069 [47:53<04:10,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  93%|█████████▎| 2850/3069 [48:43<03:15,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  94%|█████████▍| 2900/3069 [49:36<02:40,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  96%|█████████▌| 2950/3069 [50:26<02:08,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 2950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  98%|█████████▊| 3000/3069 [51:15<01:08,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  99%|█████████▉| 3050/3069 [52:05<00:19,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved temporary results at step 3050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|██████████| 3069/3069 [52:25<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch processing complete! Processed 3069 steps\n",
            "📊 Success rate: 3069/3069 (100.0%)\n",
            "💾 Full results saved to gpt4_predictions_full.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCalculator:\n",
        "    \"\"\"Computes evaluation metrics for model predictions.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize evaluation libraries\n",
        "        self.rouge = evaluate.load(\"rouge\")\n",
        "        self.bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "    def calculate_ema(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate Exact Match Accuracy with normalization.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted strings\n",
        "            references: List of ground truth strings\n",
        "\n",
        "        Returns:\n",
        "            float: EMA score (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        if len(predictions) != len(references):\n",
        "            raise ValueError(\"Predictions and references must have same length\")\n",
        "\n",
        "        matches = 0\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            # Normalize strings for comparison\n",
        "            pred_norm = self._normalize_string(pred)\n",
        "            ref_norm = self._normalize_string(ref)\n",
        "\n",
        "            if pred_norm == ref_norm:\n",
        "                matches += 1\n",
        "\n",
        "        return matches / len(predictions) if predictions else 0.0\n",
        "\n",
        "    def calculate_rouge_l(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate Rouge-L score.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted strings\n",
        "            references: List of ground truth strings\n",
        "\n",
        "        Returns:\n",
        "            float: Rouge-L F1 score\n",
        "        \"\"\"\n",
        "        if not predictions or not references:\n",
        "            return 0.0\n",
        "\n",
        "        # Handle empty predictions\n",
        "        clean_predictions = [pred if pred else \"\" for pred in predictions]\n",
        "        clean_references = [ref if ref else \"\" for ref in references]\n",
        "\n",
        "        try:\n",
        "            results = self.rouge.compute(\n",
        "                predictions=clean_predictions,\n",
        "                references=clean_references\n",
        "            )\n",
        "            return results[\"rougeL\"]\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Rouge-L calculation error: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_bertscore(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate BERTScore F1.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted strings\n",
        "            references: List of ground truth strings\n",
        "\n",
        "        Returns:\n",
        "            float: Average BERTScore F1\n",
        "        \"\"\"\n",
        "        if not predictions or not references:\n",
        "            return 0.0\n",
        "\n",
        "        # Handle empty predictions\n",
        "        clean_predictions = [pred if pred else \"empty\" for pred in predictions]\n",
        "        clean_references = [ref if ref else \"empty\" for ref in references]\n",
        "\n",
        "        try:\n",
        "            results = self.bertscore.compute(\n",
        "                predictions=clean_predictions,\n",
        "                references=clean_references,\n",
        "                lang=\"en\"\n",
        "            )\n",
        "            return np.mean(results[\"f1\"])\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ BERTScore calculation error: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _normalize_string(self, text: str) -> str:\n",
        "        \"\"\"Normalize string for comparison.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "\n",
        "        # Convert to lowercase and strip whitespace\n",
        "        text = text.lower().strip()\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Handle special cases\n",
        "        if text in ['na', 'n/a', 'none', '']:\n",
        "            return ''\n",
        "\n",
        "        return text\n",
        "\n",
        "    def evaluate_predictions(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Comprehensive evaluation of predictions.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with predictions and ground truth\n",
        "\n",
        "        Returns:\n",
        "            Dict: All evaluation metrics\n",
        "        \"\"\"\n",
        "        print(\"📊 Calculating evaluation metrics...\")\n",
        "\n",
        "        # Extract predictions and references\n",
        "        pred_inputs = df['pred_input'].tolist()\n",
        "        true_inputs = df['input'].tolist()\n",
        "        pred_outputs = df['pred_output'].tolist()\n",
        "        true_outputs = df['output'].tolist()\n",
        "\n",
        "        # Calculate EMA\n",
        "        ema_input = self.calculate_ema(pred_inputs, true_inputs)\n",
        "        ema_output = self.calculate_ema(pred_outputs, true_outputs)\n",
        "        ema_avg = (ema_input + ema_output) / 2\n",
        "\n",
        "        # Calculate Rouge-L\n",
        "        rouge_input = self.calculate_rouge_l(pred_inputs, true_inputs)\n",
        "        rouge_output = self.calculate_rouge_l(pred_outputs, true_outputs)\n",
        "\n",
        "        # Calculate BERTScore (focus on outputs as they're more complex)\n",
        "        bertscore_output = self.calculate_bertscore(pred_outputs, true_outputs)\n",
        "\n",
        "        metrics = {\n",
        "            'ema_input': ema_input,\n",
        "            'ema_output': ema_output,\n",
        "            'ema_average': ema_avg,\n",
        "            'rouge_l_input': rouge_input,\n",
        "            'rouge_l_output': rouge_output,\n",
        "            'bertscore_f1': bertscore_output,\n",
        "            'total_samples': len(df)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Initialize metrics calculator\n",
        "metrics_calc = MetricsCalculator()\n",
        "print(\"✅ Metrics calculator initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGUJ9kEUxja3",
        "outputId": "b434c375-e050-48bf-fdd0-afe1f01ce7b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Metrics calculator initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the test results\n",
        "print(\"📊 Evaluating test results...\")\n",
        "test_metrics = metrics_calc.evaluate_predictions(test_df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST RESULTS (10 samples)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"EMA Input:        {test_metrics['ema_input']:.1%}\")\n",
        "print(f\"EMA Output:       {test_metrics['ema_output']:.1%}\")\n",
        "print(f\"EMA Average:      {test_metrics['ema_average']:.1%}\")\n",
        "print(f\"Rouge-L Input:    {test_metrics['rouge_l_input']:.3f}\")\n",
        "print(f\"Rouge-L Output:   {test_metrics['rouge_l_output']:.3f}\")\n",
        "print(f\"BERTScore F1:     {test_metrics['bertscore_f1']:.3f}\")\n",
        "print(f\"Total Samples:    {test_metrics['total_samples']}\")\n",
        "\n",
        "# Compare with paper benchmarks\n",
        "paper_benchmarks = {\n",
        "    'ema_average': 0.267,  # 26.7%\n",
        "    'rouge_l_input': 0.514,  # 51.4\n",
        "    'rouge_l_output': 0.523  # 52.3\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON WITH PAPER BENCHMARKS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<20} {'Test':<10} {'Paper':<10} {'Status':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric, paper_value in paper_benchmarks.items():\n",
        "    test_value = test_metrics[metric]\n",
        "    status = \"✅ BETTER\" if test_value >= paper_value else \"❌ LOWER\"\n",
        "    print(f\"{metric:<20} {test_value:<10.3f} {paper_value:<10.3f} {status}\")\n",
        "\n",
        "print(\"\\n⚠️  Note: These are results on only 10 test samples.\")\n",
        "print(\"📈 Run full evaluation for meaningful comparison with paper.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFPnKogpx8nv",
        "outputId": "ebd4b888-cb40-4298-e24a-342993a6b5f3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Evaluating test results...\n",
            "📊 Calculating evaluation metrics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TEST RESULTS (10 samples)\n",
            "============================================================\n",
            "EMA Input:        20.0%\n",
            "EMA Output:       10.0%\n",
            "EMA Average:      15.0%\n",
            "Rouge-L Input:    0.457\n",
            "Rouge-L Output:   0.378\n",
            "BERTScore F1:     0.864\n",
            "Total Samples:    10\n",
            "\n",
            "============================================================\n",
            "COMPARISON WITH PAPER BENCHMARKS\n",
            "============================================================\n",
            "Metric               Test       Paper      Status    \n",
            "------------------------------------------------------------\n",
            "ema_average          0.150      0.267      ❌ LOWER\n",
            "rouge_l_input        0.457      0.514      ❌ LOWER\n",
            "rouge_l_output       0.378      0.523      ❌ LOWER\n",
            "\n",
            "⚠️  Note: These are results on only 10 test samples.\n",
            "📈 Run full evaluation for meaningful comparison with paper.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_full_results(csv_path: str = \"gpt4_predictions_full.csv\"):\n",
        "    \"\"\"Evaluate full results and compare with paper benchmarks.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to the CSV file with full predictions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load results\n",
        "        df = pd.read_csv(csv_path, keep_default_na=False)\n",
        "        print(f\"📁 Loaded {len(df)} predictions from {csv_path}\")\n",
        "\n",
        "        text_cols = ['input', 'output', 'pred_input', 'pred_output']\n",
        "        df[text_cols] = df[text_cols].fillna('').astype(str)\n",
        "        # Calculate metrics\n",
        "        metrics = metrics_calc.evaluate_predictions(df)\n",
        "\n",
        "        # Paper benchmarks\n",
        "        paper_benchmarks = {\n",
        "            'EMA Average': {'our': metrics['ema_average'], 'paper': 0.267, 'format': '.1%'},\n",
        "            'Rouge-L Input': {'our': metrics['rouge_l_input'], 'paper': 0.514, 'format': '.3f'},\n",
        "            'Rouge-L Output': {'our': metrics['rouge_l_output'], 'paper': 0.523, 'format': '.3f'},\n",
        "            'BERTScore F1': {'our': metrics['bertscore_f1'], 'paper': None, 'format': '.3f'}\n",
        "        }\n",
        "\n",
        "        # Display comprehensive results\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"🎯 FINAL RESULTS - PAPER REPRODUCTION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"📊 Dataset: PizzaCommonSense validation split ({metrics['total_samples']} samples)\")\n",
        "        print(f\"🤖 Model: GPT-4 Turbo (gpt-4o-mini) with Chain-of-Thought\")\n",
        "        print(f\"📅 Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"{'Metric':<20} {'Our Result':<15} {'Paper Benchmark':<18} {'Status':<15} {'Difference'}\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        success_count = 0\n",
        "        total_comparable = 0\n",
        "\n",
        "        for metric_name, values in paper_benchmarks.items():\n",
        "            our_val = values['our']\n",
        "            paper_val = values['paper']\n",
        "            fmt = values['format']\n",
        "\n",
        "            if paper_val is not None:\n",
        "                total_comparable += 1\n",
        "                diff = our_val - paper_val\n",
        "                status = \"✅ BETTER\" if our_val >= paper_val else \"❌ LOWER\"\n",
        "                if our_val >= paper_val:\n",
        "                    success_count += 1\n",
        "\n",
        "                our_str = f\"{our_val:{fmt}}\"\n",
        "                paper_str = f\"{paper_val:{fmt}}\"\n",
        "                diff_str = f\"{diff:+{fmt}}\"\n",
        "            else:\n",
        "                status = \"📊 NEW METRIC\"\n",
        "                our_str = f\"{our_val:{fmt}}\"\n",
        "                paper_str = \"N/A\"\n",
        "                diff_str = \"N/A\"\n",
        "\n",
        "            print(f\"{metric_name:<20} {our_str:<15} {paper_str:<18} {status:<15} {diff_str}\")\n",
        "\n",
        "        # Success summary\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"🏆 REPRODUCTION SUCCESS SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        success_rate = success_count / total_comparable if total_comparable > 0 else 0\n",
        "        print(f\"✅ Metrics meeting/exceeding paper: {success_count}/{total_comparable} ({success_rate:.1%})\")\n",
        "\n",
        "        if success_count > 0:\n",
        "            print(\"🎉 SUCCESS: At least one metric meets the paper benchmark!\")\n",
        "            print(\"📈 Reproduction experiment successful!\")\n",
        "        else:\n",
        "            print(\"⚠️  No metrics exceed paper benchmarks\")\n",
        "            print(\"🔍 Consider adjusting prompts or trying different techniques\")\n",
        "\n",
        "        # Detailed breakdown\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"📋 DETAILED BREAKDOWN\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"EMA Input:        {metrics['ema_input']:.1%}\")\n",
        "        print(f\"EMA Output:       {metrics['ema_output']:.1%}\")\n",
        "        print(f\"EMA Average:      {metrics['ema_average']:.1%} (Target: 26.7%)\")\n",
        "        print(f\"Rouge-L Input:    {metrics['rouge_l_input']:.3f} (Target: 0.514)\")\n",
        "        print(f\"Rouge-L Output:   {metrics['rouge_l_output']:.3f} (Target: 0.523)\")\n",
        "        print(f\"BERTScore F1:     {metrics['bertscore_f1']:.3f} (New metric)\")\n",
        "\n",
        "        # Save metrics to JSON\n",
        "        metrics_file = \"final_metrics.json\"\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        print(f\"\\n💾 Detailed metrics saved to {metrics_file}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ File {csv_path} not found. Run full batch processing first.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error evaluating results: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ Full evaluation function ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNpGC5B6x-71",
        "outputId": "c4cb454a-e303-4041-c136-2021b228bea6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Full evaluation function ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run full evaluation (uncomment after running full batch processing)\n",
        "print(\"🎯 Ready to evaluate full results\")\n",
        "\n",
        "print(\"\\n🔧 To run full evaluation, uncomment the line below:\")\n",
        "print(\"# final_metrics = evaluate_full_results('gpt4_predictions_full.csv')\")\n",
        "\n",
        "# Uncomment this line after running full batch processing:\n",
        "final_metrics = evaluate_full_results(\"gpt4_predictions_full.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T8Z6fRZyBrv",
        "outputId": "f5b23cd2-d525-4167-ca66-f9950da5bc80"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Ready to evaluate full results\n",
            "\n",
            "🔧 To run full evaluation, uncomment the line below:\n",
            "# final_metrics = evaluate_full_results('gpt4_predictions_full.csv')\n",
            "📁 Loaded 3069 predictions from gpt4_predictions_full.csv\n",
            "📊 Calculating evaluation metrics...\n",
            "\n",
            "================================================================================\n",
            "🎯 FINAL RESULTS - PAPER REPRODUCTION\n",
            "================================================================================\n",
            "📊 Dataset: PizzaCommonSense validation split (3069 samples)\n",
            "🤖 Model: GPT-4 Turbo (gpt-4o-mini) with Chain-of-Thought\n",
            "📅 Evaluation Date: 2025-07-21 17:54\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Metric               Our Result      Paper Benchmark    Status          Difference\n",
            "--------------------------------------------------------------------------------\n",
            "EMA Average          16.6%           26.7%              ❌ LOWER         -10.1%\n",
            "Rouge-L Input        0.446           0.514              ❌ LOWER         -0.068\n",
            "Rouge-L Output       0.377           0.523              ❌ LOWER         -0.146\n",
            "BERTScore F1         0.867           N/A                📊 NEW METRIC    N/A\n",
            "\n",
            "================================================================================\n",
            "🏆 REPRODUCTION SUCCESS SUMMARY\n",
            "================================================================================\n",
            "✅ Metrics meeting/exceeding paper: 0/3 (0.0%)\n",
            "⚠️  No metrics exceed paper benchmarks\n",
            "🔍 Consider adjusting prompts or trying different techniques\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "📋 DETAILED BREAKDOWN\n",
            "--------------------------------------------------------------------------------\n",
            "EMA Input:        18.7%\n",
            "EMA Output:       14.5%\n",
            "EMA Average:      16.6% (Target: 26.7%)\n",
            "Rouge-L Input:    0.446 (Target: 0.514)\n",
            "Rouge-L Output:   0.377 (Target: 0.523)\n",
            "BERTScore F1:     0.867 (New metric)\n",
            "\n",
            "💾 Detailed metrics saved to final_metrics.json\n"
          ]
        }
      ]
    }
  ]
}