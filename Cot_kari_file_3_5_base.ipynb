{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I5WCJhQtmjFW"
      },
      "outputs": [],
      "source": [
        "# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å…¥ã‚Œç›´ã— ---\n",
        "!pip uninstall -y openai -q        # â† ã„ã£ãŸã‚“å‰Šé™¤\n",
        "!pip -q install openai==0.27.2 tqdm rouge-score evaluate pandas tiktoken\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai | grep Version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB3u8PwwjH6L",
        "outputId": "b7a3a923-4f23-4dbc-b057-52b561c8dc16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import getpass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Iterator, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import openai\n",
        "import evaluate\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3hXZtETmpbZ",
        "outputId": "95b654fe-4347-459b-cbfa-d35ca78ba141"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure API key input\n",
        "print(\"ğŸ”‘ Please enter your OpenAI API key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"API Key: \")\n",
        "\n",
        "# Verify API key is set\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"âœ… API key configured successfully!\")\n",
        "else:\n",
        "    print(\"âŒ API key not set. Please run this cell again.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YatbNeNvm5tV",
        "outputId": "f3aa1d3d-91f7-496f-9947-264434469b41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”‘ Please enter your OpenAI API key:\n",
            "API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… API key configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls /content/drive/MyDrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65QaZpstnjA3",
        "outputId": "e917042f-4f2c-451b-d7d0-634a2cf4831c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'2019ä¸–ç•Œå„å›½å®˜è£½åœ°å›³ç™ºè¡¨ï¼ˆ3E17é–¢æ·µä¹‹ï¼‰ (1).pptx'\n",
            " 2019ä¸–ç•Œå„å›½å®˜è£½åœ°å›³ç™ºè¡¨ï¼ˆ3E17é–¢æ·µä¹‹ï¼‰.pptx\n",
            " 2019ä¸–ç•Œå„å›½å®˜è£½åœ°å›³ç™ºè¡¨aaï¼ˆ3E17é–¢æ·µä¹‹ï¼‰.pptx\n",
            "'Activity Diagram.drawio.png'\n",
            " adult.zip\n",
            " bc_01-17.zip\n",
            " bc_18-23.zip\n",
            " bc_24-27.zip\n",
            " bc_28-33.zip\n",
            " BGMshuu\n",
            " blind_test\n",
            "'Career Event by FAST OFFER.gform'\n",
            "'Class Diagram.drawio.png'\n",
            "'Class Diagram_White.drawio.png'\n",
            " Colab\n",
            "'Colab Notebooks'\n",
            " Component_sht.drawio.png\n",
            "'Coronavirus (COVID-19) records (1).pdf'\n",
            " CV_Fuchiyuki.docx\n",
            " data_weights\n",
            "'Emailing Offer Letter View.pdf'\n",
            "'Extracting Actionable Knowledge from Cooking Recipes: An LLM Approach to Commonsense Reasoning and Graphical RepresentationI.gdoc'\n",
            "'family mart in oomiya.csv'\n",
            "'File to replace '\n",
            "'Fuchiyuki Seki.pdf'\n",
            " GISç™ºè¡¨1A18é–¢æ·µä¹‹.pptx\n",
            "'Google AI Studio'\n",
            " hihihi\n",
            " IMG_1826.MOV\n",
            " IMG_1827.MOV\n",
            " IMG_9927.jpeg\n",
            " japan_ver80_prefecture.shp\n",
            "'Khash Bonus'\n",
            "'Level_3_Week_14_Free_Lesson_copy (1).gslides'\n",
            " Level_3_Week_14_Free_Lesson_copy.gslides\n",
            "'Literature Review.gdoc'\n",
            "'Liz to Aoi Tori Movie.mp4'\n",
            "'new doc 2019-04-18 14.14.14.pdf'\n",
            " Persona.gslides\n",
            "'persona-template (1).gslides'\n",
            " pizza_recipes\n",
            " PPC_test.ipynb\n",
            " PPC_test_new.ipynb\n",
            " PPC_weight\n",
            "'Project Plan.gdoc'\n",
            " Prompt_for_eSNLI.gdoc\n",
            " recipe1m\n",
            "'Recipe Commonsense Reasoning Project Details'$'\\n'' (1).gdoc'\n",
            "'Recipe Commonsense Reasoning Project Details'$'\\n''.gdoc'\n",
            "'Recycling and CO2 emissions helper applicationï¼ˆå›ç­”ï¼‰.gsheet'\n",
            " Resources.docx\n",
            "'SnapCrab_Piriform Speccy_2019-1-31_2-58-40_No-00.png'\n",
            "'SnapCrab_Piriform Speccy_2019-1-31_3-1-58_No-00.png'\n",
            " src\n",
            " train\n",
            " Transcript.pdf\n",
            " unet_weights.pth\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled presentation (1).gslides'\n",
            "'Untitled presentation (2).gslides'\n",
            "'Untitled presentation.gslides'\n",
            "'Untitled spreadsheet.gsheet'\n",
            " utils.py\n",
            " veg_recipe1m_output\n",
            " VID_20190109_175011.mp4\n",
            " VID_20190109_175016.mp4\n",
            " VID_20190109_175633.mp4\n",
            " VID_20190109_180103.mp4\n",
            " Weights\n",
            "'ä¾‹é¡Œ0(ãƒã‚™ãƒ¬ãƒ¼ãƒ›ã‚™ãƒ¼ãƒ«éƒ¨å“¡åç°¿â‘ )_20171004.xlsm'\n",
            "'ä¾‹é¡Œ1(æ•™p81)é †æ¬¡æ§‹é€ _20171101.xlsm'\n",
            "'ä¾‹é¡Œ1(æ•™p84)é€æ¬¡æ¢ç´¢_20171101.xlsm'\n",
            "'ä¾‹é¡Œ1(æ•™p84)é€æ¬¡æ¢ç´¢(å¿œç”¨)_20171115.xlsm'\n",
            "'ä¾‹é¡Œ2(æ•™p82)é¸æŠæ§‹é€ _20171101.xlsm'\n",
            "'ä¾‹é¡Œ2(æ•™p82)é¸æŠæ§‹é€ å¿œç”¨(åˆæ ¼è¡¨ç¤º)_20171101.xlsm'\n",
            "'ä¾‹é¡Œ2(æ•™p85)é€æ¬¡æ¢ç´¢ã®æ´»ç”¨_20171108.xlsm'\n",
            "'ä¾‹é¡Œ3-1(å•†å“åˆ†é¡å£²ä¸Šé«˜ä¸€è¦§è¡¨)_20171004.xlsm'\n",
            "'ä¾‹é¡Œ3-2(é€±åˆŠå£²ä¸Šä¸€è¦§è¡¨)_20171004.xlsm'\n",
            "'ä¾‹é¡Œ3-3(æ™‚åˆ»ã®è¡¨ç¤º)_20171004.xlsm'\n",
            "'ä¾‹é¡Œ3-4(ã‚·ãƒ¼ãƒˆã®è¿½åŠ )_20171011.xlsm'\n",
            "'ä¾‹é¡Œ3-5(åˆ¤å®š)_20171011.xlsm'\n",
            "'ä¾‹é¡Œ3-6(ãƒ’ã‚šã‚µã‚™å£²ä¸Šä¸€è¦§è¡¨)_20171025.xlsm'\n",
            "'ä¾‹é¡Œ3-7(ãƒ†ãƒ‹ã‚¹ã‚³ãƒ¼ãƒˆ)_20171204.xlsm'\n",
            "'ä¾‹é¡Œ3-8(ãƒ•ã‚©ãƒ¼ãƒ ã®ä½œæˆ1)_20171108.xlsm'\n",
            "'ä¾‹é¡Œ3-9(ãƒ•ã‚©ãƒ¼ãƒ ã®ä½œæˆ)_20171108.xlsm'\n",
            "'ä¾‹é¡Œ3(æ•™p83)ç¹°ã‚Šè¿”ã—æ§‹é€ _20171101.xlsm'\n",
            "'ä¾‹é¡Œ3(æ•™p83)ç¹°ã‚Šè¿”ã—æ§‹é€ (å¿œç”¨)_20171101.xlsm'\n",
            "'ä¾‹é¡Œ3(æ•™p86)äºŒåˆ†æ¢ç´¢_20171115.xlsm'\n",
            "'ä¾‹é¡Œ3(æ•™p86)äºŒåˆ†æ¢ç´¢(å¿œç”¨)_20171115.xlsm'\n",
            "'ä¾‹é¡Œ4(æ•™p88)äº¤æ›æ³•ã®ãƒ•ã‚šãƒ­ã‚¯ã‚™ãƒ©ãƒ _20171122.xlsm'\n",
            "'ä¾‹é¡Œ4(æ•™p88)äº¤æ›æ³•ã®ãƒ•ã‚šãƒ­ã‚¯ã‚™ãƒ©ãƒ _20171122.xlsx'\n",
            "'ä¾‹é¡Œ5(æ•™p90)äº¤æ›æ³•ã®ãƒ•ã‚šãƒ­ã‚¯ã‚™ãƒ©ãƒ _20171122.xlsm'\n",
            "'ä¾‹é¡Œ5(æ•™p90)å¾©å…ƒäº¤æ›æ³•_20171122.xlsm'\n",
            " æƒ…å ±ã®ç§‘å­¦_æˆæ¥­è³‡æ–™_20180101.pdf\n",
            "'æƒ…å ±ã®ç§‘å­¦_æˆæ¥­è³‡æ–™(å•é¡Œè§£æ±ºã®è€ƒãˆ ç¬¬ï¼“ç« )_20170906.pdf'\n",
            "'ç„¡é¡Œã®ãƒˆã‚™ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (1).gdoc'\n",
            " ç„¡é¡Œã®ãƒˆã‚™ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ.gdoc\n",
            " ç„¡é¡Œã®ãƒ•ã‚©ãƒ¼ãƒ .gform\n",
            "'çŸ¥è­˜é€£é–ï¼ˆChain of Knowledgeï¼‰ã‚’ç”¨ã„ãŸæ‰‹ç¶šãçš„å¸¸è­˜æ¨è«–ã®å†è©•ä¾¡ï¼šPizzaCommonsenseãƒ†ã‚™ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢ã™ã‚‹ç ”ç©¶ææ¡ˆ.gdoc'\n",
            "'ç·´ç¿’å•é¡Œ(RGB)_20171025.xlsm'\n",
            "'ç·´ç¿’å•é¡Œ(ã‚·ãƒ¼ãƒˆè¿½åŠ æ—¥ä»˜ã)_20171011.xlsm'\n",
            "'ç·´ç¿’å•é¡Œ(ãƒŠãƒ³ãƒã‚™ãƒªãƒ³ã‚¯ã‚™)_20171025.xlsm'\n",
            "'ç·´ç¿’å•é¡Œ(åˆå¦åˆ¤å®š)_20171011.xlsm'\n",
            "'ç·´ç¿’å•é¡Œ(æ‹…å½“è€…åå…¥åŠ›)_20171011.xlsm'\n",
            "'ç·´ç¿’å•é¡Œ(èƒŒæ™¯è‰²ã®å¤‰æ›´)_20171011.xlsm'\n",
            " è‡ªåˆ†å².docx\n",
            " é«˜å¿ å®Ÿåº¦ãƒ˜ã‚™ã‚·ã‚™ã‚¿ãƒªã‚¢ãƒ³ãƒ¬ã‚·ãƒ’ã‚šã‚³ãƒ¼ãƒã‚šã‚¹ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨çŸ¥è­˜ã®é€£é–ãƒ•ã‚šãƒ­ãƒ³ãƒ•ã‚šãƒ†ã‚£ãƒ³ã‚¯ã‚™ã«ã‚ˆã‚‹æ‰‹ç¶šãçš„ã‚³ãƒ¢ãƒ³ã‚»ãƒ³ã‚¹æ¨è«–ã®å°å‡ºã«é–¢ã™ã‚‹æ–¹æ³•è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯.gdoc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class PizzaDataLoader:\n",
        "    \"\"\"\n",
        "    Handles folder traversal and JSON parsing for PizzaCommonSense dataset.\n",
        "    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® data_path ã‚’ Google Drive å†…ã® MyDrive/train ãƒ•ã‚©ãƒ«ãƒ€ã«å¤‰æ›´ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path: str = \"/content/drive/MyDrive/train\"):\n",
        "        self.data_path = Path(data_path)\n",
        "        if not self.data_path.exists():\n",
        "            raise FileNotFoundError(f\"Data path {data_path} does not exist\")\n",
        "        print(f\"âœ… Using data path: {self.data_path}\")\n",
        "\n",
        "    def iter_tables(self, split: str = None) -> Iterator[dict]:\n",
        "        \"\"\"\n",
        "        split å¼•æ•°ã¯ä¸è¦ (ã™ã§ã« train ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã‚’æƒ³å®š)ã€‚\n",
        "        ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã™ã¹ã¦ã® .txt ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
        "        \"\"\"\n",
        "        txt_files = list(self.data_path.glob(\"*.txt\"))\n",
        "        print(f\"Found {len(txt_files)} recipe files in {self.data_path.name}\")\n",
        "\n",
        "        for file_path in txt_files:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                if 'table' in data and isinstance(data['table'], list):\n",
        "                    for step in data['table']:\n",
        "                        if self._validate_step(step):\n",
        "                            yield step\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  Skipping {file_path.name}: {e}\")\n",
        "\n",
        "    def _validate_step(self, step: dict) -> bool:\n",
        "        required = ['instructions', 'actions', 'input', 'output']\n",
        "        return all(k in step for k in required)\n",
        "\n",
        "    def get_split_stats(self) -> dict:\n",
        "        steps = list(self.iter_tables())\n",
        "        return {\n",
        "            'total_steps': len(steps),\n",
        "            'unique_actions': len(set(s['actions'] for s in steps)),\n",
        "            'avg_instruction_length': np.mean([len(s['instructions']) for s in steps])\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Fj-_tkmdnC96"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ‰ãƒ©ã‚¤ãƒ–ãƒã‚¦ãƒ³ãƒˆå¾Œã«å®Ÿè¡Œ\n",
        "data_loader = PizzaDataLoader()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ /content/drive/MyDrive/train ã‚’å‚ç…§\n",
        "stats = data_loader.get_split_stats()\n",
        "print(\"ğŸ“Š Dataset stats:\", stats)\n",
        "\n",
        "# ã‚µãƒ³ãƒ—ãƒ«3ä»¶ã‚’è¡¨ç¤º\n",
        "for i, step in enumerate(data_loader.iter_tables()):\n",
        "    print(f\"\\nStep {i+1}\")\n",
        "    print(\" Instructions:\", step['instructions'])\n",
        "    print(\" Actions:     \", step['actions'])\n",
        "    print(\" Input:       \", step['input'])\n",
        "    print(\" Output:      \", step['output'])\n",
        "    if i >= 2:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBsPBJOlbqbv",
        "outputId": "69802644-7bb4-4b66-f4d7-471983b30956"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Using data path: /content/drive/MyDrive/train\n",
            "Found 744 recipe files in train\n",
            "ğŸ“Š Dataset stats: {'total_steps': 3069, 'unique_actions': 168, 'avg_instruction_length': np.float64(40.33919843597263)}\n",
            "Found 744 recipe files in train\n",
            "\n",
            "Step 1\n",
            " Instructions: preheat the oven to 400f .\n",
            " Actions:      preheat\n",
            " Input:        NA\n",
            " Output:       NA\n",
            "\n",
            "Step 2\n",
            " Instructions: heat the oil in a large non stick frying pan\n",
            " Actions:      heat\n",
            " Input:        oil\n",
            " Output:       heated_oil\n",
            "\n",
            "Step 3\n",
            " Instructions: add the onion , pepper and zucchini\n",
            " Actions:      add\n",
            " Input:        (onion; pepper; zucchini; heated_oil)\n",
            " Output:       onion, pepper and zucchini added to heated oil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CoTPromptGenerator:\n",
        "    \"\"\"Chainâ€‘ofâ€‘Thought prompt generator for PizzaCommonSense.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # âœ… ãƒ«ãƒ¼ãƒ«ã‚’æ˜æ–‡åŒ–\n",
        "        self.system_message = (\n",
        "            \"You are an expert cookingâ€‘reasoning assistant.\\n\"\n",
        "            \"For every recipe step you must predict\\n\"\n",
        "            \"  â€¢ the *input* comestibles/items that go into the actionï¼ˆåŸææ–™ãŒãªã„å ´åˆã¯ NAï¼‰\\n\"\n",
        "            \"  â€¢ the *output* comestible/result that comes out ï¼ˆçµæœãŒé£Ÿæã§ãªã„ï¼é“å…·ã®ã¿ã®å ´åˆã¯ NAï¼‰\\n\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"1. If the step does NOT consume or transform any food, write exactly 'NA' for BOTH input and output.\\n\"\n",
        "            \"   â€‘ e.g. preâ€‘heating an oven, washing a utensil, setting a timer.\\n\"\n",
        "            \"2. Otherwise list the food items succinctly; join multiple items with semicolons.\\n\"\n",
        "            \"3. Respond **only** with the two lines:\\n\"\n",
        "            \"     Input: <your prediction>\\n\"\n",
        "            \"     Output: <your prediction>\\n\"\n",
        "        )\n",
        "\n",
        "    def make_prompt(self, step: dict) -> str:\n",
        "        \"\"\"Create a deterministic CoT prompt for one recipe step.\"\"\"\n",
        "        instr  = self._clean(step[\"instructions\"])\n",
        "        action = self._clean(step[\"action\"  ] if \"action\" in step else step[\"actions\"])\n",
        "\n",
        "        return (\n",
        "            f\"Instruction: {instr}\\n\"\n",
        "            f\"Action: {action}\\n\\n\"\n",
        "            # CoT ã‚’èª˜ç™º\n",
        "            \"Let's reason step by step.\\n\"\n",
        "            \"1ï¸âƒ£ Identify whether any food or edible item is being used or produced.\\n\"\n",
        "            \"2ï¸âƒ£ If none, decide Input=NA and Output=NA immediately.\\n\"\n",
        "            \"3ï¸âƒ£ Otherwise, list the food that goes *into* the action (Input) and \"\n",
        "            \"the food/result that comes *out of* the action (Output).\\n\\n\"\n",
        "            \"Remember the required format:\\n\"\n",
        "            \"Input: <prediction or NA>\\n\"\n",
        "            \"Output: <prediction or NA>\"\n",
        "        )\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "    @staticmethod\n",
        "    def _clean(text: str) -> str:\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "        return text.replace(\"\\u2012\", \"-\")      # â€‘â†’-\n",
        "\n",
        "    def get_system_message(self) -> str:\n",
        "        return self.system_message\n",
        "\n",
        "    # Initialize prompt generator\n",
        "        return self.system_message\n",
        "\n",
        "# Initialize prompt generator\n",
        "prompt_generator = CoTPromptGenerator()\n",
        "print(\"âœ… CoT prompt generator initialized successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMl1ZTFArYPD",
        "outputId": "a28324c6-e5aa-4427-d93c-96fa644dc77a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoT prompt generator initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompt generation with sample data\n",
        "print(\"ğŸ§ª Testing Prompt Generation:\\n\" + \"=\"*50)\n",
        "print(\"SYSTEM MESSAGE:\")\n",
        "print(\"=\"*50)\n",
        "print(prompt_generator.get_system_message())\n",
        "\n",
        "# Get a sample step for testing\n",
        "sample_step = next(data_loader.iter_tables(\"val\"))\n",
        "test_prompt = prompt_generator.make_prompt(sample_step)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAMPLE PROMPT:\")\n",
        "print(\"=\"*50)\n",
        "print(test_prompt)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GROUND TRUTH:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input: {sample_step['input']}\")\n",
        "print(f\"Output: {sample_step['output']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BNLV9HAraOG",
        "outputId": "025a9f53-f653-480c-eb15-e58820909892"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Testing Prompt Generation:\n",
            "==================================================\n",
            "SYSTEM MESSAGE:\n",
            "==================================================\n",
            "You are an expert cookingâ€‘reasoning assistant.\n",
            "For every recipe step you must predict\n",
            "  â€¢ the *input* comestibles/items that go into the actionï¼ˆåŸææ–™ãŒãªã„å ´åˆã¯ NAï¼‰\n",
            "  â€¢ the *output* comestible/result that comes out ï¼ˆçµæœãŒé£Ÿæã§ãªã„ï¼é“å…·ã®ã¿ã®å ´åˆã¯ NAï¼‰\n",
            "\n",
            "Rules:\n",
            "1. If the step does NOT consume or transform any food, write exactly 'NA' for BOTH input and output.\n",
            "   â€‘ e.g. preâ€‘heating an oven, washing a utensil, setting a timer.\n",
            "2. Otherwise list the food items succinctly; join multiple items with semicolons.\n",
            "3. Respond **only** with the two lines:\n",
            "     Input: <your prediction>\n",
            "     Output: <your prediction>\n",
            "\n",
            "Found 744 recipe files in train\n",
            "\n",
            "==================================================\n",
            "SAMPLE PROMPT:\n",
            "==================================================\n",
            "Instruction: preheat the oven to 400f .\n",
            "Action: preheat\n",
            "\n",
            "Let's reason step by step.\n",
            "1ï¸âƒ£ Identify whether any food or edible item is being used or produced.\n",
            "2ï¸âƒ£ If none, decide Input=NA and Output=NA immediately.\n",
            "3ï¸âƒ£ Otherwise, list the food that goes *into* the action (Input) and the food/result that comes *out of* the action (Output).\n",
            "\n",
            "Remember the required format:\n",
            "Input: <prediction or NA>\n",
            "Output: <prediction or NA>\n",
            "\n",
            "==================================================\n",
            "GROUND TRUTH:\n",
            "==================================================\n",
            "Input: NA\n",
            "Output: NA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai | grep Version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvKpMwUSif5x",
        "outputId": "893ce428-6c07-45bd-8202-da3ea3f0626f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, re, os, openai\n",
        "\n",
        "# â† 0.27 ç³»ã§ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ã‚­ãƒ¼ã‚’æ¸¡ã™\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "class GPT4Predictor:\n",
        "    \"\"\"\n",
        "    OpenAI 0.27.x ç”¨ GPTâ€‘4 å‘¼ã³å‡ºã—ã‚¯ãƒ©ã‚¹\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4.1-mini\",\n",
        "                 max_retries: int = 3, base_delay: float = 0.5):\n",
        "        self.model = model\n",
        "        self.max_retries = max_retries\n",
        "        self.base_delay  = base_delay\n",
        "\n",
        "        # æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
        "        openai.ChatCompletion.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"ping\"}],\n",
        "            max_tokens=1,\n",
        "            temperature=0\n",
        "        )\n",
        "        print(f\"âœ… API connection successful with model: {self.model}\")\n",
        "\n",
        "    # ---------- æ¨è«– ----------\n",
        "    def predict(self, prompt: str, system_message: str) -> str:\n",
        "        for i in range(self.max_retries):\n",
        "            try:\n",
        "                resp = openai.ChatCompletion.create(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_message},\n",
        "                        {\"role\": \"user\",   \"content\": prompt}\n",
        "                    ],\n",
        "                    temperature=0,\n",
        "                    max_tokens=200\n",
        "                )\n",
        "                return resp.choices[0].message[\"content\"].strip()\n",
        "\n",
        "            except openai.error.RateLimitError:\n",
        "                wait = self.base_delay * (2 ** i)\n",
        "                print(f\"â³ Rateâ€‘limit, retrying in {wait}s â€¦\")\n",
        "                time.sleep(wait)\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                print(f\"âŒ API error on attempt {i+1}: {e}\")\n",
        "                time.sleep(self.base_delay)\n",
        "\n",
        "        raise RuntimeError(\"Failed to get response after retries\")\n",
        "\n",
        "    # ---------- å¿œç­”ãƒ‘ãƒ¼ã‚¹ ----------\n",
        "    @staticmethod\n",
        "    def parse_io(text: str) -> tuple[str, str]:\n",
        "        in_pat  = r\"Input\\s*[:ï¼š]\\s*(.+?)(?=\\n|Output|$)\"\n",
        "        out_pat = r\"Output\\s*[:ï¼š]\\s*(.+?)(?=\\n|$)\"\n",
        "        inp  = re.search(in_pat,  text, re.I | re.S)\n",
        "        out  = re.search(out_pat, text, re.I | re.S)\n",
        "        return (inp.group(1).strip()  if inp else \"\",\n",
        "                out.group(1).strip()  if out else \"\")\n",
        "\n",
        "    def add_delay(self):\n",
        "        time.sleep(self.base_delay)\n",
        "\n",
        "# -------- ãƒ†ã‚¹ãƒˆ --------\n",
        "predictor = GPT4Predictor()\n",
        "print(\"âœ… GPT4Predictor is ready!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzp9HCRuhgYk",
        "outputId": "183ced7f-b08d-4450-a084-b32a38b68af2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API connection successful with model: gpt-4.1-mini\n",
            "âœ… GPT4Predictor is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test GPT-4 prediction with sample data\n",
        "print(\"ğŸ§ª Testing GPT-4 Prediction:\")\n",
        "\n",
        "# Use the same sample step from before\n",
        "test_prompt = prompt_generator.make_prompt(sample_step)\n",
        "system_msg = prompt_generator.get_system_message()\n",
        "\n",
        "print(\"\\nâ³ Generating prediction...\")\n",
        "response = predictor.predict(test_prompt, system_msg)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL RESPONSE:\")\n",
        "print(\"=\"*50)\n",
        "print(response)\n",
        "\n",
        "# Parse the response\n",
        "pred_input, pred_output = predictor.parse_io(response)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PARSED PREDICTIONS:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Predicted Input: '{pred_input}'\")\n",
        "print(f\"Predicted Output: '{pred_output}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GROUND TRUTH COMPARISON:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Ground Truth Input:  '{sample_step['input']}'\")\n",
        "print(f\"Ground Truth Output: '{sample_step['output']}'\")\n",
        "\n",
        "# Check exact matches\n",
        "input_match = pred_input.lower().strip() == sample_step['input'].lower().strip()\n",
        "output_match = pred_output.lower().strip() == sample_step['output'].lower().strip()\n",
        "\n",
        "print(f\"\\nğŸ“Š Exact Match Results:\")\n",
        "print(f\"Input Match: {'âœ…' if input_match else 'âŒ'}\")\n",
        "print(f\"Output Match: {'âœ…' if output_match else 'âŒ'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882nVJa_rgHb",
        "outputId": "f2e762bd-a79b-42ae-cd4d-5a904bfc6149"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Testing GPT-4 Prediction:\n",
            "\n",
            "â³ Generating prediction...\n",
            "\n",
            "==================================================\n",
            "MODEL RESPONSE:\n",
            "==================================================\n",
            "Input: NA\n",
            "Output: NA\n",
            "\n",
            "==================================================\n",
            "PARSED PREDICTIONS:\n",
            "==================================================\n",
            "Predicted Input: 'NA'\n",
            "Predicted Output: 'NA'\n",
            "\n",
            "==================================================\n",
            "GROUND TRUTH COMPARISON:\n",
            "==================================================\n",
            "Ground Truth Input:  'NA'\n",
            "Ground Truth Output: 'NA'\n",
            "\n",
            "ğŸ“Š Exact Match Results:\n",
            "Input Match: âœ…\n",
            "Output Match: âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_batch_predictions(split: str = \"val\", max_samples: int = None, save_interval: int = 50):\n",
        "    \"\"\"Run batch predictions on the dataset with progress tracking.\n",
        "\n",
        "    Args:\n",
        "        split: Dataset split to process ('val' or 'train')\n",
        "        max_samples: Maximum number of samples to process (None for all)\n",
        "        save_interval: Save results every N samples\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Results with predictions and ground truth\n",
        "    \"\"\"\n",
        "    print(f\"ğŸš€ Starting batch prediction on {split} split...\")\n",
        "\n",
        "    # Collect all steps first to get total count\n",
        "    all_steps = list(data_loader.iter_tables(split))\n",
        "\n",
        "    if max_samples:\n",
        "        all_steps = all_steps[:max_samples]\n",
        "\n",
        "    print(f\"ğŸ“Š Processing {len(all_steps)} recipe steps\")\n",
        "\n",
        "    results = []\n",
        "    system_msg = prompt_generator.get_system_message()\n",
        "\n",
        "    # Process with progress bar\n",
        "    for i, step in enumerate(tqdm(all_steps, desc=\"Generating predictions\")):\n",
        "        try:\n",
        "            # Generate prompt\n",
        "            prompt = prompt_generator.make_prompt(step)\n",
        "\n",
        "            # Get prediction\n",
        "            response = predictor.predict(prompt, system_msg)\n",
        "\n",
        "            # Parse response\n",
        "            pred_input, pred_output = predictor.parse_io(response)\n",
        "\n",
        "            # Store result\n",
        "            result = {\n",
        "                'instructions': step['instructions'],\n",
        "                'actions': step['actions'],\n",
        "                'input': step['input'],\n",
        "                'output': step['output'],\n",
        "                'pred_input': pred_input,\n",
        "                'pred_output': pred_output,\n",
        "                'response': response  # Keep full response for debugging\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Add delay for rate limiting\n",
        "            predictor.add_delay()\n",
        "\n",
        "            # Periodic saving\n",
        "            if (i + 1) % save_interval == 0:\n",
        "                temp_df = pd.DataFrame(results)\n",
        "                temp_df.to_csv(f\"temp_predictions_{i+1}.csv\", index=False)\n",
        "                print(f\"ğŸ’¾ Saved temporary results at step {i+1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error processing step {i+1}: {e}\")\n",
        "            # Add placeholder result to maintain alignment\n",
        "            result = {\n",
        "                'instructions': step['instructions'],\n",
        "                'actions': step['actions'],\n",
        "                'input': step['input'],\n",
        "                'output': step['output'],\n",
        "                'pred_input': '',\n",
        "                'pred_output': '',\n",
        "                'response': f'ERROR: {str(e)}'\n",
        "            }\n",
        "            results.append(result)\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    print(f\"âœ… Batch processing complete! Processed {len(df)} steps\")\n",
        "    print(f\"ğŸ“Š Success rate: {(df['pred_input'] != '').sum()}/{len(df)} ({(df['pred_input'] != '').mean():.1%})\")\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"âœ… Batch processing function ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMhdXVA_rhQE",
        "outputId": "918a466d-18aa-4957-b328-534c69e1a1a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Batch processing function ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a small test first (10 samples)\n",
        "print(\"ğŸ§ª Running small test with 10 samples...\")\n",
        "test_df = run_batch_predictions(split=\"val\", max_samples=10)\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\nğŸ“‹ Sample Results:\")\n",
        "display_cols = ['instructions', 'actions', 'input', 'output', 'pred_input', 'pred_output']\n",
        "print(test_df[display_cols].head())\n",
        "\n",
        "# Quick evaluation\n",
        "input_matches = (test_df['input'].str.lower().str.strip() ==\n",
        "                test_df['pred_input'].str.lower().str.strip()).sum()\n",
        "output_matches = (test_df['output'].str.lower().str.strip() ==\n",
        "                 test_df['pred_output'].str.lower().str.strip()).sum()\n",
        "\n",
        "print(f\"\\nğŸ“Š Quick Test Results:\")\n",
        "print(f\"Input EMA: {input_matches}/{len(test_df)} ({input_matches/len(test_df):.1%})\")\n",
        "print(f\"Output EMA: {output_matches}/{len(test_df)} ({output_matches/len(test_df):.1%})\")\n",
        "print(f\"Average EMA: {(input_matches + output_matches)/(2*len(test_df)):.1%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUkbNlA3lGp9",
        "outputId": "5448a640-2015-4837-90ea-3309c2f164d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Running small test with 10 samples...\n",
            "ğŸš€ Starting batch prediction on val split...\n",
            "Found 744 recipe files in train\n",
            "ğŸ“Š Processing 10 recipe steps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Batch processing complete! Processed 10 steps\n",
            "ğŸ“Š Success rate: 10/10 (100.0%)\n",
            "\n",
            "ğŸ“‹ Sample Results:\n",
            "                                   instructions  actions  \\\n",
            "0                    preheat the oven to 400f .  preheat   \n",
            "1  heat the oil in a large non stick frying pan     heat   \n",
            "2           add the onion , pepper and zucchini      add   \n",
            "3        saute over a medium heat for 4 5mins .    saute   \n",
            "4                                 add the herbs      add   \n",
            "\n",
            "                                   input  \\\n",
            "0                                     NA   \n",
            "1                                    oil   \n",
            "2  (onion; pepper; zucchini; heated_oil)   \n",
            "3               vegetables in heated_oil   \n",
            "4     (herbs; sauteed vegetable mixture)   \n",
            "\n",
            "                                           output               pred_input  \\\n",
            "0                                              NA                       NA   \n",
            "1                                      heated_oil                      oil   \n",
            "2  onion, pepper and zucchini added to heated oil  onion; pepper; zucchini   \n",
            "3                       sauteed vegetable mixture                       NA   \n",
            "4        herbs added to sauteed vegetable mixture                    herbs   \n",
            "\n",
            "               pred_output  \n",
            "0                       NA  \n",
            "1               heated oil  \n",
            "2  onion; pepper; zucchini  \n",
            "3                       NA  \n",
            "4                    herbs  \n",
            "\n",
            "ğŸ“Š Quick Test Results:\n",
            "Input EMA: 2/10 (20.0%)\n",
            "Output EMA: 1/10 (10.0%)\n",
            "Average EMA: 15.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è¿½åŠ ã‚»ãƒ«\n",
        "!pip -q install bert_score transformers torch\n"
      ],
      "metadata": {
        "id": "7uN0rvYXtcxb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Run a small test first (10 samples)\n",
        "# ------------------------------------------------------------\n",
        "print(\"ğŸ§ª Running small test with 10 samples...\")\n",
        "test_df = run_batch_predictions(split=\"val\", max_samples=10)\n",
        "\n",
        "# ------------- çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ -------------\n",
        "print(\"\\nğŸ“‹ Sample Results:\")\n",
        "display_cols = ['instructions', 'actions', 'input', 'output',\n",
        "                'pred_input', 'pred_output']\n",
        "print(test_df[display_cols].head())\n",
        "\n",
        "# ------------- Exactâ€‘Match Accuracy (EMA) -------------\n",
        "input_matches = (\n",
        "    test_df['input'].str.lower().str.strip() ==\n",
        "    test_df['pred_input'].str.lower().str.strip()\n",
        ").sum()\n",
        "\n",
        "output_matches = (\n",
        "    test_df['output'].str.lower().str.strip() ==\n",
        "    test_df['pred_output'].str.lower().str.strip()\n",
        ").sum()\n",
        "\n",
        "# ------------- BERTScore (ç›´æ¥è¨ˆç®—) -------------\n",
        "import evaluate, numpy as np\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "# ç©ºå€¤å¯¾ç­–\n",
        "pred_inputs  = test_df['pred_input'].fillna(\"\").tolist()\n",
        "true_inputs  = test_df['input'].fillna(\"\").tolist()\n",
        "pred_outputs = test_df['pred_output'].fillna(\"\").tolist()\n",
        "true_outputs = test_df['output'].fillna(\"\").tolist()\n",
        "\n",
        "bert_in  = bertscore.compute(predictions=pred_inputs,  references=true_inputs,  lang=\"en\")['f1']\n",
        "bert_out = bertscore.compute(predictions=pred_outputs, references=true_outputs, lang=\"en\")['f1']\n",
        "# è¿½åŠ ã‚»ãƒ«\n",
        "!pip -q install bert_score transformers torch\n",
        "\n",
        "# ------------- ã¾ã¨ã‚è¡¨ç¤º -------------\n",
        "print(f\"\\nğŸ“Š Quick Test Results:\")\n",
        "print(f\"Input EMA:   {input_matches}/{len(test_df)} \"\n",
        "      f\"({input_matches/len(test_df):.1%})\")\n",
        "print(f\"Output EMA:  {output_matches}/{len(test_df)} \"\n",
        "      f\"({output_matches/len(test_df):.1%})\")\n",
        "print(f\"Average EMA: {(input_matches + output_matches)/(2*len(test_df)):.1%}\")\n",
        "print(f\"BERTScoreâ€¯Inputâ€¯F1:  {np.mean(bert_in):.3f}\")\n",
        "print(f\"BERTScoreâ€¯Outputâ€¯F1: {np.mean(bert_out):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx_hjT3lqUEm",
        "outputId": "868f6bf5-bceb-40ba-9c9e-a244c2357d9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Running small test with 10 samples...\n",
            "ğŸš€ Starting batch prediction on val split...\n",
            "Found 744 recipe files in train\n",
            "ğŸ“Š Processing 10 recipe steps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Batch processing complete! Processed 10 steps\n",
            "ğŸ“Š Success rate: 10/10 (100.0%)\n",
            "\n",
            "ğŸ“‹ Sample Results:\n",
            "                                   instructions  actions  \\\n",
            "0                    preheat the oven to 400f .  preheat   \n",
            "1  heat the oil in a large non stick frying pan     heat   \n",
            "2           add the onion , pepper and zucchini      add   \n",
            "3        saute over a medium heat for 4 5mins .    saute   \n",
            "4                                 add the herbs      add   \n",
            "\n",
            "                                   input  \\\n",
            "0                                     NA   \n",
            "1                                    oil   \n",
            "2  (onion; pepper; zucchini; heated_oil)   \n",
            "3               vegetables in heated_oil   \n",
            "4     (herbs; sauteed vegetable mixture)   \n",
            "\n",
            "                                           output               pred_input  \\\n",
            "0                                              NA                       NA   \n",
            "1                                      heated_oil                      oil   \n",
            "2  onion, pepper and zucchini added to heated oil  onion; pepper; zucchini   \n",
            "3                       sauteed vegetable mixture                       NA   \n",
            "4        herbs added to sauteed vegetable mixture                    herbs   \n",
            "\n",
            "               pred_output  \n",
            "0                       NA  \n",
            "1               heated oil  \n",
            "2  onion; pepper; zucchini  \n",
            "3                       NA  \n",
            "4                    herbs  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Quick Test Results:\n",
            "Input EMA:   2/10 (20.0%)\n",
            "Output EMA:  1/10 (10.0%)\n",
            "Average EMA: 15.0%\n",
            "BERTScoreâ€¯Inputâ€¯F1:  0.884\n",
            "BERTScoreâ€¯Outputâ€¯F1: 0.864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run full batch processing (uncomment when ready)\n",
        "# WARNING: This will process the entire validation split and may take 30-60 minutes\n",
        "# and cost several dollars in API calls\n",
        "\n",
        "print(\"âš ï¸  Ready to run full batch processing on validation split\")\n",
        "print(\"ğŸ’° Estimated cost: $3-10 depending on dataset size\")\n",
        "print(\"â±ï¸  Estimated time: 30-60 minutes\")\n",
        "print(\"\\nğŸ”§ To run full processing, uncomment the lines below:\")\n",
        "print(\"# full_df = run_batch_predictions(split='val')\")\n",
        "print(\"# full_df.to_csv('gpt4_predictions_full.csv', index=False)\")\n",
        "print(\"# print('ğŸ’¾ Full results saved to gpt4_predictions_full.csv')\")\n",
        "\n",
        "# Uncomment these lines when ready to run full experiment:\n",
        "full_df = run_batch_predictions(split=\"val\")\n",
        "full_df.to_csv(\"gpt4_predictions_full.csv\", index=False)\n",
        "print(\"ğŸ’¾ Full results saved to gpt4_predictions_full.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFhUDXw9w-g3",
        "outputId": "d07481c5-cde5-4dcb-85ca-e9984471389a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸  Ready to run full batch processing on validation split\n",
            "ğŸ’° Estimated cost: $3-10 depending on dataset size\n",
            "â±ï¸  Estimated time: 30-60 minutes\n",
            "\n",
            "ğŸ”§ To run full processing, uncomment the lines below:\n",
            "# full_df = run_batch_predictions(split='val')\n",
            "# full_df.to_csv('gpt4_predictions_full.csv', index=False)\n",
            "# print('ğŸ’¾ Full results saved to gpt4_predictions_full.csv')\n",
            "ğŸš€ Starting batch prediction on val split...\n",
            "Found 744 recipe files in train\n",
            "ğŸ“Š Processing 3069 recipe steps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   2%|â–         | 50/3069 [00:47<46:11,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   3%|â–         | 100/3069 [01:48<1:39:08,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   5%|â–         | 150/3069 [02:39<50:37,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   7%|â–‹         | 200/3069 [03:27<46:47,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   8%|â–Š         | 250/3069 [04:18<47:01,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  10%|â–‰         | 300/3069 [05:09<47:42,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  11%|â–ˆâ–        | 350/3069 [05:59<43:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  13%|â–ˆâ–        | 400/3069 [06:53<59:27,  1.34s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  15%|â–ˆâ–        | 450/3069 [07:41<42:45,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  16%|â–ˆâ–‹        | 500/3069 [08:34<39:06,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  18%|â–ˆâ–Š        | 550/3069 [09:24<38:12,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  20%|â–ˆâ–‰        | 600/3069 [10:09<38:46,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  21%|â–ˆâ–ˆ        | 650/3069 [11:02<37:10,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  23%|â–ˆâ–ˆâ–       | 700/3069 [11:53<34:58,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  24%|â–ˆâ–ˆâ–       | 750/3069 [12:44<42:30,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  26%|â–ˆâ–ˆâ–Œ       | 800/3069 [13:37<47:39,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  28%|â–ˆâ–ˆâ–Š       | 850/3069 [14:29<36:44,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  29%|â–ˆâ–ˆâ–‰       | 900/3069 [15:25<35:42,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  31%|â–ˆâ–ˆâ–ˆ       | 950/3069 [16:22<36:09,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  33%|â–ˆâ–ˆâ–ˆâ–      | 1000/3069 [17:14<30:21,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  34%|â–ˆâ–ˆâ–ˆâ–      | 1050/3069 [18:07<41:45,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1100/3069 [18:58<36:42,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1150/3069 [19:46<29:38,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1200/3069 [20:41<29:51,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1250/3069 [21:41<27:13,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1300/3069 [22:29<24:26,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1350/3069 [23:17<28:18,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1400/3069 [24:19<31:00,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1450/3069 [25:07<29:03,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1500/3069 [25:59<25:14,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1550/3069 [26:49<24:40,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1600/3069 [27:43<28:24,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1650/3069 [28:35<22:13,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1700/3069 [29:24<22:48,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1750/3069 [30:16<23:15,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1800/3069 [31:05<20:27,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1850/3069 [31:55<23:29,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1900/3069 [32:45<18:35,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1950/3069 [33:39<17:24,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 1950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2000/3069 [34:30<16:04,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2050/3069 [35:19<16:10,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2100/3069 [36:07<17:37,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2150/3069 [36:59<16:05,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2200/3069 [37:51<12:58,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2250/3069 [38:39<13:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2300/3069 [39:31<12:04,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2350/3069 [40:20<12:53,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2400/3069 [41:14<11:18,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2450/3069 [42:06<19:44,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2500/3069 [42:55<09:19,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2550/3069 [43:44<08:01,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2600/3069 [44:35<06:55,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2650/3069 [45:22<06:20,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2700/3069 [46:08<05:35,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2750/3069 [47:00<04:48,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2800/3069 [47:53<04:10,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2850/3069 [48:43<03:15,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2900/3069 [49:36<02:40,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2950/3069 [50:26<02:08,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 2950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3000/3069 [51:15<01:08,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3050/3069 [52:05<00:19,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saved temporary results at step 3050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3069/3069 [52:25<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Batch processing complete! Processed 3069 steps\n",
            "ğŸ“Š Success rate: 3069/3069 (100.0%)\n",
            "ğŸ’¾ Full results saved to gpt4_predictions_full.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCalculator:\n",
        "    \"\"\"Computes evaluation metrics for model predictions.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize evaluation libraries\n",
        "        self.rouge = evaluate.load(\"rouge\")\n",
        "        self.bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "    def calculate_ema(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate Exact Match Accuracy with normalization.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted strings\n",
        "            references: List of ground truth strings\n",
        "\n",
        "        Returns:\n",
        "            float: EMA score (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        if len(predictions) != len(references):\n",
        "            raise ValueError(\"Predictions and references must have same length\")\n",
        "\n",
        "        matches = 0\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            # Normalize strings for comparison\n",
        "            pred_norm = self._normalize_string(pred)\n",
        "            ref_norm = self._normalize_string(ref)\n",
        "\n",
        "            if pred_norm == ref_norm:\n",
        "                matches += 1\n",
        "\n",
        "        return matches / len(predictions) if predictions else 0.0\n",
        "\n",
        "    def calculate_rouge_l(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate Rouge-L score.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted strings\n",
        "            references: List of ground truth strings\n",
        "\n",
        "        Returns:\n",
        "            float: Rouge-L F1 score\n",
        "        \"\"\"\n",
        "        if not predictions or not references:\n",
        "            return 0.0\n",
        "\n",
        "        # Handle empty predictions\n",
        "        clean_predictions = [pred if pred else \"\" for pred in predictions]\n",
        "        clean_references = [ref if ref else \"\" for ref in references]\n",
        "\n",
        "        try:\n",
        "            results = self.rouge.compute(\n",
        "                predictions=clean_predictions,\n",
        "                references=clean_references\n",
        "            )\n",
        "            return results[\"rougeL\"]\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Rouge-L calculation error: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_bertscore(self, predictions: List[str], references: List[str]) -> float:\n",
        "        \"\"\"Calculate BERTScore F1.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted strings\n",
        "            references: List of ground truth strings\n",
        "\n",
        "        Returns:\n",
        "            float: Average BERTScore F1\n",
        "        \"\"\"\n",
        "        if not predictions or not references:\n",
        "            return 0.0\n",
        "\n",
        "        # Handle empty predictions\n",
        "        clean_predictions = [pred if pred else \"empty\" for pred in predictions]\n",
        "        clean_references = [ref if ref else \"empty\" for ref in references]\n",
        "\n",
        "        try:\n",
        "            results = self.bertscore.compute(\n",
        "                predictions=clean_predictions,\n",
        "                references=clean_references,\n",
        "                lang=\"en\"\n",
        "            )\n",
        "            return np.mean(results[\"f1\"])\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ BERTScore calculation error: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _normalize_string(self, text: str) -> str:\n",
        "        \"\"\"Normalize string for comparison.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "\n",
        "        # Convert to lowercase and strip whitespace\n",
        "        text = text.lower().strip()\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Handle special cases\n",
        "        if text in ['na', 'n/a', 'none', '']:\n",
        "            return ''\n",
        "\n",
        "        return text\n",
        "\n",
        "    def evaluate_predictions(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Comprehensive evaluation of predictions.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with predictions and ground truth\n",
        "\n",
        "        Returns:\n",
        "            Dict: All evaluation metrics\n",
        "        \"\"\"\n",
        "        print(\"ğŸ“Š Calculating evaluation metrics...\")\n",
        "\n",
        "        # Extract predictions and references\n",
        "        pred_inputs = df['pred_input'].tolist()\n",
        "        true_inputs = df['input'].tolist()\n",
        "        pred_outputs = df['pred_output'].tolist()\n",
        "        true_outputs = df['output'].tolist()\n",
        "\n",
        "        # Calculate EMA\n",
        "        ema_input = self.calculate_ema(pred_inputs, true_inputs)\n",
        "        ema_output = self.calculate_ema(pred_outputs, true_outputs)\n",
        "        ema_avg = (ema_input + ema_output) / 2\n",
        "\n",
        "        # Calculate Rouge-L\n",
        "        rouge_input = self.calculate_rouge_l(pred_inputs, true_inputs)\n",
        "        rouge_output = self.calculate_rouge_l(pred_outputs, true_outputs)\n",
        "\n",
        "        # Calculate BERTScore (focus on outputs as they're more complex)\n",
        "        bertscore_output = self.calculate_bertscore(pred_outputs, true_outputs)\n",
        "\n",
        "        metrics = {\n",
        "            'ema_input': ema_input,\n",
        "            'ema_output': ema_output,\n",
        "            'ema_average': ema_avg,\n",
        "            'rouge_l_input': rouge_input,\n",
        "            'rouge_l_output': rouge_output,\n",
        "            'bertscore_f1': bertscore_output,\n",
        "            'total_samples': len(df)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Initialize metrics calculator\n",
        "metrics_calc = MetricsCalculator()\n",
        "print(\"âœ… Metrics calculator initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGUJ9kEUxja3",
        "outputId": "b434c375-e050-48bf-fdd0-afe1f01ce7b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Metrics calculator initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the test results\n",
        "print(\"ğŸ“Š Evaluating test results...\")\n",
        "test_metrics = metrics_calc.evaluate_predictions(test_df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST RESULTS (10 samples)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"EMA Input:        {test_metrics['ema_input']:.1%}\")\n",
        "print(f\"EMA Output:       {test_metrics['ema_output']:.1%}\")\n",
        "print(f\"EMA Average:      {test_metrics['ema_average']:.1%}\")\n",
        "print(f\"Rouge-L Input:    {test_metrics['rouge_l_input']:.3f}\")\n",
        "print(f\"Rouge-L Output:   {test_metrics['rouge_l_output']:.3f}\")\n",
        "print(f\"BERTScore F1:     {test_metrics['bertscore_f1']:.3f}\")\n",
        "print(f\"Total Samples:    {test_metrics['total_samples']}\")\n",
        "\n",
        "# Compare with paper benchmarks\n",
        "paper_benchmarks = {\n",
        "    'ema_average': 0.267,  # 26.7%\n",
        "    'rouge_l_input': 0.514,  # 51.4\n",
        "    'rouge_l_output': 0.523  # 52.3\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON WITH PAPER BENCHMARKS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<20} {'Test':<10} {'Paper':<10} {'Status':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric, paper_value in paper_benchmarks.items():\n",
        "    test_value = test_metrics[metric]\n",
        "    status = \"âœ… BETTER\" if test_value >= paper_value else \"âŒ LOWER\"\n",
        "    print(f\"{metric:<20} {test_value:<10.3f} {paper_value:<10.3f} {status}\")\n",
        "\n",
        "print(\"\\nâš ï¸  Note: These are results on only 10 test samples.\")\n",
        "print(\"ğŸ“ˆ Run full evaluation for meaningful comparison with paper.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFPnKogpx8nv",
        "outputId": "ebd4b888-cb40-4298-e24a-342993a6b5f3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Evaluating test results...\n",
            "ğŸ“Š Calculating evaluation metrics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TEST RESULTS (10 samples)\n",
            "============================================================\n",
            "EMA Input:        20.0%\n",
            "EMA Output:       10.0%\n",
            "EMA Average:      15.0%\n",
            "Rouge-L Input:    0.457\n",
            "Rouge-L Output:   0.378\n",
            "BERTScore F1:     0.864\n",
            "Total Samples:    10\n",
            "\n",
            "============================================================\n",
            "COMPARISON WITH PAPER BENCHMARKS\n",
            "============================================================\n",
            "Metric               Test       Paper      Status    \n",
            "------------------------------------------------------------\n",
            "ema_average          0.150      0.267      âŒ LOWER\n",
            "rouge_l_input        0.457      0.514      âŒ LOWER\n",
            "rouge_l_output       0.378      0.523      âŒ LOWER\n",
            "\n",
            "âš ï¸  Note: These are results on only 10 test samples.\n",
            "ğŸ“ˆ Run full evaluation for meaningful comparison with paper.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_full_results(csv_path: str = \"gpt4_predictions_full.csv\"):\n",
        "    \"\"\"Evaluate full results and compare with paper benchmarks.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to the CSV file with full predictions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load results\n",
        "        df = pd.read_csv(csv_path, keep_default_na=False)\n",
        "        print(f\"ğŸ“ Loaded {len(df)} predictions from {csv_path}\")\n",
        "\n",
        "        text_cols = ['input', 'output', 'pred_input', 'pred_output']\n",
        "        df[text_cols] = df[text_cols].fillna('').astype(str)\n",
        "        # Calculate metrics\n",
        "        metrics = metrics_calc.evaluate_predictions(df)\n",
        "\n",
        "        # Paper benchmarks\n",
        "        paper_benchmarks = {\n",
        "            'EMA Average': {'our': metrics['ema_average'], 'paper': 0.267, 'format': '.1%'},\n",
        "            'Rouge-L Input': {'our': metrics['rouge_l_input'], 'paper': 0.514, 'format': '.3f'},\n",
        "            'Rouge-L Output': {'our': metrics['rouge_l_output'], 'paper': 0.523, 'format': '.3f'},\n",
        "            'BERTScore F1': {'our': metrics['bertscore_f1'], 'paper': None, 'format': '.3f'}\n",
        "        }\n",
        "\n",
        "        # Display comprehensive results\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ¯ FINAL RESULTS - PAPER REPRODUCTION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"ğŸ“Š Dataset: PizzaCommonSense validation split ({metrics['total_samples']} samples)\")\n",
        "        print(f\"ğŸ¤– Model: GPT-4 Turbo (gpt-4o-mini) with Chain-of-Thought\")\n",
        "        print(f\"ğŸ“… Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"{'Metric':<20} {'Our Result':<15} {'Paper Benchmark':<18} {'Status':<15} {'Difference'}\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        success_count = 0\n",
        "        total_comparable = 0\n",
        "\n",
        "        for metric_name, values in paper_benchmarks.items():\n",
        "            our_val = values['our']\n",
        "            paper_val = values['paper']\n",
        "            fmt = values['format']\n",
        "\n",
        "            if paper_val is not None:\n",
        "                total_comparable += 1\n",
        "                diff = our_val - paper_val\n",
        "                status = \"âœ… BETTER\" if our_val >= paper_val else \"âŒ LOWER\"\n",
        "                if our_val >= paper_val:\n",
        "                    success_count += 1\n",
        "\n",
        "                our_str = f\"{our_val:{fmt}}\"\n",
        "                paper_str = f\"{paper_val:{fmt}}\"\n",
        "                diff_str = f\"{diff:+{fmt}}\"\n",
        "            else:\n",
        "                status = \"ğŸ“Š NEW METRIC\"\n",
        "                our_str = f\"{our_val:{fmt}}\"\n",
        "                paper_str = \"N/A\"\n",
        "                diff_str = \"N/A\"\n",
        "\n",
        "            print(f\"{metric_name:<20} {our_str:<15} {paper_str:<18} {status:<15} {diff_str}\")\n",
        "\n",
        "        # Success summary\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ† REPRODUCTION SUCCESS SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        success_rate = success_count / total_comparable if total_comparable > 0 else 0\n",
        "        print(f\"âœ… Metrics meeting/exceeding paper: {success_count}/{total_comparable} ({success_rate:.1%})\")\n",
        "\n",
        "        if success_count > 0:\n",
        "            print(\"ğŸ‰ SUCCESS: At least one metric meets the paper benchmark!\")\n",
        "            print(\"ğŸ“ˆ Reproduction experiment successful!\")\n",
        "        else:\n",
        "            print(\"âš ï¸  No metrics exceed paper benchmarks\")\n",
        "            print(\"ğŸ” Consider adjusting prompts or trying different techniques\")\n",
        "\n",
        "        # Detailed breakdown\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"ğŸ“‹ DETAILED BREAKDOWN\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"EMA Input:        {metrics['ema_input']:.1%}\")\n",
        "        print(f\"EMA Output:       {metrics['ema_output']:.1%}\")\n",
        "        print(f\"EMA Average:      {metrics['ema_average']:.1%} (Target: 26.7%)\")\n",
        "        print(f\"Rouge-L Input:    {metrics['rouge_l_input']:.3f} (Target: 0.514)\")\n",
        "        print(f\"Rouge-L Output:   {metrics['rouge_l_output']:.3f} (Target: 0.523)\")\n",
        "        print(f\"BERTScore F1:     {metrics['bertscore_f1']:.3f} (New metric)\")\n",
        "\n",
        "        # Save metrics to JSON\n",
        "        metrics_file = \"final_metrics.json\"\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        print(f\"\\nğŸ’¾ Detailed metrics saved to {metrics_file}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ File {csv_path} not found. Run full batch processing first.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error evaluating results: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… Full evaluation function ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNpGC5B6x-71",
        "outputId": "c4cb454a-e303-4041-c136-2021b228bea6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Full evaluation function ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run full evaluation (uncomment after running full batch processing)\n",
        "print(\"ğŸ¯ Ready to evaluate full results\")\n",
        "\n",
        "print(\"\\nğŸ”§ To run full evaluation, uncomment the line below:\")\n",
        "print(\"# final_metrics = evaluate_full_results('gpt4_predictions_full.csv')\")\n",
        "\n",
        "# Uncomment this line after running full batch processing:\n",
        "final_metrics = evaluate_full_results(\"gpt4_predictions_full.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T8Z6fRZyBrv",
        "outputId": "f5b23cd2-d525-4167-ca66-f9950da5bc80"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Ready to evaluate full results\n",
            "\n",
            "ğŸ”§ To run full evaluation, uncomment the line below:\n",
            "# final_metrics = evaluate_full_results('gpt4_predictions_full.csv')\n",
            "ğŸ“ Loaded 3069 predictions from gpt4_predictions_full.csv\n",
            "ğŸ“Š Calculating evaluation metrics...\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ FINAL RESULTS - PAPER REPRODUCTION\n",
            "================================================================================\n",
            "ğŸ“Š Dataset: PizzaCommonSense validation split (3069 samples)\n",
            "ğŸ¤– Model: GPT-4 Turbo (gpt-4o-mini) with Chain-of-Thought\n",
            "ğŸ“… Evaluation Date: 2025-07-21 17:54\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Metric               Our Result      Paper Benchmark    Status          Difference\n",
            "--------------------------------------------------------------------------------\n",
            "EMA Average          16.6%           26.7%              âŒ LOWER         -10.1%\n",
            "Rouge-L Input        0.446           0.514              âŒ LOWER         -0.068\n",
            "Rouge-L Output       0.377           0.523              âŒ LOWER         -0.146\n",
            "BERTScore F1         0.867           N/A                ğŸ“Š NEW METRIC    N/A\n",
            "\n",
            "================================================================================\n",
            "ğŸ† REPRODUCTION SUCCESS SUMMARY\n",
            "================================================================================\n",
            "âœ… Metrics meeting/exceeding paper: 0/3 (0.0%)\n",
            "âš ï¸  No metrics exceed paper benchmarks\n",
            "ğŸ” Consider adjusting prompts or trying different techniques\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ“‹ DETAILED BREAKDOWN\n",
            "--------------------------------------------------------------------------------\n",
            "EMA Input:        18.7%\n",
            "EMA Output:       14.5%\n",
            "EMA Average:      16.6% (Target: 26.7%)\n",
            "Rouge-L Input:    0.446 (Target: 0.514)\n",
            "Rouge-L Output:   0.377 (Target: 0.523)\n",
            "BERTScore F1:     0.867 (New metric)\n",
            "\n",
            "ğŸ’¾ Detailed metrics saved to final_metrics.json\n"
          ]
        }
      ]
    }
  ]
}