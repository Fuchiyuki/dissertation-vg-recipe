{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_cell"
   },
   "source": [
    "# PizzaCommonSense × GPT-4 Turbo (CoT) Reproduction\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook reproduces the PizzaCommonSense dataset experiments using GPT-4 Turbo with Chain-of-Thought (CoT) prompting. The goal is to achieve performance metrics equal to or better than the published paper results:\n",
    "\n",
    "**Target Benchmarks (GPT-4 + CoT):**\n",
    "- EMA: 26.7%\n",
    "- Rouge-L Input: 51.4\n",
    "- Rouge-L Output: 52.3\n",
    "\n",
    "**Success Criteria:** At least one metric should meet or exceed the paper values.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Setup & Configuration** - Environment preparation and API key management\n",
    "2. **Data Processing** - Folder traversal and JSON parsing\n",
    "3. **Prompt Engineering** - CoT template generation and formatting\n",
    "4. **Model Inference** - GPT-4 Turbo API calls with rate limiting\n",
    "5. **Batch Processing** - Automated prediction generation for validation split\n",
    "6. **Evaluation & Reporting** - Metric calculation and benchmark comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "Install required libraries and configure the environment for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libraries"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip -q install openai==1.30.0 tqdm rouge-score evaluate pandas tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Iterator, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import evaluate\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_key_setup"
   },
   "outputs": [],
   "source": [
    "# Secure API key input\n",
    "print(\"🔑 Please enter your OpenAI API key:\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"API Key: \")\n",
    "\n",
    "# Verify API key is set\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"✅ API key configured successfully!\")\n",
    "else:\n",
    "    print(\"❌ API key not set. Please run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Load and process the PizzaCommonSense dataset from the existing folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loader_class"
   },
   "outputs": [],
   "source": [
    "class PizzaDataLoader:\n",
    "    \"\"\"Handles folder traversal and JSON parsing for PizzaCommonSense dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str = \"data/PizzaCommonsense\"):\n",
    "        self.data_path = Path(data_path)\n",
    "        if not self.data_path.exists():\n",
    "            raise FileNotFoundError(f\"Data path {data_path} does not exist\")\n",
    "    \n",
    "    def iter_tables(self, split: str = \"val\") -> Iterator[Dict]:\n",
    "        \"\"\"Iterate through recipe steps from the specified split.\n",
    "        \n",
    "        Args:\n",
    "            split: Dataset split to process ('train' or 'val')\n",
    "            \n",
    "        Yields:\n",
    "            Dict: Recipe step with instructions, actions, input, output\n",
    "        \"\"\"\n",
    "        split_path = self.data_path / split\n",
    "        \n",
    "        if not split_path.exists():\n",
    "            raise FileNotFoundError(f\"Split path {split_path} does not exist\")\n",
    "        \n",
    "        # Get all .txt files in the split directory\n",
    "        txt_files = list(split_path.glob(\"*.txt\"))\n",
    "        print(f\"Found {len(txt_files)} recipe files in {split} split\")\n",
    "        \n",
    "        for file_path in txt_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Extract recipe steps from the 'table' field\n",
    "                if 'table' in data and isinstance(data['table'], list):\n",
    "                    for step in data['table']:\n",
    "                        # Validate required fields\n",
    "                        if self._validate_step(step):\n",
    "                            yield step\n",
    "                        else:\n",
    "                            print(f\"⚠️ Skipping invalid step in {file_path.name}\")\n",
    "                            \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"❌ Error parsing {file_path.name}: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Unexpected error processing {file_path.name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def _validate_step(self, step: Dict) -> bool:\n",
    "        \"\"\"Validate that a recipe step has required fields.\"\"\"\n",
    "        required_fields = ['instructions', 'actions', 'input', 'output']\n",
    "        return all(field in step for field in required_fields)\n",
    "    \n",
    "    def get_split_stats(self, split: str = \"val\") -> Dict:\n",
    "        \"\"\"Get statistics about a dataset split.\"\"\"\n",
    "        steps = list(self.iter_tables(split))\n",
    "        return {\n",
    "            'total_steps': len(steps),\n",
    "            'unique_actions': len(set(step['actions'] for step in steps)),\n",
    "            'avg_instruction_length': np.mean([len(step['instructions']) for step in steps])\n",
    "        }\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = PizzaDataLoader()\n",
    "print(\"✅ Data loader initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_data_loader"
   },
   "outputs": [],
   "source": [
    "# Test data loader and show statistics\n",
    "print(\"📊 Dataset Statistics:\")\n",
    "val_stats = data_loader.get_split_stats(\"val\")\n",
    "print(f\"Validation split: {val_stats}\")\n",
    "\n",
    "# Show a few sample steps\n",
    "print(\"\\n📝 Sample Recipe Steps:\")\n",
    "sample_steps = []\n",
    "for i, step in enumerate(data_loader.iter_tables(\"val\")):\n",
    "    sample_steps.append(step)\n",
    "    if i >= 2:  # Show first 3 steps\n",
    "        break\n",
    "\n",
    "for i, step in enumerate(sample_steps, 1):\n",
    "    print(f\"\\nStep {i}:\")\n",
    "    print(f\"  Instructions: {step['instructions'][:100]}...\")\n",
    "    print(f\"  Action: {step['actions']}\")\n",
    "    print(f\"  Input: {step['input']}\")\n",
    "    print(f\"  Output: {step['output']}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}  }
,
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prompt_section"
   },
   "source": [
    "## 3. Prompt Engineering\n",
    "\n",
    "Implement Chain-of-Thought prompting for cooking step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cot_prompt_generator"
   },
   "outputs": [],
   "source": [
    "class CoTPromptGenerator:\n",
    "    \"\"\"Creates Chain-of-Thought prompts for cooking step reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.system_message = (\n",
    "            \"You are a cooking-reasoning assistant. \"\n",
    "            \"Given an instruction and an action, predict the input comestibles \"\n",
    "            \"and the output comestible/result for that step. \"\n",
    "            \"Think step by step about what ingredients or items are needed (input) \"\n",
    "            \"and what results from the cooking action (output).\"\n",
    "        )\n",
    "    \n",
    "    def make_prompt(self, step: Dict) -> str:\n",
    "        \"\"\"Generate a Chain-of-Thought prompt for a recipe step.\n",
    "        \n",
    "        Args:\n",
    "            step: Recipe step dictionary with 'instructions' and 'actions'\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt with CoT trigger\n",
    "        \"\"\"\n",
    "        # Clean and escape the text\n",
    "        instructions = self._clean_text(step['instructions'])\n",
    "        actions = self._clean_text(step['actions'])\n",
    "        \n",
    "        prompt = (\n",
    "            f\"Instruction: {instructions}\\n\"\n",
    "            f\"Action: {actions}\\n\\n\"\n",
    "            \"Let's think step by step.\\n\\n\"\n",
    "            \"What are the input ingredients or items needed for this step?\\n\"\n",
    "            \"What is the output result after performing this action?\\n\\n\"\n",
    "            \"Please provide your answer in this format:\\n\"\n",
    "            \"Input: [your prediction]\\n\"\n",
    "            \"Output: [your prediction]\"\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text for prompting.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return str(text)\n",
    "        \n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Handle special characters that might cause issues\n",
    "        text = text.replace('\\u2012', '-')  # Replace en-dash with hyphen\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_system_message(self) -> str:\n",
    "        \"\"\"Get the system message for the assistant.\"\"\"\n",
    "        return self.system_message\n",
    "\n",
    "# Initialize prompt generator\n",
    "prompt_generator = CoTPromptGenerator()\n",
    "print(\"✅ CoT prompt generator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_prompt_generation"
   },
   "outputs": [],
   "source": [
    "# Test prompt generation with sample data\n",
    "print(\"🧪 Testing Prompt Generation:\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SYSTEM MESSAGE:\")\n",
    "print(\"=\"*50)\n",
    "print(prompt_generator.get_system_message())\n",
    "\n",
    "# Get a sample step for testing\n",
    "sample_step = next(data_loader.iter_tables(\"val\"))\n",
    "test_prompt = prompt_generator.make_prompt(sample_step)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE PROMPT:\")\n",
    "print(\"=\"*50)\n",
    "print(test_prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GROUND TRUTH:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input: {sample_step['input']}\")\n",
    "print(f\"Output: {sample_step['output']}\")"
   ]
  }  }
,
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_section"
   },
   "source": [
    "## 4. Model Inference\n",
    "\n",
    "Configure GPT-4 Turbo API calls with proper error handling and rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpt4_predictor_class"
   },
   "outputs": [],
   "source": [
    "class GPT4Predictor:\n",
    "    \"\"\"Handles OpenAI API calls and response parsing for GPT-4 Turbo.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\", max_retries: int = 3, base_delay: float = 0.5):\n",
    "        self.client = openai.Client()\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay = base_delay\n",
    "        \n",
    "        # Test API connection\n",
    "        try:\n",
    "            # Make a simple test call\n",
    "            test_response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                max_tokens=5,\n",
    "                temperature=0\n",
    "            )\n",
    "            print(f\"✅ API connection successful with model: {self.model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ API connection failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, prompt: str, system_message: str) -> str:\n",
    "        \"\"\"Generate prediction using GPT-4 Turbo with retry logic.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User prompt for the model\n",
    "            system_message: System message to set context\n",
    "            \n",
    "        Returns:\n",
    "            str: Model response text\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0,  # Deterministic responses\n",
    "                    max_tokens=200  # Reasonable limit for input/output predictions\n",
    "                )\n",
    "                \n",
    "                return response.choices[0].message.content.strip()\n",
    "                \n",
    "            except openai.RateLimitError as e:\n",
    "                wait_time = self.base_delay * (2 ** attempt)\n",
    "                print(f\"⏳ Rate limit hit, waiting {wait_time}s (attempt {attempt + 1}/{self.max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "            except openai.APIError as e:\n",
    "                print(f\"❌ API error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(self.base_delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Unexpected error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(self.base_delay)\n",
    "        \n",
    "        raise Exception(f\"Failed to get response after {self.max_retries} attempts\")\n",
    "    \n",
    "    def parse_io(self, response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Parse input and output predictions from model response.\n",
    "        \n",
    "        Args:\n",
    "            response: Raw model response text\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[str, str]: (predicted_input, predicted_output)\n",
    "        \"\"\"\n",
    "        # Try multiple regex patterns for flexibility\n",
    "        input_patterns = [\n",
    "            r\"Input\\s*[:：]\\s*(.+?)(?=\\n|Output|$)\",\n",
    "            r\"input\\s*[:：]\\s*(.+?)(?=\\n|output|$)\",\n",
    "            r\"INPUT\\s*[:：]\\s*(.+?)(?=\\n|OUTPUT|$)\"\n",
    "        ]\n",
    "        \n",
    "        output_patterns = [\n",
    "            r\"Output\\s*[:：]\\s*(.+?)(?=\\n|$)\",\n",
    "            r\"output\\s*[:：]\\s*(.+?)(?=\\n|$)\",\n",
    "            r\"OUTPUT\\s*[:：]\\s*(.+?)(?=\\n|$)\"\n",
    "        ]\n",
    "        \n",
    "        predicted_input = \"\"\n",
    "        predicted_output = \"\"\n",
    "        \n",
    "        # Try to extract input\n",
    "        for pattern in input_patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                predicted_input = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        # Try to extract output\n",
    "        for pattern in output_patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                predicted_output = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        return predicted_input, predicted_output\n",
    "    \n",
    "    def add_delay(self):\n",
    "        \"\"\"Add base delay between requests for rate limiting.\"\"\"\n",
    "        time.sleep(self.base_delay)\n",
    "\n",
    "# Initialize GPT-4 predictor\n",
    "predictor = GPT4Predictor()\n",
    "print(\"✅ GPT-4 predictor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_gpt4_prediction"
   },
   "outputs": [],
   "source": [
    "# Test GPT-4 prediction with sample data\n",
    "print(\"🧪 Testing GPT-4 Prediction:\")\n",
    "\n",
    "# Use the same sample step from before\n",
    "test_prompt = prompt_generator.make_prompt(sample_step)\n",
    "system_msg = prompt_generator.get_system_message()\n",
    "\n",
    "print(\"\\n⏳ Generating prediction...\")\n",
    "response = predictor.predict(test_prompt, system_msg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response)\n",
    "\n",
    "# Parse the response\n",
    "pred_input, pred_output = predictor.parse_io(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PARSED PREDICTIONS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Predicted Input: '{pred_input}'\")\n",
    "print(f\"Predicted Output: '{pred_output}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GROUND TRUTH COMPARISON:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Ground Truth Input:  '{sample_step['input']}'\")\n",
    "print(f\"Ground Truth Output: '{sample_step['output']}'\")\n",
    "\n",
    "# Check exact matches\n",
    "input_match = pred_input.lower().strip() == sample_step['input'].lower().strip()\n",
    "output_match = pred_output.lower().strip() == sample_step['output'].lower().strip()\n",
    "\n",
    "print(f\"\\n📊 Exact Match Results:\")\n",
    "print(f\"Input Match: {'✅' if input_match else '❌'}\")\n",
    "print(f\"Output Match: {'✅' if output_match else '❌'}\")"
   ]
  }  },
  {

   "cell_type": "markdown",
   "metadata": {
    "id": "batch_section"
   },
   "source": [
    "## 5. Batch Processing\n",
    "\n",
    "Execute predictions on the validation split with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_processing_setup"
   },
   "outputs": [],
   "source": [
    "def run_batch_predictions(split: str = \"val\", max_samples: int = None, save_interval: int = 50):\n",
    "    \"\"\"Run batch predictions on the dataset with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        split: Dataset split to process ('val' or 'train')\n",
    "        max_samples: Maximum number of samples to process (None for all)\n",
    "        save_interval: Save results every N samples\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Results with predictions and ground truth\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting batch prediction on {split} split...\")\n",
    "    \n",
    "    # Collect all steps first to get total count\n",
    "    all_steps = list(data_loader.iter_tables(split))\n",
    "    \n",
    "    if max_samples:\n",
    "        all_steps = all_steps[:max_samples]\n",
    "    \n",
    "    print(f\"📊 Processing {len(all_steps)} recipe steps\")\n",
    "    \n",
    "    results = []\n",
    "    system_msg = prompt_generator.get_system_message()\n",
    "    \n",
    "    # Process with progress bar\n",
    "    for i, step in enumerate(tqdm(all_steps, desc=\"Generating predictions\")):\n",
    "        try:\n",
    "            # Generate prompt\n",
    "            prompt = prompt_generator.make_prompt(step)\n",
    "            \n",
    "            # Get prediction\n",
    "            response = predictor.predict(prompt, system_msg)\n",
    "            \n",
    "            # Parse response\n",
    "            pred_input, pred_output = predictor.parse_io(response)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'instructions': step['instructions'],\n",
    "                'actions': step['actions'],\n",
    "                'input': step['input'],\n",
    "                'output': step['output'],\n",
    "                'pred_input': pred_input,\n",
    "                'pred_output': pred_output,\n",
    "                'response': response  # Keep full response for debugging\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Add delay for rate limiting\n",
    "            predictor.add_delay()\n",
    "            \n",
    "            # Periodic saving\n",
    "            if (i + 1) % save_interval == 0:\n",
    "                temp_df = pd.DataFrame(results)\n",
    "                temp_df.to_csv(f\"temp_predictions_{i+1}.csv\", index=False)\n",
    "                print(f\"💾 Saved temporary results at step {i+1}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing step {i+1}: {e}\")\n",
    "            # Add placeholder result to maintain alignment\n",
    "            result = {\n",
    "                'instructions': step['instructions'],\n",
    "                'actions': step['actions'],\n",
    "                'input': step['input'],\n",
    "                'output': step['output'],\n",
    "                'pred_input': '',\n",
    "                'pred_output': '',\n",
    "                'response': f'ERROR: {str(e)}'\n",
    "            }\n",
    "            results.append(result)\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"✅ Batch processing complete! Processed {len(df)} steps\")\n",
    "    print(f\"📊 Success rate: {(df['pred_input'] != '').sum()}/{len(df)} ({(df['pred_input'] != '').mean():.1%})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✅ Batch processing function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_small_test"
   },
   "outputs": [],
   "source": [
    "# Run a small test first (10 samples)\n",
    "print(\"🧪 Running small test with 10 samples...\")\n",
    "test_df = run_batch_predictions(split=\"val\", max_samples=10)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n📋 Sample Results:\")\n",
    "display_cols = ['instructions', 'actions', 'input', 'output', 'pred_input', 'pred_output']\n",
    "print(test_df[display_cols].head())\n",
    "\n",
    "# Quick evaluation\n",
    "input_matches = (test_df['input'].str.lower().str.strip() == \n",
    "                test_df['pred_input'].str.lower().str.strip()).sum()\n",
    "output_matches = (test_df['output'].str.lower().str.strip() == \n",
    "                 test_df['pred_output'].str.lower().str.strip()).sum()\n",
    "\n",
    "print(f\"\\n📊 Quick Test Results:\")\n",
    "print(f\"Input EMA: {input_matches}/{len(test_df)} ({input_matches/len(test_df):.1%})\")\n",
    "print(f\"Output EMA: {output_matches}/{len(test_df)} ({output_matches/len(test_df):.1%})\")\n",
    "print(f\"Average EMA: {(input_matches + output_matches)/(2*len(test_df)):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full_batch_processing"
   },
   "outputs": [],
   "source": [
    "# Run full batch processing (uncomment when ready)\n",
    "# WARNING: This will process the entire validation split and may take 30-60 minutes\n",
    "# and cost several dollars in API calls\n",
    "\n",
    "print(\"⚠️  Ready to run full batch processing on validation split\")\n",
    "print(\"💰 Estimated cost: $3-10 depending on dataset size\")\n",
    "print(\"⏱️  Estimated time: 30-60 minutes\")\n",
    "print(\"\\n🔧 To run full processing, uncomment the lines below:\")\n",
    "print(\"# full_df = run_batch_predictions(split='val')\")\n",
    "print(\"# full_df.to_csv('gpt4_predictions_full.csv', index=False)\")\n",
    "print(\"# print('💾 Full results saved to gpt4_predictions_full.csv')\")\n",
    "\n",
    "# Uncomment these lines when ready to run full experiment:\n",
    "# full_df = run_batch_predictions(split=\"val\")\n",
    "# full_df.to_csv(\"gpt4_predictions_full.csv\", index=False)\n",
    "# print(\"💾 Full results saved to gpt4_predictions_full.csv\")"
   ]
  }  
},
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## 6. Evaluation & Reporting\n",
    "\n",
    "Calculate metrics and compare results against published benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_calculator_class"
   },
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"Computes evaluation metrics for model predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize evaluation libraries\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.bertscore = evaluate.load(\"bertscore\")\n",
    "    \n",
    "    def calculate_ema(self, predictions: List[str], references: List[str]) -> float:\n",
    "        \"\"\"Calculate Exact Match Accuracy with normalization.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of predicted strings\n",
    "            references: List of ground truth strings\n",
    "            \n",
    "        Returns:\n",
    "            float: EMA score (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        if len(predictions) != len(references):\n",
    "            raise ValueError(\"Predictions and references must have same length\")\n",
    "        \n",
    "        matches = 0\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Normalize strings for comparison\n",
    "            pred_norm = self._normalize_string(pred)\n",
    "            ref_norm = self._normalize_string(ref)\n",
    "            \n",
    "            if pred_norm == ref_norm:\n",
    "                matches += 1\n",
    "        \n",
    "        return matches / len(predictions) if predictions else 0.0\n",
    "    \n",
    "    def calculate_rouge_l(self, predictions: List[str], references: List[str]) -> float:\n",
    "        \"\"\"Calculate Rouge-L score.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of predicted strings\n",
    "            references: List of ground truth strings\n",
    "            \n",
    "        Returns:\n",
    "            float: Rouge-L F1 score\n",
    "        \"\"\"\n",
    "        if not predictions or not references:\n",
    "            return 0.0\n",
    "        \n",
    "        # Handle empty predictions\n",
    "        clean_predictions = [pred if pred else \"\" for pred in predictions]\n",
    "        clean_references = [ref if ref else \"\" for ref in references]\n",
    "        \n",
    "        try:\n",
    "            results = self.rouge.compute(\n",
    "                predictions=clean_predictions,\n",
    "                references=clean_references\n",
    "            )\n",
    "            return results[\"rougeL\"]\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Rouge-L calculation error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_bertscore(self, predictions: List[str], references: List[str]) -> float:\n",
    "        \"\"\"Calculate BERTScore F1.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of predicted strings\n",
    "            references: List of ground truth strings\n",
    "            \n",
    "        Returns:\n",
    "            float: Average BERTScore F1\n",
    "        \"\"\"\n",
    "        if not predictions or not references:\n",
    "            return 0.0\n",
    "        \n",
    "        # Handle empty predictions\n",
    "        clean_predictions = [pred if pred else \"empty\" for pred in predictions]\n",
    "        clean_references = [ref if ref else \"empty\" for ref in references]\n",
    "        \n",
    "        try:\n",
    "            results = self.bertscore.compute(\n",
    "                predictions=clean_predictions,\n",
    "                references=clean_references,\n",
    "                lang=\"en\"\n",
    "            )\n",
    "            return np.mean(results[\"f1\"])\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ BERTScore calculation error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _normalize_string(self, text: str) -> str:\n",
    "        \"\"\"Normalize string for comparison.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # Convert to lowercase and strip whitespace\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Handle special cases\n",
    "        if text in ['na', 'n/a', 'none', '']:\n",
    "            return ''\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def evaluate_predictions(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Comprehensive evaluation of predictions.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with predictions and ground truth\n",
    "            \n",
    "        Returns:\n",
    "            Dict: All evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"📊 Calculating evaluation metrics...\")\n",
    "        \n",
    "        # Extract predictions and references\n",
    "        pred_inputs = df['pred_input'].tolist()\n",
    "        true_inputs = df['input'].tolist()\n",
    "        pred_outputs = df['pred_output'].tolist()\n",
    "        true_outputs = df['output'].tolist()\n",
    "        \n",
    "        # Calculate EMA\n",
    "        ema_input = self.calculate_ema(pred_inputs, true_inputs)\n",
    "        ema_output = self.calculate_ema(pred_outputs, true_outputs)\n",
    "        ema_avg = (ema_input + ema_output) / 2\n",
    "        \n",
    "        # Calculate Rouge-L\n",
    "        rouge_input = self.calculate_rouge_l(pred_inputs, true_inputs)\n",
    "        rouge_output = self.calculate_rouge_l(pred_outputs, true_outputs)\n",
    "        \n",
    "        # Calculate BERTScore (focus on outputs as they're more complex)\n",
    "        bertscore_output = self.calculate_bertscore(pred_outputs, true_outputs)\n",
    "        \n",
    "        metrics = {\n",
    "            'ema_input': ema_input,\n",
    "            'ema_output': ema_output,\n",
    "            'ema_average': ema_avg,\n",
    "            'rouge_l_input': rouge_input,\n",
    "            'rouge_l_output': rouge_output,\n",
    "            'bertscore_f1': bertscore_output,\n",
    "            'total_samples': len(df)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Initialize metrics calculator\n",
    "metrics_calc = MetricsCalculator()\n",
    "print(\"✅ Metrics calculator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_test_results"
   },
   "outputs": [],
   "source": [
    "# Evaluate the test results\n",
    "print(\"📊 Evaluating test results...\")\n",
    "test_metrics = metrics_calc.evaluate_predictions(test_df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS (10 samples)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"EMA Input:        {test_metrics['ema_input']:.1%}\")\n",
    "print(f\"EMA Output:       {test_metrics['ema_output']:.1%}\")\n",
    "print(f\"EMA Average:      {test_metrics['ema_average']:.1%}\")\n",
    "print(f\"Rouge-L Input:    {test_metrics['rouge_l_input']:.3f}\")\n",
    "print(f\"Rouge-L Output:   {test_metrics['rouge_l_output']:.3f}\")\n",
    "print(f\"BERTScore F1:     {test_metrics['bertscore_f1']:.3f}\")\n",
    "print(f\"Total Samples:    {test_metrics['total_samples']}\")\n",
    "\n",
    "# Compare with paper benchmarks\n",
    "paper_benchmarks = {\n",
    "    'ema_average': 0.267,  # 26.7%\n",
    "    'rouge_l_input': 0.514,  # 51.4\n",
    "    'rouge_l_output': 0.523  # 52.3\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH PAPER BENCHMARKS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Test':<10} {'Paper':<10} {'Status':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric, paper_value in paper_benchmarks.items():\n",
    "    test_value = test_metrics[metric]\n",
    "    status = \"✅ BETTER\" if test_value >= paper_value else \"❌ LOWER\"\n",
    "    print(f\"{metric:<20} {test_value:<10.3f} {paper_value:<10.3f} {status}\")\n",
    "\n",
    "print(\"\\n⚠️  Note: These are results on only 10 test samples.\")\n",
    "print(\"📈 Run full evaluation for meaningful comparison with paper.\")"
   ]
  }  },
  
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full_evaluation_function"
   },
   "outputs": [],
   "source": [
    "def evaluate_full_results(csv_path: str = \"gpt4_predictions_full.csv\"):\n",
    "    \"\"\"Evaluate full results and compare with paper benchmarks.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file with full predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load results\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"📁 Loaded {len(df)} predictions from {csv_path}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = metrics_calc.evaluate_predictions(df)\n",
    "        \n",
    "        # Paper benchmarks\n",
    "        paper_benchmarks = {\n",
    "            'EMA Average': {'our': metrics['ema_average'], 'paper': 0.267, 'format': '.1%'},\n",
    "            'Rouge-L Input': {'our': metrics['rouge_l_input'], 'paper': 0.514, 'format': '.3f'},\n",
    "            'Rouge-L Output': {'our': metrics['rouge_l_output'], 'paper': 0.523, 'format': '.3f'},\n",
    "            'BERTScore F1': {'our': metrics['bertscore_f1'], 'paper': None, 'format': '.3f'}\n",
    "        }\n",
    "        \n",
    "        # Display comprehensive results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🎯 FINAL RESULTS - PAPER REPRODUCTION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"📊 Dataset: PizzaCommonSense validation split ({metrics['total_samples']} samples)\")\n",
    "        print(f\"🤖 Model: GPT-4 Turbo (gpt-4o-mini) with Chain-of-Thought\")\n",
    "        print(f\"📅 Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"{'Metric':<20} {'Our Result':<15} {'Paper Benchmark':<18} {'Status':<15} {'Difference'}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        success_count = 0\n",
    "        total_comparable = 0\n",
    "        \n",
    "        for metric_name, values in paper_benchmarks.items():\n",
    "            our_val = values['our']\n",
    "            paper_val = values['paper']\n",
    "            fmt = values['format']\n",
    "            \n",
    "            if paper_val is not None:\n",
    "                total_comparable += 1\n",
    "                diff = our_val - paper_val\n",
    "                status = \"✅ BETTER\" if our_val >= paper_val else \"❌ LOWER\"\n",
    "                if our_val >= paper_val:\n",
    "                    success_count += 1\n",
    "                \n",
    "                our_str = f\"{our_val:{fmt}}\"\n",
    "                paper_str = f\"{paper_val:{fmt}}\"\n",
    "                diff_str = f\"{diff:+{fmt}}\"\n",
    "            else:\n",
    "                status = \"📊 NEW METRIC\"\n",
    "                our_str = f\"{our_val:{fmt}}\"\n",
    "                paper_str = \"N/A\"\n",
    "                diff_str = \"N/A\"\n",
    "            \n",
    "            print(f\"{metric_name:<20} {our_str:<15} {paper_str:<18} {status:<15} {diff_str}\")\n",
    "        \n",
    "        # Success summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🏆 REPRODUCTION SUCCESS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        success_rate = success_count / total_comparable if total_comparable > 0 else 0\n",
    "        print(f\"✅ Metrics meeting/exceeding paper: {success_count}/{total_comparable} ({success_rate:.1%})\")\n",
    "        \n",
    "        if success_count > 0:\n",
    "            print(\"🎉 SUCCESS: At least one metric meets the paper benchmark!\")\n",
    "            print(\"📈 Reproduction experiment successful!\")\n",
    "        else:\n",
    "            print(\"⚠️  No metrics exceed paper benchmarks\")\n",
    "            print(\"🔍 Consider adjusting prompts or trying different techniques\")\n",
    "        \n",
    "        # Detailed breakdown\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"📋 DETAILED BREAKDOWN\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"EMA Input:        {metrics['ema_input']:.1%}\")\n",
    "        print(f\"EMA Output:       {metrics['ema_output']:.1%}\")\n",
    "        print(f\"EMA Average:      {metrics['ema_average']:.1%} (Target: 26.7%)\")\n",
    "        print(f\"Rouge-L Input:    {metrics['rouge_l_input']:.3f} (Target: 0.514)\")\n",
    "        print(f\"Rouge-L Output:   {metrics['rouge_l_output']:.3f} (Target: 0.523)\")\n",
    "        print(f\"BERTScore F1:     {metrics['bertscore_f1']:.3f} (New metric)\")\n",
    "        \n",
    "        # Save metrics to JSON\n",
    "        metrics_file = \"final_metrics.json\"\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        print(f\"\\n💾 Detailed metrics saved to {metrics_file}\")\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File {csv_path} not found. Run full batch processing first.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating results: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Full evaluation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_full_evaluation"
   },
   "outputs": [],
   "source": [
    "# Run full evaluation (uncomment after running full batch processing)\n",
    "print(\"🎯 Ready to evaluate full results\")\n",
    "print(\"\\n🔧 To run full evaluation, uncomment the line below:\")\n",
    "print(\"# final_metrics = evaluate_full_results('gpt4_predictions_full.csv')\")\n",
    "\n",
    "# Uncomment this line after running full batch processing:\n",
    "# final_metrics = evaluate_full_results(\"gpt4_predictions_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "extensions_section"
   },
   "source": [
    "## 7. Extensions & Advanced Techniques\n",
    "\n",
    "Optional implementations for Self-Consistency and Few-Shot CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "self_consistency_framework"
   },
   "outputs": [],
   "source": [
    "# Self-Consistency Framework (Optional)\n",
    "def self_consistency_predict(step: Dict, n_samples: int = 3, temperature: float = 0.7):\n",
    "    \"\"\"Generate multiple predictions and use majority voting.\n",
    "    \n",
    "    Args:\n",
    "        step: Recipe step dictionary\n",
    "        n_samples: Number of predictions to generate\n",
    "        temperature: Temperature for diverse generation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, str]: Final input and output predictions\n",
    "    \"\"\"\n",
    "    prompt = prompt_generator.make_prompt(step)\n",
    "    system_msg = prompt_generator.get_system_message()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        try:\n",
    "            # Generate with temperature for diversity\n",
    "            response = predictor.client.chat.completions.create(\n",
    "                model=predictor.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            pred_input, pred_output = predictor.parse_io(response.choices[0].message.content)\n",
    "            predictions.append((pred_input, pred_output))\n",
    "            \n",
    "            predictor.add_delay()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in self-consistency sample {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not predictions:\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    # Simple majority voting (could be enhanced)\n",
    "    input_votes = [pred[0] for pred in predictions]\n",
    "    output_votes = [pred[1] for pred in predictions]\n",
    "    \n",
    "    # Return most common prediction (or first if tie)\n",
    "    final_input = max(set(input_votes), key=input_votes.count) if input_votes else \"\"\n",
    "    final_output = max(set(output_votes), key=output_votes.count) if output_votes else \"\"\n",
    "    \n",
    "    return final_input, final_output\n",
    "\n",
    "print(\"🔄 Self-Consistency framework ready (optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "few_shot_framework"
   },
   "outputs": [],
   "source": [
    "# Few-Shot CoT Framework (Optional)\n",
    "def create_few_shot_prompt(step: Dict, n_shots: int = 3) -> str:\n",
    "    \"\"\"Create few-shot prompt with examples from training data.\n",
    "    \n",
    "    Args:\n",
    "        step: Target recipe step\n",
    "        n_shots: Number of examples to include\n",
    "        \n",
    "    Returns:\n",
    "        str: Few-shot prompt with examples\n",
    "    \"\"\"\n",
    "    # Get examples from training data\n",
    "    examples = []\n",
    "    for i, train_step in enumerate(data_loader.iter_tables(\"train\")):\n",
    "        if i >= n_shots:\n",
    "            break\n",
    "        examples.append(train_step)\n",
    "    \n",
    "    # Build few-shot prompt\n",
    "    prompt_parts = []\n",
    "    \n",
    "    # Add examples\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        prompt_parts.append(f\"Example {i}:\")\n",
    "        prompt_parts.append(f\"Instruction: {example['instructions']}\")\n",
    "        prompt_parts.append(f\"Action: {example['actions']}\")\n",
    "        prompt_parts.append(\"Let's think step by step.\")\n",
    "        prompt_parts.append(f\"Input: {example['input']}\")\n",
    "        prompt_parts.append(f\"Output: {example['output']}\")\n",
    "        prompt_parts.append(\"\")\n",
    "    \n",
    "    # Add target question\n",
    "    prompt_parts.append(\"Now solve this:\")\n",
    "    prompt_parts.append(f\"Instruction: {step['instructions']}\")\n",
    "    prompt_parts.append(f\"Action: {step['actions']}\")\n",
    "    prompt_parts.append(\"Let's think step by step.\")\n",
    "    prompt_parts.append(\"Input: \")\n",
    "    prompt_parts.append(\"Output: \")\n",
    "    \n",
    "    return \"\\n\".join(prompt_parts)\n",
    "\n",
    "print(\"📚 Few-Shot CoT framework ready (optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "documentation_section"
   },
   "source": [
    "## 8. Documentation & Usage Instructions\n",
    "\n",
    "### Setup Instructions\n",
    "\n",
    "1. **Environment Setup**:\n",
    "   - Run the library installation cell\n",
    "   - Enter your OpenAI API key when prompted\n",
    "   - Verify all imports work correctly\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Ensure `data/PizzaCommonsense/` folder exists with `train/` and `val/` subfolders\n",
    "   - Each subfolder should contain `.txt` files with recipe JSON data\n",
    "   - Test data loading with the sample code\n",
    "\n",
    "3. **Running Experiments**:\n",
    "   - Start with small test (10 samples) to verify everything works\n",
    "   - Uncomment full batch processing when ready\n",
    "   - Monitor progress and costs during execution\n",
    "\n",
    "### Cost Estimation\n",
    "\n",
    "- **Model**: gpt-4o-mini (~$0.15/1K input tokens, ~$0.60/1K output tokens)\n",
    "- **Estimated cost per sample**: ~$0.01-0.02\n",
    "- **Full validation split**: $5-15 depending on dataset size\n",
    "- **Runtime**: 30-60 minutes for full dataset\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Common Issues**:\n",
    "- **API Key Error**: Ensure key is correctly entered and has sufficient credits\n",
    "- **Rate Limits**: Built-in delays should handle this automatically\n",
    "- **Parsing Errors**: Check model responses if predictions seem empty\n",
    "- **Memory Issues**: Process in smaller batches if needed\n",
    "\n",
    "**Performance Tips**:\n",
    "- Use `max_samples` parameter for testing\n",
    "- Monitor intermediate saves during long runs\n",
    "- Check sample predictions before full run\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "**Success Criteria**: At least one metric should meet/exceed paper values:\n",
    "- EMA Average: ≥26.7%\n",
    "- Rouge-L Input: ≥0.514\n",
    "- Rouge-L Output: ≥0.523\n",
    "\n",
    "**Metrics Explanation**:\n",
    "- **EMA**: Exact string match after normalization\n",
    "- **Rouge-L**: Longest common subsequence similarity\n",
    "- **BERTScore**: Semantic similarity using BERT embeddings\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "If results don't meet benchmarks, try:\n",
    "1. **Self-Consistency**: Generate multiple predictions and vote\n",
    "2. **Few-Shot CoT**: Add training examples to prompts\n",
    "3. **Prompt Engineering**: Refine instructions and format\n",
    "4. **Different Models**: Try gpt-4-turbo or other variants"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}