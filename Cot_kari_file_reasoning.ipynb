{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"43c752226a414e8780fc73a1af5fc558":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01aacfa90cde4ac58b19bbe5dc8dca4d","IPY_MODEL_bd41405dcd234a8b91bf3c1863839d0c","IPY_MODEL_3a7d3daab35245719486f3f40d9083f9"],"layout":"IPY_MODEL_c75d042df7884f039fbf408ce7aded1a"}},"01aacfa90cde4ac58b19bbe5dc8dca4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8afd269c0a1a488dbfb3643af50667df","placeholder":"​","style":"IPY_MODEL_b4cefa8a5bf6400184d11cfe1ce66424","value":"Downloading builder script: "}},"bd41405dcd234a8b91bf3c1863839d0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4933fe49607b44469610c448e2a277a1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc9cc8d43ea6450cb0feffde614a674f","value":1}},"3a7d3daab35245719486f3f40d9083f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff4e9eaccda44f0daa2311b4d9d0a52d","placeholder":"​","style":"IPY_MODEL_3ec28265d0fa46198ffa8fa326e88515","value":" 7.95k/? [00:00&lt;00:00, 651kB/s]"}},"c75d042df7884f039fbf408ce7aded1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8afd269c0a1a488dbfb3643af50667df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4cefa8a5bf6400184d11cfe1ce66424":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4933fe49607b44469610c448e2a277a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"fc9cc8d43ea6450cb0feffde614a674f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff4e9eaccda44f0daa2311b4d9d0a52d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ec28265d0fa46198ffa8fa326e88515":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de558cd8979e4274b104a84d97622834":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d80781a73014fabaa4d380bec43f88a","IPY_MODEL_14b48b6fd1ae43579aa8f5bfd0a24df3","IPY_MODEL_e5ffe43124ba4839aa0418d6bb443b5c"],"layout":"IPY_MODEL_ed42af7ddf1949028b6eb963a194e78e"}},"4d80781a73014fabaa4d380bec43f88a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b7d5a9716234ac6b9339dfa1ea34bcc","placeholder":"​","style":"IPY_MODEL_459a3f2e2aa540e1b222925055cd16f6","value":"tokenizer_config.json: 100%"}},"14b48b6fd1ae43579aa8f5bfd0a24df3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d294cbb6534943cda4520b57a1cfafd1","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4749f2cb8cf44c219fec170c47540aa2","value":25}},"e5ffe43124ba4839aa0418d6bb443b5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5282d9a0002147aaa17dd60f378898d7","placeholder":"​","style":"IPY_MODEL_ccadb438444f460a8163b7f8e8804b21","value":" 25.0/25.0 [00:00&lt;00:00, 2.73kB/s]"}},"ed42af7ddf1949028b6eb963a194e78e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b7d5a9716234ac6b9339dfa1ea34bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"459a3f2e2aa540e1b222925055cd16f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d294cbb6534943cda4520b57a1cfafd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4749f2cb8cf44c219fec170c47540aa2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5282d9a0002147aaa17dd60f378898d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccadb438444f460a8163b7f8e8804b21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf276063cc5345b99e3140ba7905552d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bea05b8c5e384ec7b08241eaed285468","IPY_MODEL_18d79f96368646dd80548d4803d9ac29","IPY_MODEL_eca7d70f6c5a4ac1aa61a670c9131e9f"],"layout":"IPY_MODEL_e9d28eab952d44269d53893ee0528848"}},"bea05b8c5e384ec7b08241eaed285468":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef89d0c432be438db7902edd2cd9ed00","placeholder":"​","style":"IPY_MODEL_d7a4d8d6299140d4b3fe20f1b21210c3","value":"config.json: 100%"}},"18d79f96368646dd80548d4803d9ac29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98d71cce7f8748f3af1fb215ab29f46b","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdedf57326dd442185bcf6f4cdff8125","value":482}},"eca7d70f6c5a4ac1aa61a670c9131e9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_634d92fa3e6548dbad8e14cb7c79c377","placeholder":"​","style":"IPY_MODEL_1d7204f3c3d14ff199fd1d8fad0b3a80","value":" 482/482 [00:00&lt;00:00, 50.2kB/s]"}},"e9d28eab952d44269d53893ee0528848":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef89d0c432be438db7902edd2cd9ed00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7a4d8d6299140d4b3fe20f1b21210c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98d71cce7f8748f3af1fb215ab29f46b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdedf57326dd442185bcf6f4cdff8125":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"634d92fa3e6548dbad8e14cb7c79c377":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d7204f3c3d14ff199fd1d8fad0b3a80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e17a8fc9a764009ac1f5a9e6f357707":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e375a8a49384f5f81c5b80c9ffc37a8","IPY_MODEL_a1ccb6f10ec24920a9da2c26c7e460fa","IPY_MODEL_17cc5bf72681429a8e82f78f5f2f88d7"],"layout":"IPY_MODEL_3ea97f4cd91446e898b4fcabcddb78d3"}},"9e375a8a49384f5f81c5b80c9ffc37a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4848cece46e45d69d1ccd287cb63bcc","placeholder":"​","style":"IPY_MODEL_bc2fa0cd2f38475c862d9a1b47aba7ab","value":"vocab.json: 100%"}},"a1ccb6f10ec24920a9da2c26c7e460fa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ad8ee7c30c34242995edc344f59b4ae","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d025b4c477cf43c8bcf44dd32edbed40","value":898823}},"17cc5bf72681429a8e82f78f5f2f88d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23fb09519bdc4d99b659fa15374c4e7c","placeholder":"​","style":"IPY_MODEL_905b02bc019e4329b1e5833bf6e295b0","value":" 899k/899k [00:00&lt;00:00, 10.8MB/s]"}},"3ea97f4cd91446e898b4fcabcddb78d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4848cece46e45d69d1ccd287cb63bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc2fa0cd2f38475c862d9a1b47aba7ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ad8ee7c30c34242995edc344f59b4ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d025b4c477cf43c8bcf44dd32edbed40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23fb09519bdc4d99b659fa15374c4e7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"905b02bc019e4329b1e5833bf6e295b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed508ca7d63445b08a9a96801386beb7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e39bd66aa648486491daf53dda66b7c7","IPY_MODEL_0c9b61633de8466b97ca2b103737b818","IPY_MODEL_cc26e56455454bb5b60be44afa1c9936"],"layout":"IPY_MODEL_45dbdbc628524cd2a536b96eede046a3"}},"e39bd66aa648486491daf53dda66b7c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a802b0d0cd34e68a867e980dd6109c6","placeholder":"​","style":"IPY_MODEL_1b0880f761fb4cf8afc74b4ca85e8a0c","value":"merges.txt: 100%"}},"0c9b61633de8466b97ca2b103737b818":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_143c8e119e914ae1b0ebd37c30d2ae8c","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f22eec95f92a449b80dfba26756405f6","value":456318}},"cc26e56455454bb5b60be44afa1c9936":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0530cfe28036464f9d51c182bc9bd737","placeholder":"​","style":"IPY_MODEL_26d98286658543b5b400abe7102127e7","value":" 456k/456k [00:00&lt;00:00, 18.1MB/s]"}},"45dbdbc628524cd2a536b96eede046a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a802b0d0cd34e68a867e980dd6109c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b0880f761fb4cf8afc74b4ca85e8a0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"143c8e119e914ae1b0ebd37c30d2ae8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f22eec95f92a449b80dfba26756405f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0530cfe28036464f9d51c182bc9bd737":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26d98286658543b5b400abe7102127e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5cd584658fd4ce3a6f863c34280959c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a525535073c94630ba49da8f0e9650b3","IPY_MODEL_c45296c21d034d8d9c2cbd097a0185a4","IPY_MODEL_a72ca0ad518f4c7fb39a011eeacaf4fc"],"layout":"IPY_MODEL_1f2f845a040e421e8737c90113df8199"}},"a525535073c94630ba49da8f0e9650b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ade815fe11a3436b86d26d96fee7135e","placeholder":"​","style":"IPY_MODEL_58214d0f6c954837a5b9a49e09da5c47","value":"tokenizer.json: 100%"}},"c45296c21d034d8d9c2cbd097a0185a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_21305d14e4e044ff985d700341aa7980","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b017bd7e26cb46cc952b4ed06a308cf3","value":1355863}},"a72ca0ad518f4c7fb39a011eeacaf4fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc05f5a02de9445ebf12a9da2647992e","placeholder":"​","style":"IPY_MODEL_8b5f6fb9498744b099d5e4dc27763aaf","value":" 1.36M/1.36M [00:00&lt;00:00, 50.2MB/s]"}},"1f2f845a040e421e8737c90113df8199":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ade815fe11a3436b86d26d96fee7135e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58214d0f6c954837a5b9a49e09da5c47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21305d14e4e044ff985d700341aa7980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b017bd7e26cb46cc952b4ed06a308cf3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc05f5a02de9445ebf12a9da2647992e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b5f6fb9498744b099d5e4dc27763aaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eabeb865f0e2441c9aed1cbbf49e7251":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d37fa53f4c7c42f191f35c58522d4d72","IPY_MODEL_8665e92b087e4d7b8ba0914de472bf7b","IPY_MODEL_da0359a7f66b4b8ea7bb0d56f50a7bbd"],"layout":"IPY_MODEL_459616c86ce542cd8b5025880c91b9e7"}},"d37fa53f4c7c42f191f35c58522d4d72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4df0419c795e4063ae838ee153bc2b6d","placeholder":"​","style":"IPY_MODEL_005117cf5f974d518818a9139eafaa9e","value":"model.safetensors: 100%"}},"8665e92b087e4d7b8ba0914de472bf7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_291212e7d82941bea84bd19010412d25","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af15ba5eb79748b1ab7ee930a1762f5c","value":1421700479}},"da0359a7f66b4b8ea7bb0d56f50a7bbd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba7e7b621ad54ae68c278fff70c92237","placeholder":"​","style":"IPY_MODEL_8d22c0e1ada04723a0991ab088743c67","value":" 1.42G/1.42G [00:06&lt;00:00, 342MB/s]"}},"459616c86ce542cd8b5025880c91b9e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4df0419c795e4063ae838ee153bc2b6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"005117cf5f974d518818a9139eafaa9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"291212e7d82941bea84bd19010412d25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af15ba5eb79748b1ab7ee930a1762f5c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba7e7b621ad54ae68c278fff70c92237":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d22c0e1ada04723a0991ab088743c67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import sys\n","!{sys.executable} -m pip uninstall -y openai\n","!{sys.executable} -m pip install -U \"openai>=1.0.0\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqEXYAhrNZg1","executionInfo":{"status":"ok","timestamp":1755017825651,"user_tz":-60,"elapsed":6257,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}},"outputId":"30fe4764-0a6e-46d2-b3b1-135693b33550"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: openai 1.99.1\n","Uninstalling openai-1.99.1:\n","  Successfully uninstalled openai-1.99.1\n","Collecting openai>=1.0.0\n","  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0) (4.14.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0) (0.4.1)\n","Downloading openai-1.99.9-py3-none-any.whl (786 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: openai\n","Successfully installed openai-1.99.9\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"I5WCJhQtmjFW","executionInfo":{"status":"ok","timestamp":1755017830383,"user_tz":-60,"elapsed":4715,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"094f5dbe-9237-422d-851c-76580f3d7220"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip -q install tqdm rouge-score evaluate pandas tiktoken\n"]},{"cell_type":"code","source":["!pip show openai | grep Version\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xB3u8PwwjH6L","outputId":"0e0cc37a-8be9-4480-ad51-507eed3276f0","executionInfo":{"status":"ok","timestamp":1755017831856,"user_tz":-60,"elapsed":1464,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Version: 1.99.9\n"]}]},{"cell_type":"code","source":["# Import required modules\n","import os\n","import json\n","import time\n","import re\n","import getpass\n","from pathlib import Path\n","from typing import Dict, List, Iterator, Tuple\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import openai\n","import evaluate\n","\n","print(\"✅ All libraries imported successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3hXZtETmpbZ","outputId":"60f60eb3-a5da-4ed5-ea67-dc3c7248a174","executionInfo":{"status":"ok","timestamp":1755017855205,"user_tz":-60,"elapsed":20684,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ All libraries imported successfully!\n"]}]},{"cell_type":"code","source":["# Secure API key input\n","print(\"🔑 Please enter your OpenAI API key:\")\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"API Key: \")\n","\n","# Verify API key is set\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    print(\"✅ API key configured successfully!\")\n","else:\n","    print(\"❌ API key not set. Please run this cell again.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YatbNeNvm5tV","outputId":"b7164e5d-47bb-4083-cc54-f23455cb55f8","executionInfo":{"status":"ok","timestamp":1755017871751,"user_tz":-60,"elapsed":16540,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["🔑 Please enter your OpenAI API key:\n","API Key: ··········\n","✅ API key configured successfully!\n"]}]},{"cell_type":"code","source":["from openai import OpenAI, __version__ as openai_version\n","print(\"openai version:\", openai_version)\n","\n","client = OpenAI()\n","res = client.responses.create(\n","    model=\"gpt-5\",\n","    input=\"ping\",\n","    max_output_tokens=32\n",")\n","print(res.output_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3oZj23yNma-","executionInfo":{"status":"ok","timestamp":1755018113577,"user_tz":-60,"elapsed":1911,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}},"outputId":"388dbea8-9d81-41d6-cc3c-1634d23fd036"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["openai version: 1.99.9\n","\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!ls /content/drive/MyDrive\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65QaZpstnjA3","outputId":"9d22bbf5-118d-480a-8788-61826a124aa9","executionInfo":{"status":"ok","timestamp":1755018141497,"user_tz":-60,"elapsed":22382,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","'2019世界各国官製地図発表（3E17関淵之） (1).pptx'\n"," 2019世界各国官製地図発表（3E17関淵之）.pptx\n"," 2019世界各国官製地図発表aa（3E17関淵之）.pptx\n"," 202507140043-Kiro-win32-x64.exe\n","'Activity Diagram.drawio.png'\n"," adult.zip\n"," bc_01-17.zip\n"," bc_18-23.zip\n"," bc_24-27.zip\n"," bc_28-33.zip\n"," BGMshuu\n"," blind_test\n","'Career Event by FAST OFFER.gform'\n","'Class Diagram.drawio.png'\n","'Class Diagram_White.drawio.png'\n"," Colab\n","'Colab Notebooks'\n"," Completed_CSV\n"," Component_sht.drawio.png\n","'Coronavirus (COVID-19) records (1).pdf'\n"," CV_Fuchiyuki.docx\n"," data_weights\n","'Emailing Offer Letter View.pdf'\n"," Evaluation_ReAct\n","'Extracting Actionable Knowledge from Cooking Recipes: An LLM Approach to Commonsense Reasoning and Graphical RepresentationI.gdoc'\n","'family mart in oomiya.csv'\n","'File to replace '\n","'Fuchiyuki Seki.pdf'\n"," GIS発表1A18関淵之.pptx\n","'Google AI Studio'\n"," gpt4.1_predictions_full.csv\n"," hihihi\n"," IMG_1826.MOV\n"," IMG_1827.MOV\n"," IMG_9927.jpeg\n"," japan_ver80_prefecture.shp\n","'Khash Bonus'\n","'Level_3_Week_14_Free_Lesson_copy (1).gslides'\n"," Level_3_Week_14_Free_Lesson_copy.gslides\n","'Literature Review.gdoc'\n","'Liz to Aoi Tori Movie.mp4'\n","'new doc 2019-04-18 14.14.14.pdf'\n"," Persona.gslides\n","'persona-template (1).gslides'\n"," pizza_recipes\n"," PPC_test.ipynb\n"," PPC_test_new.ipynb\n"," PPC_weight\n","'Project Plan.gdoc'\n"," Prompt_for_eSNLI.gdoc\n"," recipe1m\n"," recipe1m_cache\n"," recipe1m_data\n","'Recipe Commonsense Reasoning Project Details'$'\\n'' (1).gdoc'\n","'Recipe Commonsense Reasoning Project Details'$'\\n''.gdoc'\n","'Recycling and CO2 emissions helper application（回答）.gsheet'\n"," Resources.docx\n","'SnapCrab_Piriform Speccy_2019-1-31_2-58-40_No-00.png'\n","'SnapCrab_Piriform Speccy_2019-1-31_3-1-58_No-00.png'\n"," src\n"," train\n"," Transcript.pdf\n"," unet_weights.pth\n","'Untitled document (1).gdoc'\n","'Untitled document.gdoc'\n","'Untitled presentation (1).gslides'\n","'Untitled presentation (2).gslides'\n","'Untitled presentation.gslides'\n","'Untitled spreadsheet.gsheet'\n"," utils.py\n"," veg_recipe1m_output\n"," VID_20190109_175011.mp4\n"," VID_20190109_175016.mp4\n"," VID_20190109_175633.mp4\n"," VID_20190109_180103.mp4\n"," Weights\n","'例題0(バレーボール部員名簿①)_20171004.xlsm'\n","'例題1(教p81)順次構造_20171101.xlsm'\n","'例題1(教p84)逐次探索_20171101.xlsm'\n","'例題1(教p84)逐次探索(応用)_20171115.xlsm'\n","'例題2(教p82)選択構造_20171101.xlsm'\n","'例題2(教p82)選択構造応用(合格表示)_20171101.xlsm'\n","'例題2(教p85)逐次探索の活用_20171108.xlsm'\n","'例題3-1(商品分類売上高一覧表)_20171004.xlsm'\n","'例題3-2(週刊売上一覧表)_20171004.xlsm'\n","'例題3-3(時刻の表示)_20171004.xlsm'\n","'例題3-4(シートの追加)_20171011.xlsm'\n","'例題3-5(判定)_20171011.xlsm'\n","'例題3-6(ピザ売上一覧表)_20171025.xlsm'\n","'例題3-7(テニスコート)_20171204.xlsm'\n","'例題3-8(フォームの作成1)_20171108.xlsm'\n","'例題3-9(フォームの作成)_20171108.xlsm'\n","'例題3(教p83)繰り返し構造_20171101.xlsm'\n","'例題3(教p83)繰り返し構造(応用)_20171101.xlsm'\n","'例題3(教p86)二分探索_20171115.xlsm'\n","'例題3(教p86)二分探索(応用)_20171115.xlsm'\n","'例題4(教p88)交換法のプログラム_20171122.xlsm'\n","'例題4(教p88)交換法のプログラム_20171122.xlsx'\n","'例題5(教p90)交換法のプログラム_20171122.xlsm'\n","'例題5(教p90)復元交換法_20171122.xlsm'\n"," 情報の科学_授業資料_20180101.pdf\n","'情報の科学_授業資料(問題解決の考え 第３章)_20170906.pdf'\n","'無題のドキュメント (1).gdoc'\n"," 無題のドキュメント.gdoc\n"," 無題のフォーム.gform\n","'知識連鎖（Chain of Knowledge）を用いた手続き的常識推論の再評価：PizzaCommonsenseデータセットに関する研究提案.gdoc'\n","'統合評価指標 StrictScore の提案.gdoc'\n","'練習問題(RGB)_20171025.xlsm'\n","'練習問題(シート追加日付き)_20171011.xlsm'\n","'練習問題(ナンバリング)_20171025.xlsm'\n","'練習問題(合否判定)_20171011.xlsm'\n","'練習問題(担当者名入力)_20171011.xlsm'\n","'練習問題(背景色の変更)_20171011.xlsm'\n"," 自分史.docx\n"," 高忠実度ベジタリアンレシピコーパスのキュレーションと知識の連鎖プロンプティングによる手続き的コモンセンス推論の導出に関する方法論的フレームワーク.gdoc\n"]}]},{"cell_type":"code","source":["from pathlib import Path\n","import json\n","import numpy as np\n","\n","class PizzaDataLoader:\n","    \"\"\"\n","    Handles folder traversal and JSON parsing for PizzaCommonSense dataset.\n","    デフォルトの data_path を Google Drive 内の MyDrive/train フォルダに変更。\n","    \"\"\"\n","    def __init__(self, data_path: str = \"/content/drive/MyDrive/train\"):\n","        self.data_path = Path(data_path)\n","        if not self.data_path.exists():\n","            raise FileNotFoundError(f\"Data path {data_path} does not exist\")\n","        print(f\"✅ Using data path: {self.data_path}\")\n","\n","    def iter_tables(self, split: str = None) -> Iterator[dict]:\n","        \"\"\"\n","        split 引数は不要 (すでに train フォルダ直下を想定)。\n","        フォルダ内のすべての .txt ファイルを読み込みます。\n","        \"\"\"\n","        txt_files = list(self.data_path.glob(\"*.txt\"))\n","        print(f\"Found {len(txt_files)} recipe files in {self.data_path.name}\")\n","\n","        for file_path in txt_files:\n","            try:\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    data = json.load(f)\n","                if 'table' in data and isinstance(data['table'], list):\n","                    for step in data['table']:\n","                        if self._validate_step(step):\n","                            yield step\n","            except Exception as e:\n","                print(f\"⚠️  Skipping {file_path.name}: {e}\")\n","\n","    def _validate_step(self, step: dict) -> bool:\n","        required = ['instructions', 'actions', 'input', 'output']\n","        return all(k in step for k in required)\n","\n","    def get_split_stats(self) -> dict:\n","        steps = list(self.iter_tables())\n","        return {\n","            'total_steps': len(steps),\n","            'unique_actions': len(set(s['actions'] for s in steps)),\n","            'avg_instruction_length': np.mean([len(s['instructions']) for s in steps])\n","        }\n"],"metadata":{"id":"Fj-_tkmdnC96","executionInfo":{"status":"ok","timestamp":1755018145156,"user_tz":-60,"elapsed":21,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# ドライブマウント後に実行\n","data_loader = PizzaDataLoader()  # デフォルトで /content/drive/MyDrive/train を参照\n","stats = data_loader.get_split_stats()\n","print(\"📊 Dataset stats:\", stats)\n","\n","# サンプル3件を表示\n","for i, step in enumerate(data_loader.iter_tables()):\n","    print(f\"\\nStep {i+1}\")\n","    print(\" Instructions:\", step['instructions'])\n","    print(\" Actions:     \", step['actions'])\n","    print(\" Input:       \", step['input'])\n","    print(\" Output:      \", step['output'])\n","    if i >= 2:\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LBsPBJOlbqbv","outputId":"d15c8884-7b95-40d5-defb-581b5f67ff46","executionInfo":{"status":"ok","timestamp":1755018158755,"user_tz":-60,"elapsed":10022,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Using data path: /content/drive/MyDrive/train\n","Found 744 recipe files in train\n","📊 Dataset stats: {'total_steps': 3069, 'unique_actions': 168, 'avg_instruction_length': np.float64(40.33919843597263)}\n","Found 744 recipe files in train\n","\n","Step 1\n"," Instructions: preheat the oven to 400f .\n"," Actions:      preheat\n"," Input:        NA\n"," Output:       NA\n","\n","Step 2\n"," Instructions: heat the oil in a large non stick frying pan\n"," Actions:      heat\n"," Input:        oil\n"," Output:       heated_oil\n","\n","Step 3\n"," Instructions: add the onion , pepper and zucchini\n"," Actions:      add\n"," Input:        (onion; pepper; zucchini; heated_oil)\n"," Output:       onion, pepper and zucchini added to heated oil\n"]}]},{"cell_type":"code","source":["class CoTPromptGenerator:\n","    \"\"\"Chain‑of‑Thought prompt generator for PizzaCommonSense.\"\"\"\n","\n","    def __init__(self):\n","        # ✅ ルールを明文化\n","        self.system_message = (\n","            \"You are an expert cooking‑reasoning assistant.\\n\"\n","            \"For every recipe step you must predict\\n\"\n","            \"  • the *input* comestibles/items that go into the action（原材料がない場合は NA）\\n\"\n","            \"  • the *output* comestible/result that comes out （結果が食材でない／道具のみの場合は NA）\\n\\n\"\n","            \"Rules:\\n\"\n","            \"1. If the step does NOT consume or transform any food, write exactly 'NA' for BOTH input and output.\\n\"\n","            \"   ‑ e.g. pre‑heating an oven, washing a utensil, setting a timer.\\n\"\n","            \"2. Otherwise list the food items succinctly; join multiple items with semicolons.\\n\"\n","            \"3. Respond **only** with the two lines:\\n\"\n","            \"     Input: <your prediction>\\n\"\n","            \"     Output: <your prediction>\\n\"\n","        )\n","\n","    def make_prompt(self, step: dict) -> str:\n","        \"\"\"Create a deterministic CoT prompt for one recipe step.\"\"\"\n","        instr  = self._clean(step[\"instructions\"])\n","        action = self._clean(step[\"action\"  ] if \"action\" in step else step[\"actions\"])\n","\n","        return (\n","            f\"Instruction: {instr}\\n\"\n","            f\"Action: {action}\\n\\n\"\n","            # CoT を誘発\n","            \"Let's reason step by step.\\n\"\n","            \"1️⃣ Identify whether any food or edible item is being used or produced.\\n\"\n","            \"2️⃣ If none, decide Input=NA and Output=NA immediately.\\n\"\n","            \"3️⃣ Otherwise, list the food that goes *into* the action (Input) and \"\n","            \"the food/result that comes *out of* the action (Output).\\n\\n\"\n","            \"Remember the required format:\\n\"\n","            \"Input: <prediction or NA>\\n\"\n","            \"Output: <prediction or NA>\"\n","        )\n","\n","    # ---------- helpers ----------\n","    @staticmethod\n","    def _clean(text: str) -> str:\n","        text = str(text).strip()\n","        text = re.sub(r\"\\s+\", \" \", text)\n","        return text.replace(\"\\u2012\", \"-\")      # ‑→-\n","\n","    def get_system_message(self) -> str:\n","        return self.system_message\n","\n","    # Initialize prompt generator\n","        return self.system_message\n","\n","# Initialize prompt generator\n","prompt_generator = CoTPromptGenerator()\n","print(\"✅ CoT prompt generator initialized successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPcY4KRsw-Ss","executionInfo":{"status":"ok","timestamp":1755018158777,"user_tz":-60,"elapsed":27,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}},"outputId":"2d04838f-7e18-450d-99ad-b241934d425a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ CoT prompt generator initialized successfully!\n"]}]},{"cell_type":"code","source":["# class CoTPromptGenerator:\n","#     \"\"\"Creates Chain-of-Thought prompts for cooking step reasoning.\"\"\"\n","\n","#     def __init__(self):\n","#         self.system_message = (\n","#             \"You are a cooking-reasoning assistant. \"\n","#             \"Given an instruction and an action, predict the input comestibles \"\n","#             \"and the output comestible/result for that step. \"\n","#             \"Think step by step about what ingredients or items are needed (input) \"\n","#             \"and what results from the cooking action (output).\"\n","#         )\n","\n","#     def make_prompt(self, step: Dict) -> str:\n","#         \"\"\"Generate a Chain-of-Thought prompt for a recipe step.\n","\n","#         Args:\n","#             step: Recipe step dictionary with 'instructions' and 'actions'\n","\n","#         Returns:\n","#             str: Formatted prompt with CoT trigger\n","#         \"\"\"\n","#         # Clean and escape the text\n","#         instructions = self._clean_text(step['instructions'])\n","#         actions = self._clean_text(step['actions'])\n","\n","#         prompt = (\n","#             f\"Instruction: {instructions}\\n\"\n","#             f\"Action: {actions}\\n\\n\"\n","#             \"Let's think step by step.\\n\\n\"\n","#             \"What are the input ingredients or items needed for this step?\\n\"\n","#             \"What is the output result after performing this action?\\n\\n\"\n","#             \"Please provide your answer in this format:\\n\"\n","#             \"Input: [your prediction]\\n\"\n","#             \"Output: [your prediction]\"\n","#         )\n","\n","#         return prompt\n","\n","#     def _clean_text(self, text: str) -> str:\n","#         \"\"\"Clean and normalize text for prompting.\"\"\"\n","#         if not isinstance(text, str):\n","#             return str(text)\n","\n","#         # Remove extra whitespace and normalize\n","#         text = re.sub(r'\\s+', ' ', text.strip())\n","\n","#         # Handle special characters that might cause issues\n","#         text = text.replace('\\u2012', '-')  # Replace en-dash with hyphen\n","\n","#         return text\n","\n","#     def get_system_message(self) -> str:\n","#         \"\"\"Get the system message for the assistant.\"\"\"\n","#         text = re.sub(r'\\s+', ' ', text.strip())\n","\n","#         # Handle special characters that might cause issues\n","#         text = text.replace('\\u2012', '-')  # Replace en-dash with hyphen\n","\n","#         return text\n","\n","#     def get_system_message(self) -> str:\n","#         \"\"\"Get the system message for the assistant.\"\"\"\n","#         return self.system_message\n","\n","# # Initialize prompt generator\n","#         return self.system_message\n","\n","# # Initialize prompt generator\n","# prompt_generator = CoTPromptGenerator()\n","# print(\"✅ CoT prompt generator initialized successfully!\")\n"],"metadata":{"id":"xMl1ZTFArYPD","executionInfo":{"status":"ok","timestamp":1755017636170,"user_tz":-60,"elapsed":40,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Test prompt generation with sample data\n","print(\"🧪 Testing Prompt Generation:\\n\" + \"=\"*50)\n","print(\"SYSTEM MESSAGE:\")\n","print(\"=\"*50)\n","print(prompt_generator.get_system_message())\n","\n","# Get a sample step for testing\n","sample_step = next(data_loader.iter_tables(\"val\"))\n","test_prompt = prompt_generator.make_prompt(sample_step)\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"SAMPLE PROMPT:\")\n","print(\"=\"*50)\n","print(test_prompt)\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"GROUND TRUTH:\")\n","print(\"=\"*50)\n","print(f\"Input: {sample_step['input']}\")\n","print(f\"Output: {sample_step['output']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BNLV9HAraOG","outputId":"22b8493a-7c30-4618-9da6-291e1a13ccee","executionInfo":{"status":"ok","timestamp":1755018161850,"user_tz":-60,"elapsed":48,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["🧪 Testing Prompt Generation:\n","==================================================\n","SYSTEM MESSAGE:\n","==================================================\n","You are an expert cooking‑reasoning assistant.\n","For every recipe step you must predict\n","  • the *input* comestibles/items that go into the action（原材料がない場合は NA）\n","  • the *output* comestible/result that comes out （結果が食材でない／道具のみの場合は NA）\n","\n","Rules:\n","1. If the step does NOT consume or transform any food, write exactly 'NA' for BOTH input and output.\n","   ‑ e.g. pre‑heating an oven, washing a utensil, setting a timer.\n","2. Otherwise list the food items succinctly; join multiple items with semicolons.\n","3. Respond **only** with the two lines:\n","     Input: <your prediction>\n","     Output: <your prediction>\n","\n","Found 744 recipe files in train\n","\n","==================================================\n","SAMPLE PROMPT:\n","==================================================\n","Instruction: preheat the oven to 400f .\n","Action: preheat\n","\n","Let's reason step by step.\n","1️⃣ Identify whether any food or edible item is being used or produced.\n","2️⃣ If none, decide Input=NA and Output=NA immediately.\n","3️⃣ Otherwise, list the food that goes *into* the action (Input) and the food/result that comes *out of* the action (Output).\n","\n","Remember the required format:\n","Input: <prediction or NA>\n","Output: <prediction or NA>\n","\n","==================================================\n","GROUND TRUTH:\n","==================================================\n","Input: NA\n","Output: NA\n"]}]},{"cell_type":"code","source":["!pip show openai | grep Version\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvKpMwUSif5x","outputId":"4d20c672-e402-4466-c5d5-57e8fe2adc5f","executionInfo":{"status":"ok","timestamp":1755018169387,"user_tz":-60,"elapsed":1542,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Version: 1.99.9\n"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","client = OpenAI()\n","\n","class GPT4Predictor:\n","    def __init__(self, model=\"gpt-5\", max_retries=3, base_delay=1.0,\n","                 default_max_output_tokens=512):\n","        self.model = model\n","        self.max_retries = max_retries\n","        self.base_delay = base_delay\n","        self.default_max_output_tokens = default_max_output_tokens\n","\n","        # 接続テスト（temperatureを渡さない）\n","        client.responses.create(\n","            model=self.model,\n","            input=\"ping\",\n","            max_output_tokens=32\n","        )\n","\n","    def predict(self, prompt, max_output_tokens=None, **kwargs):\n","        mot = max_output_tokens or self.default_max_output_tokens\n","        res = client.responses.create(\n","            model=self.model,\n","            input=prompt,\n","            max_output_tokens=mot\n","            # temperature は渡さない（推論系は非対応）\n","        )\n","        return res.output_text\n","\n","\n","        # （参考）会話メッセージで送りたい場合はこの形に差し替えでもOK\n","        # res = client.responses.create(\n","        #     model=self.model,\n","        #     messages=[\n","        #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        #         {\"role\": \"user\", \"content\": prompt}\n","        #     ],\n","        #     max_output_tokens=mot,\n","        #     temperature=kwargs.get(\"temperature\", self.temperature)\n","        # )\n","        # return res.output_text\n","\n","# -------- テスト --------\n","predictor = GPT4Predictor()  # デフォルトで gpt-5 を使う\n","print(\"✅ GPT4Predictor is ready!\")\n","print(predictor.predict(\"日本語で一言ご挨拶を。\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kB7q4e0oMnRm","executionInfo":{"status":"ok","timestamp":1755018218145,"user_tz":-60,"elapsed":6777,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}},"outputId":"4588b41a-1ccf-4c96-f5c5-14762278928c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ GPT4Predictor is ready!\n","どうぞよろしくお願いいたします。\n"]}]},{"cell_type":"code","source":["# ==== OpenAI v1 / Responses API 版 ====\n","# pip install -U \"openai>=1.0.0\" 済み想定。OPENAI_API_KEY は環境変数に。\n","import time, re, os\n","from openai import OpenAI, RateLimitError, APIError\n","\n","client = OpenAI()  # envのOPENAI_API_KEYを自動参照\n","\n","def _is_reasoning_model(name: str) -> bool:\n","    return any(k in name for k in (\"gpt-5\", \"o3\", \"o1\"))\n","\n","class GPT4Predictor:\n","    \"\"\"\n","    OpenAI v1 用。Responses APIで gpt-5 を叩く版\n","    \"\"\"\n","    def __init__(self, model: str = \"gpt-5\",\n","                 max_retries: int = 3, base_delay: float = 0.5,\n","                 default_max_output_tokens: int = 400):\n","        self.model = model\n","        self.max_retries = max_retries\n","        self.base_delay  = base_delay\n","        self.default_max_output_tokens = default_max_output_tokens\n","\n","        # 接続テスト（temperatureは渡さない）\n","        client.responses.create(\n","            model=self.model,\n","            input=\"ping\",\n","            max_output_tokens=32  # reasoning系は16以上必須\n","        )\n","        print(f\"✅ API connection successful with model: {self.model}\")\n","\n","    # ---------- 推論 ----------\n","    def predict(self, prompt: str, system_message: str) -> str:\n","      for i in range(self.max_retries):\n","        try:\n","            body = {\n","                \"model\": self.model,\n","                \"instructions\": system_message,              # system\n","                \"input\": prompt,                              # user\n","                \"max_output_tokens\": self.default_max_output_tokens,\n","            }\n","            if not _is_reasoning_model(self.model):\n","                body[\"temperature\"] = 1.0                     # reasoning系には付けない\n","\n","            res = client.responses.create(**body)\n","            return res.output_text.strip()\n","\n","        except RateLimitError:\n","            wait = self.base_delay * (2 ** i)\n","            print(f\"⏳ Rate-limit, retrying in {wait}s …\")\n","            time.sleep(wait)\n","\n","        except APIError as e:\n","            print(f\"❌ API error on attempt {i+1}: {e}\")\n","            time.sleep(self.base_delay)\n","\n","      raise RuntimeError(\"Failed to get response after retries\")\n","\n","\n","\n","    # ---------- 応答パース ----------\n","    @staticmethod\n","    def parse_io(text: str) -> tuple[str, str]:\n","        in_pat  = r\"Input\\s*[:：]\\s*(.+?)(?=\\n|Output|$)\"\n","        out_pat = r\"Output\\s*[:：]\\s*(.+?)(?=\\n|$)\"\n","        inp  = re.search(in_pat,  text, re.I | re.S)\n","        out  = re.search(out_pat, text, re.I | re.S)\n","        return (inp.group(1).strip()  if inp else \"\",\n","                out.group(1).strip()  if out else \"\")\n","\n","    def add_delay(self):\n","        time.sleep(self.base_delay)\n","\n","# -------- テスト --------\n","predictor = GPT4Predictor()\n","print(\"✅ GPT5Predictor is ready!\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzp9HCRuhgYk","outputId":"dee51c3d-e5f6-48a1-a239-db32ac7432be","executionInfo":{"status":"ok","timestamp":1755018868636,"user_tz":-60,"elapsed":1419,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ API connection successful with model: gpt-5\n","✅ GPT4Predictor is ready!\n"]}]},{"cell_type":"code","source":["# Test GPT-5 prediction with sample data\n","print(\"🧪 Testing GPT-5 Prediction:\")\n","\n","# Use the same sample step from before\n","test_prompt = prompt_generator.make_prompt(sample_step)\n","system_msg = prompt_generator.get_system_message()\n","\n","print(\"\\n⏳ Generating prediction...\")\n","response = predictor.predict(test_prompt, system_msg)\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"MODEL RESPONSE:\")\n","print(\"=\"*50)\n","print(response)\n","\n","# Parse the response\n","pred_input, pred_output = predictor.parse_io(response)\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"PARSED PREDICTIONS:\")\n","print(\"=\"*50)\n","print(f\"Predicted Input: '{pred_input}'\")\n","print(f\"Predicted Output: '{pred_output}'\")\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"GROUND TRUTH COMPARISON:\")\n","print(\"=\"*50)\n","print(f\"Ground Truth Input:  '{sample_step['input']}'\")\n","print(f\"Ground Truth Output: '{sample_step['output']}'\")\n","\n","# Check exact matches\n","input_match = pred_input.lower().strip() == sample_step['input'].lower().strip()\n","output_match = pred_output.lower().strip() == sample_step['output'].lower().strip()\n","\n","print(f\"\\n📊 Exact Match Results:\")\n","print(f\"Input Match: {'✅' if input_match else '❌'}\")\n","print(f\"Output Match: {'✅' if output_match else '❌'}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"882nVJa_rgHb","outputId":"386f8645-c3c9-4a12-b890-5fa67b501414","executionInfo":{"status":"ok","timestamp":1755018891668,"user_tz":-60,"elapsed":4525,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["🧪 Testing GPT-5 Prediction:\n","\n","⏳ Generating prediction...\n","\n","==================================================\n","MODEL RESPONSE:\n","==================================================\n","Input: NA\n","Output: NA\n","\n","==================================================\n","PARSED PREDICTIONS:\n","==================================================\n","Predicted Input: 'NA'\n","Predicted Output: 'NA'\n","\n","==================================================\n","GROUND TRUTH COMPARISON:\n","==================================================\n","Ground Truth Input:  'NA'\n","Ground Truth Output: 'NA'\n","\n","📊 Exact Match Results:\n","Input Match: ✅\n","Output Match: ✅\n"]}]},{"cell_type":"code","source":["def run_batch_predictions(split: str = \"val\", max_samples: int = None, save_interval: int = 50):\n","    \"\"\"Run batch predictions on the dataset with progress tracking.\n","\n","    Args:\n","        split: Dataset split to process ('val' or 'train')\n","        max_samples: Maximum number of samples to process (None for all)\n","        save_interval: Save results every N samples\n","\n","    Returns:\n","        pd.DataFrame: Results with predictions and ground truth\n","    \"\"\"\n","    print(f\"🚀 Starting batch prediction on {split} split...\")\n","\n","    # Collect all steps first to get total count\n","    all_steps = list(data_loader.iter_tables(split))\n","\n","    if max_samples:\n","        all_steps = all_steps[:max_samples]\n","\n","    print(f\"📊 Processing {len(all_steps)} recipe steps\")\n","\n","    results = []\n","    system_msg = prompt_generator.get_system_message()\n","\n","    # Process with progress bar\n","    for i, step in enumerate(tqdm(all_steps, desc=\"Generating predictions\")):\n","        try:\n","            # Generate prompt\n","            prompt = prompt_generator.make_prompt(step)\n","\n","            # Get prediction\n","            response = predictor.predict(prompt, system_msg)\n","\n","            # Parse response\n","            pred_input, pred_output = predictor.parse_io(response)\n","\n","            # Store result\n","            result = {\n","                'instructions': step['instructions'],\n","                'actions': step['actions'],\n","                'input': step['input'],\n","                'output': step['output'],\n","                'pred_input': pred_input,\n","                'pred_output': pred_output,\n","                'response': response  # Keep full response for debugging\n","            }\n","            results.append(result)\n","\n","            # Add delay for rate limiting\n","            predictor.add_delay()\n","\n","            # Periodic saving\n","            if (i + 1) % save_interval == 0:\n","                temp_df = pd.DataFrame(results)\n","                temp_df.to_csv(f\"temp_predictions_{i+1}.csv\", index=False)\n","                print(f\"💾 Saved temporary results at step {i+1}\")\n","\n","        except Exception as e:\n","            print(f\"❌ Error processing step {i+1}: {e}\")\n","            # Add placeholder result to maintain alignment\n","            result = {\n","                'instructions': step['instructions'],\n","                'actions': step['actions'],\n","                'input': step['input'],\n","                'output': step['output'],\n","                'pred_input': '',\n","                'pred_output': '',\n","                'response': f'ERROR: {str(e)}'\n","            }\n","            results.append(result)\n","            continue\n","\n","    # Convert to DataFrame\n","    df = pd.DataFrame(results)\n","\n","    print(f\"✅ Batch processing complete! Processed {len(df)} steps\")\n","    print(f\"📊 Success rate: {(df['pred_input'] != '').sum()}/{len(df)} ({(df['pred_input'] != '').mean():.1%})\")\n","\n","    return df\n","\n","print(\"✅ Batch processing function ready!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMhdXVA_rhQE","outputId":"9e6a65ff-4401-49b3-e214-a1be19129d9d","executionInfo":{"status":"ok","timestamp":1755018994678,"user_tz":-60,"elapsed":43,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Batch processing function ready!\n"]}]},{"cell_type":"code","source":["# Run a small test first (10 samples)\n","print(\"🧪 Running small test with 10 samples...\")\n","test_df = run_batch_predictions(split=\"val\", max_samples=10)\n","\n","# Display sample results\n","print(\"\\n📋 Sample Results:\")\n","display_cols = ['instructions', 'actions', 'input', 'output', 'pred_input', 'pred_output']\n","print(test_df[display_cols].head())\n","\n","# Quick evaluation\n","input_matches = (test_df['input'].str.lower().str.strip() ==\n","                test_df['pred_input'].str.lower().str.strip()).sum()\n","output_matches = (test_df['output'].str.lower().str.strip() ==\n","                 test_df['pred_output'].str.lower().str.strip()).sum()\n","\n","print(f\"\\n📊 Quick Test Results:\")\n","print(f\"Input EMA: {input_matches}/{len(test_df)} ({input_matches/len(test_df):.1%})\")\n","print(f\"Output EMA: {output_matches}/{len(test_df)} ({output_matches/len(test_df):.1%})\")\n","print(f\"Average EMA: {(input_matches + output_matches)/(2*len(test_df)):.1%}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUkbNlA3lGp9","outputId":"de64aeb2-a938-496a-91af-8ea574ebd1f4","executionInfo":{"status":"ok","timestamp":1755019085469,"user_tz":-60,"elapsed":86450,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["🧪 Running small test with 10 samples...\n","🚀 Starting batch prediction on val split...\n","Found 744 recipe files in train\n","📊 Processing 10 recipe steps\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions: 100%|██████████| 10/10 [01:23<00:00,  8.32s/it]"]},{"output_type":"stream","name":"stdout","text":["✅ Batch processing complete! Processed 10 steps\n","📊 Success rate: 6/10 (60.0%)\n","\n","📋 Sample Results:\n","                                   instructions  actions  \\\n","0                    preheat the oven to 400f .  preheat   \n","1  heat the oil in a large non stick frying pan     heat   \n","2           add the onion , pepper and zucchini      add   \n","3        saute over a medium heat for 4 5mins .    saute   \n","4                                 add the herbs      add   \n","\n","                                   input  \\\n","0                                     NA   \n","1                                    oil   \n","2  (onion; pepper; zucchini; heated_oil)   \n","3               vegetables in heated_oil   \n","4     (herbs; sauteed vegetable mixture)   \n","\n","                                           output pred_input pred_output  \n","0                                              NA         NA          NA  \n","1                                      heated_oil        oil     hot oil  \n","2  onion, pepper and zucchini added to heated oil                         \n","3                       sauteed vegetable mixture         NA          NA  \n","4        herbs added to sauteed vegetable mixture                         \n","\n","📊 Quick Test Results:\n","Input EMA: 2/10 (20.0%)\n","Output EMA: 1/10 (10.0%)\n","Average EMA: 15.0%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"rkqs98OYV-R8","executionInfo":{"status":"ok","timestamp":1755019133472,"user_tz":-60,"elapsed":1220,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"337e5801-2d06-4431-8df7-a11782f839d3"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# 追加セル\n","!pip -q install bert_score transformers torch\n"],"metadata":{"id":"7uN0rvYXtcxb","executionInfo":{"status":"ok","timestamp":1755019215437,"user_tz":-60,"elapsed":79823,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"57d51a7e-a56f-45eb-e3f6-d65e801fecd3"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# Run a small test first (10 samples)\n","# ------------------------------------------------------------\n","print(\"🧪 Running small test with 10 samples...\")\n","test_df = run_batch_predictions(split=\"val\", max_samples=10)\n","\n","# ------------- 結果プレビュー -------------\n","print(\"\\n📋 Sample Results:\")\n","display_cols = ['instructions', 'actions', 'input', 'output',\n","                'pred_input', 'pred_output']\n","print(test_df[display_cols].head())\n","\n","# ------------- Exact‑Match Accuracy (EMA) -------------\n","input_matches = (\n","    test_df['input'].str.lower().str.strip() ==\n","    test_df['pred_input'].str.lower().str.strip()\n",").sum()\n","\n","output_matches = (\n","    test_df['output'].str.lower().str.strip() ==\n","    test_df['pred_output'].str.lower().str.strip()\n",").sum()\n","\n","# ------------- BERTScore (直接計算) -------------\n","import evaluate, numpy as np\n","bertscore = evaluate.load(\"bertscore\")\n","\n","# 空値対策\n","pred_inputs  = test_df['pred_input'].fillna(\"\").tolist()\n","true_inputs  = test_df['input'].fillna(\"\").tolist()\n","pred_outputs = test_df['pred_output'].fillna(\"\").tolist()\n","true_outputs = test_df['output'].fillna(\"\").tolist()\n","\n","bert_in  = bertscore.compute(predictions=pred_inputs,  references=true_inputs,  lang=\"en\")['f1']\n","bert_out = bertscore.compute(predictions=pred_outputs, references=true_outputs, lang=\"en\")['f1']\n","# 追加セル\n","!pip -q install bert_score transformers torch\n","\n","# ------------- まとめ表示 -------------\n","print(f\"\\n📊 Quick Test Results:\")\n","print(f\"Input EMA:   {input_matches}/{len(test_df)} \"\n","      f\"({input_matches/len(test_df):.1%})\")\n","print(f\"Output EMA:  {output_matches}/{len(test_df)} \"\n","      f\"({output_matches/len(test_df):.1%})\")\n","print(f\"Average EMA: {(input_matches + output_matches)/(2*len(test_df)):.1%}\")\n","print(f\"BERTScore Input F1:  {np.mean(bert_in):.3f}\")\n","print(f\"BERTScore Output F1: {np.mean(bert_out):.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["43c752226a414e8780fc73a1af5fc558","01aacfa90cde4ac58b19bbe5dc8dca4d","bd41405dcd234a8b91bf3c1863839d0c","3a7d3daab35245719486f3f40d9083f9","c75d042df7884f039fbf408ce7aded1a","8afd269c0a1a488dbfb3643af50667df","b4cefa8a5bf6400184d11cfe1ce66424","4933fe49607b44469610c448e2a277a1","fc9cc8d43ea6450cb0feffde614a674f","ff4e9eaccda44f0daa2311b4d9d0a52d","3ec28265d0fa46198ffa8fa326e88515","de558cd8979e4274b104a84d97622834","4d80781a73014fabaa4d380bec43f88a","14b48b6fd1ae43579aa8f5bfd0a24df3","e5ffe43124ba4839aa0418d6bb443b5c","ed42af7ddf1949028b6eb963a194e78e","9b7d5a9716234ac6b9339dfa1ea34bcc","459a3f2e2aa540e1b222925055cd16f6","d294cbb6534943cda4520b57a1cfafd1","4749f2cb8cf44c219fec170c47540aa2","5282d9a0002147aaa17dd60f378898d7","ccadb438444f460a8163b7f8e8804b21","cf276063cc5345b99e3140ba7905552d","bea05b8c5e384ec7b08241eaed285468","18d79f96368646dd80548d4803d9ac29","eca7d70f6c5a4ac1aa61a670c9131e9f","e9d28eab952d44269d53893ee0528848","ef89d0c432be438db7902edd2cd9ed00","d7a4d8d6299140d4b3fe20f1b21210c3","98d71cce7f8748f3af1fb215ab29f46b","cdedf57326dd442185bcf6f4cdff8125","634d92fa3e6548dbad8e14cb7c79c377","1d7204f3c3d14ff199fd1d8fad0b3a80","8e17a8fc9a764009ac1f5a9e6f357707","9e375a8a49384f5f81c5b80c9ffc37a8","a1ccb6f10ec24920a9da2c26c7e460fa","17cc5bf72681429a8e82f78f5f2f88d7","3ea97f4cd91446e898b4fcabcddb78d3","c4848cece46e45d69d1ccd287cb63bcc","bc2fa0cd2f38475c862d9a1b47aba7ab","1ad8ee7c30c34242995edc344f59b4ae","d025b4c477cf43c8bcf44dd32edbed40","23fb09519bdc4d99b659fa15374c4e7c","905b02bc019e4329b1e5833bf6e295b0","ed508ca7d63445b08a9a96801386beb7","e39bd66aa648486491daf53dda66b7c7","0c9b61633de8466b97ca2b103737b818","cc26e56455454bb5b60be44afa1c9936","45dbdbc628524cd2a536b96eede046a3","6a802b0d0cd34e68a867e980dd6109c6","1b0880f761fb4cf8afc74b4ca85e8a0c","143c8e119e914ae1b0ebd37c30d2ae8c","f22eec95f92a449b80dfba26756405f6","0530cfe28036464f9d51c182bc9bd737","26d98286658543b5b400abe7102127e7","d5cd584658fd4ce3a6f863c34280959c","a525535073c94630ba49da8f0e9650b3","c45296c21d034d8d9c2cbd097a0185a4","a72ca0ad518f4c7fb39a011eeacaf4fc","1f2f845a040e421e8737c90113df8199","ade815fe11a3436b86d26d96fee7135e","58214d0f6c954837a5b9a49e09da5c47","21305d14e4e044ff985d700341aa7980","b017bd7e26cb46cc952b4ed06a308cf3","dc05f5a02de9445ebf12a9da2647992e","8b5f6fb9498744b099d5e4dc27763aaf","eabeb865f0e2441c9aed1cbbf49e7251","d37fa53f4c7c42f191f35c58522d4d72","8665e92b087e4d7b8ba0914de472bf7b","da0359a7f66b4b8ea7bb0d56f50a7bbd","459616c86ce542cd8b5025880c91b9e7","4df0419c795e4063ae838ee153bc2b6d","005117cf5f974d518818a9139eafaa9e","291212e7d82941bea84bd19010412d25","af15ba5eb79748b1ab7ee930a1762f5c","ba7e7b621ad54ae68c278fff70c92237","8d22c0e1ada04723a0991ab088743c67"]},"id":"bx_hjT3lqUEm","outputId":"ac002347-1111-4761-bf9b-49833b1f6a2c","executionInfo":{"status":"ok","timestamp":1755019292100,"user_tz":-60,"elapsed":76629,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["🧪 Running small test with 10 samples...\n","🚀 Starting batch prediction on val split...\n","Found 744 recipe files in train\n","📊 Processing 10 recipe steps\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions: 100%|██████████| 10/10 [00:59<00:00,  5.94s/it]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Batch processing complete! Processed 10 steps\n","📊 Success rate: 5/10 (50.0%)\n","\n","📋 Sample Results:\n","                                   instructions  actions  \\\n","0                    preheat the oven to 400f .  preheat   \n","1  heat the oil in a large non stick frying pan     heat   \n","2           add the onion , pepper and zucchini      add   \n","3        saute over a medium heat for 4 5mins .    saute   \n","4                                 add the herbs      add   \n","\n","                                   input  \\\n","0                                     NA   \n","1                                    oil   \n","2  (onion; pepper; zucchini; heated_oil)   \n","3               vegetables in heated_oil   \n","4     (herbs; sauteed vegetable mixture)   \n","\n","                                           output pred_input pred_output  \n","0                                              NA         NA          NA  \n","1                                      heated_oil        oil  heated oil  \n","2  onion, pepper and zucchini added to heated oil                         \n","3                       sauteed vegetable mixture         NA          NA  \n","4        herbs added to sauteed vegetable mixture                         \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c752226a414e8780fc73a1af5fc558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de558cd8979e4274b104a84d97622834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf276063cc5345b99e3140ba7905552d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e17a8fc9a764009ac1f5a9e6f357707"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed508ca7d63445b08a9a96801386beb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5cd584658fd4ce3a6f863c34280959c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eabeb865f0e2441c9aed1cbbf49e7251"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"]},{"output_type":"stream","name":"stdout","text":["\n","📊 Quick Test Results:\n","Input EMA:   2/10 (20.0%)\n","Output EMA:  1/10 (10.0%)\n","Average EMA: 15.0%\n","BERTScore Input F1:  0.461\n","BERTScore Output F1: 0.441\n"]}]},{"cell_type":"code","source":["# Run full batch processing (uncomment when ready)\n","# WARNING: This will process the entire validation split and may take 30-60 minutes\n","# and cost several dollars in API calls\n","\n","print(\"⚠️  Ready to run full batch processing on validation split\")\n","print(\"💰 Estimated cost: $3-10 depending on dataset size\")\n","print(\"⏱️  Estimated time: 30-60 minutes\")\n","print(\"\\n🔧 To run full processing, uncomment the lines below:\")\n","print(\"# full_df = run_batch_predictions(split='val')\")\n","print(\"# full_df.to_csv('gpt5_predictions_full.csv', index=False)\")\n","print(\"# print('💾 Full results saved to gpt5_predictions_full.csv')\")\n","\n","# Uncomment these lines when ready to run full experiment:\n","full_df = run_batch_predictions(split=\"val\")\n","full_df.to_csv(\"gpt5_predictions_full.csv\", index=False)\n","print(\"💾 Full results saved to gpt5_predictions_full.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":810},"id":"hFhUDXw9w-g3","outputId":"2669aaba-b1c0-4837-d30d-fac89bc578a8","executionInfo":{"status":"error","timestamp":1755022720620,"user_tz":-60,"elapsed":3364878,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️  Ready to run full batch processing on validation split\n","💰 Estimated cost: $3-10 depending on dataset size\n","⏱️  Estimated time: 30-60 minutes\n","\n","🔧 To run full processing, uncomment the lines below:\n","# full_df = run_batch_predictions(split='val')\n","# full_df.to_csv('gpt5_predictions_full.csv', index=False)\n","# print('💾 Full results saved to gpt5_predictions_full.csv')\n","🚀 Starting batch prediction on val split...\n","Found 744 recipe files in train\n","📊 Processing 3069 recipe steps\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:   2%|▏         | 50/3069 [06:05<6:43:10,  8.01s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 50\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:   3%|▎         | 100/3069 [11:53<4:37:05,  5.60s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 100\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:   5%|▍         | 150/3069 [17:42<11:21:50, 14.02s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 150\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:   7%|▋         | 200/3069 [23:16<5:49:05,  7.30s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 200\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:   8%|▊         | 250/3069 [28:40<4:25:33,  5.65s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 250\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:  10%|▉         | 300/3069 [33:17<3:30:28,  4.56s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 300\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:  11%|█▏        | 350/3069 [38:49<4:44:26,  6.28s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 350\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:  13%|█▎        | 400/3069 [44:05<4:32:17,  6.12s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 400\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:  15%|█▍        | 450/3069 [49:12<6:45:00,  9.28s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 450\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:  16%|█▋        | 500/3069 [54:27<3:23:47,  4.76s/it]"]},{"output_type":"stream","name":"stdout","text":["💾 Saved temporary results at step 500\n"]},{"output_type":"stream","name":"stderr","text":["Generating predictions:  17%|█▋        | 513/3069 [56:01<4:39:06,  6.55s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3224026700.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Uncomment these lines when ready to run full experiment:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfull_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_batch_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt5_predictions_full.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"💾 Full results saved to gpt5_predictions_full.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1581992574.py\u001b[0m in \u001b[0;36mrun_batch_predictions\u001b[0;34m(split, max_samples, save_interval)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Get prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Parse response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1337804628.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, prompt, system_message)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m                     \u001b[0;31m# reasoning系には付けない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/responses/responses.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[0;32m--> 795\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    796\u001b[0m             \u001b[0;34m\"/responses\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["class MetricsCalculator:\n","    \"\"\"Computes evaluation metrics for model predictions.\"\"\"\n","\n","    def __init__(self):\n","        # Initialize evaluation libraries\n","        self.rouge = evaluate.load(\"rouge\")\n","        self.bertscore = evaluate.load(\"bertscore\")\n","\n","    def calculate_ema(self, predictions: List[str], references: List[str]) -> float:\n","        \"\"\"Calculate Exact Match Accuracy with normalization.\n","\n","        Args:\n","            predictions: List of predicted strings\n","            references: List of ground truth strings\n","\n","        Returns:\n","            float: EMA score (0.0 to 1.0)\n","        \"\"\"\n","        if len(predictions) != len(references):\n","            raise ValueError(\"Predictions and references must have same length\")\n","\n","        matches = 0\n","        for pred, ref in zip(predictions, references):\n","            # Normalize strings for comparison\n","            pred_norm = self._normalize_string(pred)\n","            ref_norm = self._normalize_string(ref)\n","\n","            if pred_norm == ref_norm:\n","                matches += 1\n","\n","        return matches / len(predictions) if predictions else 0.0\n","\n","    def calculate_rouge_l(self, predictions: List[str], references: List[str]) -> float:\n","        \"\"\"Calculate Rouge-L score.\n","\n","        Args:\n","            predictions: List of predicted strings\n","            references: List of ground truth strings\n","\n","        Returns:\n","            float: Rouge-L F1 score\n","        \"\"\"\n","        if not predictions or not references:\n","            return 0.0\n","\n","        # Handle empty predictions\n","        clean_predictions = [pred if pred else \"\" for pred in predictions]\n","        clean_references = [ref if ref else \"\" for ref in references]\n","\n","        try:\n","            results = self.rouge.compute(\n","                predictions=clean_predictions,\n","                references=clean_references\n","            )\n","            return results[\"rougeL\"]\n","        except Exception as e:\n","            print(f\"⚠️ Rouge-L calculation error: {e}\")\n","            return 0.0\n","\n","    def calculate_bertscore(self, predictions: List[str], references: List[str]) -> float:\n","        \"\"\"Calculate BERTScore F1.\n","\n","        Args:\n","            predictions: List of predicted strings\n","            references: List of ground truth strings\n","\n","        Returns:\n","            float: Average BERTScore F1\n","        \"\"\"\n","        if not predictions or not references:\n","            return 0.0\n","\n","        # Handle empty predictions\n","        clean_predictions = [pred if pred else \"empty\" for pred in predictions]\n","        clean_references = [ref if ref else \"empty\" for ref in references]\n","\n","        try:\n","            results = self.bertscore.compute(\n","                predictions=clean_predictions,\n","                references=clean_references,\n","                lang=\"en\"\n","            )\n","            return np.mean(results[\"f1\"])\n","        except Exception as e:\n","            print(f\"⚠️ BERTScore calculation error: {e}\")\n","            return 0.0\n","\n","    def _normalize_string(self, text: str) -> str:\n","        \"\"\"Normalize string for comparison.\"\"\"\n","        if not isinstance(text, str):\n","            text = str(text)\n","\n","        # Convert to lowercase and strip whitespace\n","        text = text.lower().strip()\n","\n","        # Remove extra whitespace\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        # Handle special cases\n","        if text in ['na', 'n/a', 'none', '']:\n","            return ''\n","\n","        return text\n","\n","    def evaluate_predictions(self, df: pd.DataFrame) -> Dict:\n","        \"\"\"Comprehensive evaluation of predictions.\n","\n","        Args:\n","            df: DataFrame with predictions and ground truth\n","\n","        Returns:\n","            Dict: All evaluation metrics\n","        \"\"\"\n","        print(\"📊 Calculating evaluation metrics...\")\n","\n","        # Extract predictions and references\n","        pred_inputs = df['pred_input'].tolist()\n","        true_inputs = df['input'].tolist()\n","        pred_outputs = df['pred_output'].tolist()\n","        true_outputs = df['output'].tolist()\n","\n","        # Calculate EMA\n","        ema_input = self.calculate_ema(pred_inputs, true_inputs)\n","        ema_output = self.calculate_ema(pred_outputs, true_outputs)\n","        ema_avg = (ema_input + ema_output) / 2\n","\n","        # Calculate Rouge-L\n","        rouge_input = self.calculate_rouge_l(pred_inputs, true_inputs)\n","        rouge_output = self.calculate_rouge_l(pred_outputs, true_outputs)\n","\n","        # Calculate BERTScore (focus on outputs as they're more complex)\n","        bertscore_output = self.calculate_bertscore(pred_outputs, true_outputs)\n","\n","        metrics = {\n","            'ema_input': ema_input,\n","            'ema_output': ema_output,\n","            'ema_average': ema_avg,\n","            'rouge_l_input': rouge_input,\n","            'rouge_l_output': rouge_output,\n","            'bertscore_f1': bertscore_output,\n","            'total_samples': len(df)\n","        }\n","\n","        return metrics\n","\n","# Initialize metrics calculator\n","metrics_calc = MetricsCalculator()\n","print(\"✅ Metrics calculator initialized successfully!\")\n"],"metadata":{"id":"JGUJ9kEUxja3","executionInfo":{"status":"aborted","timestamp":1755022720713,"user_tz":-60,"elapsed":3319690,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the test results\n","print(\"📊 Evaluating test results...\")\n","test_metrics = metrics_calc.evaluate_predictions(test_df)\n","\n","# Display results\n","print(\"\\n\" + \"=\"*60)\n","print(\"TEST RESULTS (10 samples)\")\n","print(\"=\"*60)\n","print(f\"EMA Input:        {test_metrics['ema_input']:.1%}\")\n","print(f\"EMA Output:       {test_metrics['ema_output']:.1%}\")\n","print(f\"EMA Average:      {test_metrics['ema_average']:.1%}\")\n","print(f\"Rouge-L Input:    {test_metrics['rouge_l_input']:.3f}\")\n","print(f\"Rouge-L Output:   {test_metrics['rouge_l_output']:.3f}\")\n","print(f\"BERTScore F1:     {test_metrics['bertscore_f1']:.3f}\")\n","print(f\"Total Samples:    {test_metrics['total_samples']}\")\n","\n","# Compare with paper benchmarks\n","paper_benchmarks = {\n","    'ema_average': 0.267,  # 26.7%\n","    'rouge_l_input': 0.514,  # 51.4\n","    'rouge_l_output': 0.523  # 52.3\n","}\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"COMPARISON WITH PAPER BENCHMARKS\")\n","print(\"=\"*60)\n","print(f\"{'Metric':<20} {'Test':<10} {'Paper':<10} {'Status':<10}\")\n","print(\"-\" * 60)\n","\n","for metric, paper_value in paper_benchmarks.items():\n","    test_value = test_metrics[metric]\n","    status = \"✅ BETTER\" if test_value >= paper_value else \"❌ LOWER\"\n","    print(f\"{metric:<20} {test_value:<10.3f} {paper_value:<10.3f} {status}\")\n","\n","print(\"\\n⚠️  Note: These are results on only 10 test samples.\")\n","print(\"📈 Run full evaluation for meaningful comparison with paper.\")\n"],"metadata":{"id":"tFPnKogpx8nv","executionInfo":{"status":"aborted","timestamp":1755022720753,"user_tz":-60,"elapsed":3315002,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_full_results(csv_path: str = \"gpt4.1_predictions_full.csv\"):\n","    \"\"\"Evaluate full results and compare with paper benchmarks.\n","\n","    Args:\n","        csv_path: Path to the CSV file with full predictions\n","    \"\"\"\n","    try:\n","        # Load results\n","        df = pd.read_csv(csv_path, keep_default_na=False)\n","        print(f\"📁 Loaded {len(df)} predictions from {csv_path}\")\n","\n","        text_cols = ['input', 'output', 'pred_input', 'pred_output']\n","        df[text_cols] = df[text_cols].fillna('').astype(str)\n","        # Calculate metrics\n","        metrics = metrics_calc.evaluate_predictions(df)\n","\n","        # Paper benchmarks\n","        paper_benchmarks = {\n","            'EMA Average': {'our': metrics['ema_average'], 'paper': 0.267, 'format': '.1%'},\n","            'Rouge-L Input': {'our': metrics['rouge_l_input'], 'paper': 0.514, 'format': '.3f'},\n","            'Rouge-L Output': {'our': metrics['rouge_l_output'], 'paper': 0.523, 'format': '.3f'},\n","            'BERTScore F1': {'our': metrics['bertscore_f1'], 'paper': None, 'format': '.3f'}\n","        }\n","\n","        # Display comprehensive results\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"🎯 FINAL RESULTS - PAPER REPRODUCTION\")\n","        print(\"=\"*80)\n","\n","        print(f\"📊 Dataset: PizzaCommonSense validation split ({metrics['total_samples']} samples)\")\n","        print(f\"🤖 Model: GPT-4.1 with Chain-of-Thought\")\n","        print(f\"📅 Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n","\n","        print(\"\\n\" + \"-\"*80)\n","        print(f\"{'Metric':<20} {'Our Result':<15} {'Paper Benchmark':<18} {'Status':<15} {'Difference'}\")\n","        print(\"-\"*80)\n","\n","        success_count = 0\n","        total_comparable = 0\n","\n","        for metric_name, values in paper_benchmarks.items():\n","            our_val = values['our']\n","            paper_val = values['paper']\n","            fmt = values['format']\n","\n","            if paper_val is not None:\n","                total_comparable += 1\n","                diff = our_val - paper_val\n","                status = \"✅ BETTER\" if our_val >= paper_val else \"❌ LOWER\"\n","                if our_val >= paper_val:\n","                    success_count += 1\n","\n","                our_str = f\"{our_val:{fmt}}\"\n","                paper_str = f\"{paper_val:{fmt}}\"\n","                diff_str = f\"{diff:+{fmt}}\"\n","            else:\n","                status = \"📊 NEW METRIC\"\n","                our_str = f\"{our_val:{fmt}}\"\n","                paper_str = \"N/A\"\n","                diff_str = \"N/A\"\n","\n","            print(f\"{metric_name:<20} {our_str:<15} {paper_str:<18} {status:<15} {diff_str}\")\n","\n","        # Success summary\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"🏆 REPRODUCTION SUCCESS SUMMARY\")\n","        print(\"=\"*80)\n","\n","        success_rate = success_count / total_comparable if total_comparable > 0 else 0\n","        print(f\"✅ Metrics meeting/exceeding paper: {success_count}/{total_comparable} ({success_rate:.1%})\")\n","\n","        if success_count > 0:\n","            print(\"🎉 SUCCESS: At least one metric meets the paper benchmark!\")\n","            print(\"📈 Reproduction experiment successful!\")\n","        else:\n","            print(\"⚠️  No metrics exceed paper benchmarks\")\n","            print(\"🔍 Consider adjusting prompts or trying different techniques\")\n","\n","        # Detailed breakdown\n","        print(\"\\n\" + \"-\"*80)\n","        print(\"📋 DETAILED BREAKDOWN\")\n","        print(\"-\"*80)\n","        print(f\"EMA Input:        {metrics['ema_input']:.1%}\")\n","        print(f\"EMA Output:       {metrics['ema_output']:.1%}\")\n","        print(f\"EMA Average:      {metrics['ema_average']:.1%} (Target: 26.7%)\")\n","        print(f\"Rouge-L Input:    {metrics['rouge_l_input']:.3f} (Target: 0.514)\")\n","        print(f\"Rouge-L Output:   {metrics['rouge_l_output']:.3f} (Target: 0.523)\")\n","        print(f\"BERTScore F1:     {metrics['bertscore_f1']:.3f} (New metric)\")\n","\n","        # Save metrics to JSON\n","        metrics_file = \"final_metrics.json\"\n","        with open(metrics_file, 'w') as f:\n","            json.dump(metrics, f, indent=2)\n","        print(f\"\\n💾 Detailed metrics saved to {metrics_file}\")\n","\n","        return metrics\n","\n","    except FileNotFoundError:\n","        print(f\"❌ File {csv_path} not found. Run full batch processing first.\")\n","        return None\n","    except Exception as e:\n","        print(f\"❌ Error evaluating results: {e}\")\n","        return None\n","\n","print(\"✅ Full evaluation function ready!\")\n"],"metadata":{"id":"sNpGC5B6x-71","executionInfo":{"status":"aborted","timestamp":1755022720788,"user_tz":-60,"elapsed":3312103,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run full evaluation (uncomment after running full batch processing)\n","print(\"🎯 Ready to evaluate full results\")\n","\n","print(\"\\n🔧 To run full evaluation, uncomment the line below:\")\n","print(\"# final_metrics = evaluate_full_results('gpt4.1_predictions_full.csv')\")\n","\n","# Uncomment this line after running full batch processing:\n","final_metrics = evaluate_full_results(\"gpt4.1_predictions_full.csv\")\n"],"metadata":{"id":"_T8Z6fRZyBrv","executionInfo":{"status":"aborted","timestamp":1755022720806,"user_tz":-60,"elapsed":3308906,"user":{"displayName":"Fuchiyuki Seki (Fuchi)","userId":"03417443119321133441"}}},"execution_count":null,"outputs":[]}]}