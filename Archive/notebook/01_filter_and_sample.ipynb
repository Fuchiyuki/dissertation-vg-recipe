{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlyNrVoIkvUq",
        "outputId": "b60a883a-e291-4cd6-b63f-39e1cc30a271"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zayjeC3-k9gl"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle tqdm pandas numpy nltk unidecode rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "di8BPI_35RBh"
      },
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# 0. Environment Preparation\n",
        "# =============================================================\n",
        "\n",
        "# If running in Google Colab, uncomment to mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !pip install -q kaggle tqdm pandas numpy nltk unidecode rapidfuzz\n",
        "\n",
        "import os, json, re, unicodedata, warnings\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rapidfuzz import fuzz, process\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk6qIxrn5Vfk",
        "outputId": "96a30408-16be-4848-c921-55336c15c5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Using recipe file: layer1.json\n",
            "✓ Using ingredients file: det_ingrs.json\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# 1. Dataset Acquisition & File Detection\n",
        "# =============================================================\n",
        "\n",
        "DATA_DIR = Path('/content/drive/MyDrive/recipe1m')\n",
        "assert DATA_DIR.exists(), f\"DATA_DIR does not exist: {DATA_DIR}. Did you unzip the dataset?\"\n",
        "\n",
        "# Candidate filenames used by different Recipe1M mirrors\n",
        "RECIPE_CANDIDATES = ['recipes.json', 'layer1.json', 'layer1+.json']\n",
        "INGR_CANDIDATES   = ['ingredients.json', 'det_ingrs.json']\n",
        "\n",
        "RECIPES_FILE = next((DATA_DIR / f for f in RECIPE_CANDIDATES if (DATA_DIR / f).exists()), None)\n",
        "INGRS_FILE   = next((DATA_DIR / f for f in INGR_CANDIDATES   if (DATA_DIR / f).exists()), None)\n",
        "\n",
        "if RECIPES_FILE is None:\n",
        "    raise FileNotFoundError(\n",
        "        f\"None of {RECIPE_CANDIDATES} found under {DATA_DIR}. \"\n",
        "        \"Please verify the extraction path or update DATA_DIR.\")\n",
        "else:\n",
        "    print(f\"✓ Using recipe file: {RECIPES_FILE.name}\")\n",
        "\n",
        "if INGRS_FILE is None:\n",
        "    warnings.warn(\n",
        "        f\"Ingredients JSON not found (looked for {INGR_CANDIDATES}). \"\n",
        "        \"Proceeding without it — only recipe metadata will be parsed.\")\n",
        "else:\n",
        "    print(f\"✓ Using ingredients file: {INGRS_FILE.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE6kiDD95gFI",
        "outputId": "712a3ed0-9409-401a-b7b9-36dbd3391c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading recipes… this may take a minute ⏳\n",
            "Loaded 1,029,720 recipes from layer1.json\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# 2. Load JSON into DataFrame\n",
        "# =============================================================\n",
        "\n",
        "def load_recipe1m(json_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Load Recipe1M layer1/recipes JSON into pandas DataFrame, handling both schemas.\"\"\"\n",
        "    with open(json_path, 'r', encoding='utf‑8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Schema A (layer1.json, list of dicts) — official Recipe1M format\n",
        "    if isinstance(data, list) and 'id' in data[0]:\n",
        "        records = []\n",
        "        for r in data:\n",
        "            records.append({\n",
        "                'id':            r['id'],\n",
        "                'title':         r['title'],\n",
        "                'ingredients':   [i['text'] if isinstance(i, dict) else i for i in r['ingredients']],\n",
        "                'instructions':  [s['text'] if isinstance(s, dict) else s for s in r['instructions']],\n",
        "                'partition':     r.get('partition', 'train')\n",
        "            })\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    # Schema B (recipes.json, dict keyed by ids)\n",
        "    if isinstance(data, dict):\n",
        "        records = []\n",
        "        for _id, r in data.items():\n",
        "            records.append({\n",
        "                'id':            _id,\n",
        "                'title':         r['title'],\n",
        "                'ingredients':   [i['text'] if isinstance(i, dict) else i for i in r['ingredients']],\n",
        "                'instructions':  [s['text'] if isinstance(s, dict) else s for s in r['instructions']],\n",
        "                'partition':     r.get('partition', 'train')\n",
        "            })\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    raise ValueError(\"Unrecognised JSON schema in recipe file — inspect format manually.\")\n",
        "\n",
        "print(\"Loading recipes… this may take a minute ⏳\")\n",
        "df = load_recipe1m(RECIPES_FILE)\n",
        "print(f\"Loaded {len(df):,} recipes from {RECIPES_FILE.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KjYHx_kg5lhJ"
      },
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# 3. Knowledge Base\n",
        "# =============================================================\n",
        "\n",
        "EXPLICIT_ANIMAL = [\n",
        "    'chicken','beef','pork','ham','bacon','turkey','duck','lamb','veal',\n",
        "    'salmon','tuna','shrimp','prawn','crab','lobster','anchovy','sardine',\n",
        "    'fish','fish sauce','clam','oyster']\n",
        "\n",
        "DAIRY_EGGS = ['milk','butter','cream','cheese','yogurt','egg','eggs','albumen','whey','casein','ghee']\n",
        "\n",
        "HIDDEN_ANIMAL = [\n",
        "    'gelatin','isinglass','carmine','cochineal','lard','tallow','rennet',\n",
        "    'l‑cysteine','shellac','bone char','anchovy paste']\n",
        "\n",
        "AMBIGUOUS = ['lecithin','natural flavor','stearic acid','glycerides','mono‑ and diglycerides','emulsifier']\n",
        "\n",
        "KB = {\n",
        "    'explicit_animal': [unidecode(t) for t in EXPLICIT_ANIMAL],\n",
        "    'hidden_animal':   [unidecode(t) for t in HIDDEN_ANIMAL],\n",
        "    'ambiguous':       [unidecode(t) for t in AMBIGUOUS],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9j5aTvU5qAs",
        "outputId": "fd7e083b-a02c-4a09-c205-4f1cde0faa1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenising ingredients…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1029720/1029720 [00:51<00:00, 20172.49it/s]\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# 4. Normalisation helpers\n",
        "# =============================================================\n",
        "\n",
        "def normalise(text: str) -> str:\n",
        "    text = unidecode(text.lower())\n",
        "    text = re.sub(r'[^a-z0-9\\s\\-/]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def ingredient_tokens(ings: List[str]) -> List[str]:\n",
        "    toks = []\n",
        "    for ing in ings:\n",
        "        toks.extend(normalise(ing).split())\n",
        "    return toks\n",
        "\n",
        "print(\"Tokenising ingredients…\")\n",
        "tqdm.pandas()\n",
        "df['tokens'] = df['ingredients'].progress_apply(ingredient_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0XjEzf55vPn",
        "outputId": "eca44bac-94cd-4d09-88da-df1bca830c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running multi‑stage filter…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1029720/1029720 [00:17<00:00, 59872.27it/s]\n",
            "100%|██████████| 1029720/1029720 [00:19<00:00, 54156.35it/s]\n",
            "100%|██████████| 1029720/1029720 [00:16<00:00, 63441.85it/s]\n",
            "100%|██████████| 1029720/1029720 [00:13<00:00, 79126.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "High‑confidence vegetarian recipes:   654,664\n",
            "Medium‑confidence (ambiguous) recipes: 144\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# 5. Filtering pipeline (Stages 1–4)\n",
        "# =============================================================\n",
        "\n",
        "def stage1_keyword(row) -> bool:\n",
        "    markers = ['vegetarian','vegan','plant based','meatless']\n",
        "    title = normalise(row['title'])\n",
        "    text  = ' '.join(row['tokens'])\n",
        "    return any(m in title or m in text for m in markers)\n",
        "\n",
        "\n",
        "def contains_any(tokens: List[str], vocab: List[str]) -> bool:\n",
        "    return any(tok in vocab for tok in tokens)\n",
        "\n",
        "\n",
        "def stage2_explicit(row) -> bool:\n",
        "    return contains_any(row['tokens'], KB['explicit_animal'])\n",
        "\n",
        "\n",
        "def stage3_hidden(row) -> bool:\n",
        "    return contains_any(row['tokens'], KB['hidden_animal'])\n",
        "\n",
        "\n",
        "def stage4_ambiguous(row) -> bool:\n",
        "    return contains_any(row['tokens'], KB['ambiguous'])\n",
        "\n",
        "print(\"Running multi‑stage filter…\")\n",
        "df['stage1']          = df.progress_apply(stage1_keyword, axis=1)\n",
        "df['exclude_explicit'] = df.progress_apply(stage2_explicit, axis=1)\n",
        "df['exclude_hidden']   = df.progress_apply(stage3_hidden, axis=1)\n",
        "df['flag_ambiguous']   = df.progress_apply(stage4_ambiguous, axis=1)\n",
        "\n",
        "conditions_high   = (~df['exclude_explicit']) & (~df['exclude_hidden']) & (~df['flag_ambiguous'])\n",
        "conditions_medium = (~df['exclude_explicit']) & (~df['exclude_hidden']) & ( df['flag_ambiguous'])\n",
        "\n",
        "df_high   = df[conditions_high]\n",
        "df_medium = df[conditions_medium]\n",
        "\n",
        "print(f\"High‑confidence vegetarian recipes:   {len(df_high):,}\")\n",
        "print(f\"Medium‑confidence (ambiguous) recipes: {len(df_medium):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "SAMPLE_SIZE = 1000  # sample size\n",
        "\n",
        "# ── ランダムサンプリング ──\n",
        "df_sample = df_high.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ── 確認表示 ──\n",
        "print(f\"Sampled {len(df_sample)} recipes (SAMPLE_SIZE={SAMPLE_SIZE})\")\n",
        "df_sample.head()\n",
        "\n",
        "# ── Drive へ保存 ──\n",
        "import os\n",
        "out_dir = '/content/drive/MyDrive/veg_recipe1m_subset'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "df_sample.to_json(\n",
        "    f\"{out_dir}/veg_recipe1m_sample_{SAMPLE_SIZE}.json\",\n",
        "    orient='records',\n",
        "    force_ascii=False\n",
        ")\n",
        "print(f\"Saved sample JSON to {out_dir}/veg_recipe1m_sample_{SAMPLE_SIZE}.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTj2Ta1MkcYb",
        "outputId": "8e2bbafd-b2ed-4446-cf05-bd1f2768b4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved JSON subsets to /content/veg_recipe1m\n",
            "Top ingredients (high‑confidence set):\n",
            "1              2026776\n",
            "cup            1225333\n",
            "2               972128\n",
            "teaspoon        671851\n",
            "12              544472\n",
            "cups            538426\n",
            "1/2             511233\n",
            "tablespoons     441493\n",
            "sugar           403321\n",
            "salt            354982\n",
            "3               333507\n",
            "chopped         327196\n",
            "or              292041\n",
            "14              267614\n",
            "butter          265003\n",
            "1/4             257257\n",
            "4               245411\n",
            "tablespoon      242720\n",
            "teaspoons       235091\n",
            "and             234649\n",
            "pepper          227194\n",
            "oil             224718\n",
            "flour           222616\n",
            "fresh           218938\n",
            "cheese          182271\n",
            "ounce           179212\n",
            "ounces          175728\n",
            "ground          167009\n",
            "cream           163190\n",
            "vanilla         156118\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# 6. Save subsets\n",
        "# =============================================================\n",
        "\n",
        "OUT_DIR = Path('/content/veg_recipe1m')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_high.to_json(OUT_DIR / 'veg_recipe1m_high.json',  orient='records', force_ascii=False)\n",
        "df_medium.to_json(OUT_DIR / 'veg_recipe1m_medium.json', orient='records', force_ascii=False)\n",
        "print(f\"✓ Saved JSON subsets to {OUT_DIR}\")\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# 7. Quick EDA helper\n",
        "# =============================================================\n",
        "\n",
        "def top_ingredients(df_subset: pd.DataFrame, n: int = 30):\n",
        "    cnt = Counter()\n",
        "    for toks in df_subset['tokens']:\n",
        "        cnt.update(toks)\n",
        "    return pd.Series(cnt).sort_values(ascending=False).head(n)\n",
        "\n",
        "print(\"Top ingredients (high‑confidence set):\")\n",
        "print(top_ingredients(df_high))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
